{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.预测模型 3.扩展数据集(ccks+FN) 4.扩展数据集用于预训练(ccks+FN) 5.验证(四川的500条上) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.训练出ccks的预测模型\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/ccks/train-json.csv',\n",
    "    # 'eval_file': './data/ccks/dev.csv',\n",
    "    # 'test_file': './data/ccks/test.csv',\n",
    "    'eval_file': './data/FN/sc-json/dev.csv',\n",
    "    'test_file': './data/FN/sc-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'ccks_predict_model'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.预测四川数据集（多标签）\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/ccks_predict_model/lstm_crf/lstm_crf_1320.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/ccks_predict_model/LEBert/LEBert_1320.pth\"\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/sc-super/sc-train_400.json\"\n",
    "\n",
    "batch_size = 64\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc-super/sc_super_400.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert预训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_2/Bert_8960/pytorch_model.bin',\n",
    "    'max_seq_length': 256,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/CDD/train.json',\n",
    "    'eval_file': './data/CDD/dev.json',\n",
    "    'test_file': './data/CDD/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/CDD/cdd_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v3',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'cdd_pre_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        # \"KEYWORD\": \"关键词\",\n",
    "        # \"DIS\": \"疾病或诊断\",\n",
    "        # \"ANA\": \"解剖部位\",\n",
    "        # \"LAB\": \"实验室检验\",\n",
    "        # \"MED\": \"药物\",\n",
    "        # \"OPE\": \"手术\",\n",
    "        # \"IMA\": \"影像检查\",\n",
    "        \"CHECK\":\"检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert预训练\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pre_2_512/Bert_4375/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/before/subtask1_train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v3',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'ccks_pre_3_512',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        # \"KEYWORD\": \"关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "        # \"CHECK\":\"检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/cdd_pre_1/Bert_12215/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/CDD/train.json',\n",
    "    'eval_file': './data/CDD/dev.json',\n",
    "    'test_file': './data/CDD/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/CDD/cdd_tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'classify':'crf',\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'cdd_8_LEBert_crf_pro_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "args['task_name'] = 'cdd_8_LEBert_crf_pro_2'\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args['task_name'] = 'cdd_8_LEBert_crf_pro_3'\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args['task_name'] = 'cdd_8_LEBert_crf_pro_4'\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args['task_name'] = 'cdd_8_LEBert_crf_pro_5'\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.验证2\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_3/Bert_33600/pytorch_model.bin',\n",
    "\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/ccks/train.json',\n",
    "    # 'eval_file': './data/ccks/sc-json-500/dev.json',\n",
    "    # 'test_file': './data/ccks/sc-json-500/test.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc-json-500/dev.json',\n",
    "    'test_file': './data/FN/sc-json-500/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_dev_3'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc-json/dev.csv',\n",
    "    'test_file': './data/FN/sc-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_dev_base_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1预训练(ccks+FJ)   2.预测模型 3.扩展数据集(ccks+FN) 4.扩展数据集用于预训练(ccks+FN) 5.验证(四川的500条上) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.预训练\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pretrained/Bert_3160/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/ccks/train_all.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'ccks_pretrained_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.预测 修改：训练集要替换成ccks，预测ccks标签+keyword\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pretrained/Bert_6055/pytorch_model.bin', #第一次的 只有ccks的预训练\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pretrained/Bert_6055/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/ccks+FN_FJ/train.json',\n",
    "    'eval_file': './data/FN/sc-json-500/dev.json',\n",
    "    'test_file': './data/FN/sc-json-500/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 32,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN+CCKS_predict_model'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer():\n",
    "#     a = i\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 扩展数据集   修改：要扩展ccks标签+FN\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/FN+CCKS_predict_model/lstm_crf/lstm_crf_.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/FN+CCKS_predict_model/LEBert/LEBert_.pth\"\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/fj+sc/train(400).json\"\n",
    "\n",
    "batch_size = 40\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc_fj_tags_keyword/train(400)_ccks.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练 \n",
    "# sc&fj_tags_ccks_pretrain:多标签的sc和fj数据集（各400条）进行预训练\n",
    "# sc&fj_tags_ccks_pretrain_2:跟上面一样 lr=1e-5\n",
    "# sc_fj_tags_keyword_pretrain:keyword标签的sc和fj数据集（各400条）进行预训练\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc&fj_tags_ccks_pretrain/Bert_11640/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc_fj_tags_keyword/fj_400_sc_400.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.jsonn',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    # 'tag_file': './data/ccks/ccks+FN_tags_list.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'sc_fj_tags_keyword_pretrain',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-4):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc&fj_tags_ccks_pretrain_2/Bert_10088/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_fj_sc_tag_ccks_pre_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.外部数据集训练一个预测模型 2.把福建和四川总共800条的数据集标签拓展成多标签的 \n",
    "#### 3.把数据集预训练 4.验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.预测 \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pretrained_2/Bert_6055/pytorch_model.bin', \n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'ccks_predict_model_2'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer():\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.预测fj+sc数据集（多标签）\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/ccks_predict_model_2/lstm_crf/lstm_crf_690.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/ccks_predict_model_2/LEBert/LEBert_690.pth\"\n",
    "\n",
    "\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/sc_fj_tags_keyword/fj_400_sc_400.json\"\n",
    "\n",
    "batch_size = 64\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc_fj_tags_ccks/fj_400_sc_400_ccks.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练 \n",
    "# sc&fj_tags_ccks_pretrain:多标签的sc和fj数据集（各400条）进行预训练  需要重新预训练\n",
    "# sc&fj_tags_ccks_pretrain_2:跟上面一样 lr=1e-5\n",
    "# sc_fj_tags_keyword_pretrain:keyword标签的sc和fj数据集（各400条）进行预训练\n",
    "\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='2, 3'\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_fj_tags_keyword_pretrain/Bert_12080/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc_fj_tags_keyword/fj_400_sc_400.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.jsonn',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    # 'tag_file': './data/ccks/ccks+FN_tags_list.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'sc_fj_tags_keyword_pretrain_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 lr2=1e-2\n",
    "#  5.验证 7是有左括号切分 8是没有左括号 9是强制120切分 10在9的基础上120匹配左括号切分 67% 11 keyword 12 lexloader \n",
    "#  只有400条的sc数据集做多标签预训练  13 训练集只有四川   14 800条 15 只有keyword \n",
    "#  16 只有福建，训练测试验证也是福建。出现了0\n",
    "#  17 在训练集只有福建\n",
    "#  FN_fj_sc_tag_ccks_pre_2_eval ccks多标签 测试集100\n",
    "#  FN_sc_fj_tags_keyword_pretrain_2 只有keyword\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_fj_tags_keyword_pretrain_2/Bert_15100/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_fj_tags_keyword_pretrain_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 lr2=1e-2\n",
    "#  5.验证 7是有左括号切分 8是没有左括号 9是强制120切分 10在9的基础上120匹配左括号切分 67% 11 keyword 12 lexloader \n",
    "#  只有400条的sc数据集做多标签预训练  13 训练集只有四川   14 800条 15 只有keyword\n",
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc&fj_tags_ccks_pretrain_2/Bert_10088/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_fj_sc_tag_ccks_pre_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换成json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.to_json import conll_to_json\n",
    "conll_to_json('./data/ccks/subtask1_training_part2.txt', './data/ccks/3.csv', split_tag='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.to_json import conll_to_json\n",
    "conll_to_json('./data/FN/sc-super/dev.csv', './data/FN/sc-super/dev.json', split_tag='\\n\\n')\n",
    "conll_to_json('./data/FN/sc-super/test.csv', './data/FN/sc-super/test.json', split_tag='\\n\\n')\n",
    "conll_to_json('./data/FN/sc-super/train.csv', './data/FN/sc-super/train.json', split_tag='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"data/FN/fj+sc/train(400).json\"\n",
    "sum = 0\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        text = data[\"text\"]\n",
    "        label = data[\"label\"]\n",
    "        \n",
    "        if len(text)>2000:\n",
    "            sum = sum+1\n",
    "print(sum)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "from tqdm import *\n",
    "from CC.loaders.pretrain import *\n",
    "from CC.loaders.pretrain.ptloader_v2 import *\n",
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_400_6/Bert_1200/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc-super/train_super_400.json',\n",
    "    'eval_file': './data/FN/sc-json-500/dev.json',\n",
    "    'test_file': './data/FN/sc-json-500/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'FN_multiple_pretrained_400_6',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"关键词\",\n",
    "        \"DIS\": \"疾病诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "loader = PTLoaderV2(**args)\n",
    "\n",
    "\n",
    "# loader.tag_vocab.id2token(loader.myData[0][\"labels\"].tolist())\n",
    "\n",
    "# loader.tokenizer.decode(loader.myData[0][\"input_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.myData[799][\"input_ids\"]\n",
    "loader.tokenizer.decode(loader.myData[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.myData[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重新验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/FN/FN_spuer_pretrained_400_5/Bert_12832/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_test100_pre'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN/FN_spuer_pretrained_400_5/Bert_12832/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_test100_bs_4'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_spuer_pretrained_fj_pt/Bert_21700/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/fj+sc/test(100+100).json',\n",
    "    'test_file': './data/FN/fj+sc/test(100+100).json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_fj_test200'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证仅四川的和福建的（ccks标签&keyword标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.预测 \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pretrained_2/Bert_6055/pytorch_model.bin', \n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'ccks_predict_model_2'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer():\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拓展数据集\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/ccks_predict_model_2/lstm_crf/lstm_crf_690.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/ccks_predict_model_2/LEBert/LEBert_690.pth\"\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/sc_json_500/pretrain_keyword.json\"\n",
    "\n",
    "batch_size = 40\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc_json_500/pretrain_ccks.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练\n",
    "# sc_ccks_pretrain_3:sc的ccks预训练 13600\n",
    "# sc_keyword_pretrain_2:sc的keyword预训练 34000\n",
    "# fj_keyword_pretrain_2 27200\n",
    "# fj_ccks_pretrain_2 10940\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/fj_ccks_pretrain_1/Bert_4080/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj_json/pretrain_keyword.json',\n",
    "    'eval_file': './data/FN/fj_json/eval.json',\n",
    "    'test_file': './data/FN/fj_json/test.jsonn',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/ccks/ccks+FN_tags_list.txt',\n",
    "    # 'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'fj_ccks_pretrain_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/FN/medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    # 'task_name': 'sc_medicine_vocab_baseline_4'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#四川\n",
    "# sc_400_baseline:0.6795、0.6949、0.6936 0.6868 0.6804\n",
    "# sc_ccks_dev_1 0.6831 0.6833 0.6804\n",
    "# sc_keyword_dev_1 预训练和文件的标签keyword 0.6868 0.6936 0.6788\n",
    "\n",
    "# sc_medicine_vocab_baseline 0.6802 0.6882 0.6824 0.7127 0.6702 0.6872\n",
    "# sc_medicine_vocab_pretrain_ccks 0.6647\n",
    "# sc_medicine_vocab_pretrain_keyword 0.6890\n",
    "\n",
    "# sc_tx_med_baseline 0.6820 0.6838 0.6917 0.6936 0.68\n",
    "\n",
    "# args[\"pretrained_file_name\"] = \"./model/chinese_wwm_ext/pytorch_model.bin\"\n",
    "args[\"pretrained_file_name\"] = \"./save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin\"\n",
    "\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_vocab.txt\" # TX\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/FN_medicine_vocab.txt\" # FN\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tx_med_repetition.txt\"  #he tengxun jiehe mei quchong\n",
    "args[\"word_vocab_file\"] =\"./data/tencent/tencent_medicine_vocab.txt\" #quchong\n",
    "\n",
    "# args[\"task_name\"] = \"sc_tx_med_only_pre_ccks_1\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_ccks_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_ccks_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_ccks_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_ccks_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args[\"pretrained_file_name\"] = \"./model/chinese_wwm_ext/pytorch_model.bin\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin\"\n",
    "args[\"pretrained_file_name\"] = \"./save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin\"\n",
    "args[\"task_name\"] = \"sc_tx_med_only_pre_keyword_1\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_keyword_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_keyword_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_keyword_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"sc_tx_med_only_pre_keyword_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 福建\n",
    "\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/fj_ccks_pretrain_2/Bert_19040/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file\":\"./data/FN/medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    # 'task_name': 'fj_medicine_vocab_keyword'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 福建\n",
    "# fj_baseline 0.9233\n",
    "# fj_keyword_dev_1 0.9039\n",
    "# fj_ccks_dev_1 0.9219\n",
    "\n",
    "# fj_medicine_vocab_baseline 0.9289\n",
    "# fj_medicine_vocab_ccks 0.9220\n",
    "# fj_medicine_vocab_keyword 0.9079\n",
    "\n",
    "# args[\"pretrained_file_name\"] = \"./model/chinese_wwm_ext/pytorch_model.bin\"\n",
    "args[\"pretrained_file_name\"] = \"./save_pretrained/fj_ccks_pretrain_2/Bert_19040/pytorch_model.bin\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin\"\n",
    "\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/FN_medicine_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tx_med_repetition.txt\"\n",
    "args[\"word_vocab_file\"] =\"./data/tencent/tencent_medicine_vocab.txt\"\n",
    "args[\"task_name\"] = \"fj_tx_med_only_pre_ccks_1\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 福建\n",
    "\n",
    "\n",
    "# args[\"pretrained_file_name\"] = \"./model/chinese_wwm_ext/pytorch_model.bin\"\n",
    "args[\"pretrained_file_name\"] = \"./save_pretrained/fj_ccks_pretrain_2/Bert_19040/pytorch_model.bin\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin\"\n",
    "\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/FN_medicine_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tx_med_repetition.txt\"\n",
    "args[\"word_vocab_file\"] =\"./data/tencent/tencent_medicine_vocab.txt\"\n",
    "args[\"task_name\"] = \"fj_tx_med_only_pre_ccks_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_tx_med_only_pre_ccks_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_tx_med_only_pre_ccks_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_tx_med_only_pre_ccks_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"pretrained_file_name\"] = \"./save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin\"\n",
    "args[\"task_name\"] = \"fj_tx_med_only_pre_keyword_1\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_tx_med_only_pre_keyword_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_tx_med_only_pre_keyword_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_tx_med_only_pre_keyword_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_tx_med_only_pre_keyword_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_med_pretrain_ccks_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_med_pretrain_ccks_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_med_pretrain_ccks_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args[\"pretrained_file_name\"] = \"./save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin\"\n",
    "\n",
    "args[\"word_vocab_file\"] =\"./data/tencent/FN_medicine_vocab.txt\"\n",
    "\n",
    "args[\"task_name\"] = \"fj_med_pretrain_keyword_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_med_pretrain_keyword_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_med_pretrain_keyword_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_med_pretrain_keyword_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 福建\n",
    "# fj_keyword_dev_1 0.9039\n",
    "# fj_ccks_dev_1 0.9219\n",
    "\n",
    "# args[\"pretrained_file_name\"] = \"./model/chinese_wwm_ext/pytorch_model.bin\"\n",
    "args[\"pretrained_file_name\"] = \"./save_pretrained/fj_ccks_pretrain_2/Bert_19040/pytorch_model.bin\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin\"\n",
    "\n",
    "args[\"word_vocab_file\"] =\"./data/tencent/tencent_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/FN_medicine_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tx_med_repetition.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_medicine_vocab.txt\"\n",
    "args[\"task_name\"] = \"fj_ccks_dev_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_ccks_dev_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_ccks_dev_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_ccks_dev_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"pretrained_file_name\"] = \"./save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin\"\n",
    "args[\"task_name\"] = \"fj_keyword_dev_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_keyword_dev_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_keyword_dev_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"fj_keyword_dev_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccks\n",
    "# ccks_baseline 0.8258 0.8197 \n",
    "# ccks_medicine_vocab_baseline 0.8149 0.8120\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    # \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/FN/medicine_vocab.txt\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    # 'task_name': 'ccks_medicine_vocab_baseline_2'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/FN_medicine_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tx_med_repetition.txt\"\n",
    "args[\"word_vocab_file\"] =\"./data/tencent/tencent_medicine_vocab.txt\"\n",
    "args[\"task_name\"] = \"ccks_tx_med_only_1\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"ccks_tx_med_only_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args[\"task_name\"] = \"ccks_tx_med_only_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"ccks_tx_med_only_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_tx_med_only_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9392d1f0914889243d058bb73f0d89e61311fd6d751bbc8fa50e38d7d4ff811"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NER': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
