{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 四川\n",
    "# from CC.trainer import NERTrainer\n",
    "# args = {\n",
    "#     'num_epochs': 30,\n",
    "#     'num_gpus': [0, 1],\n",
    "#     'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "#     # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "#     'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "#     # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "#     'hidden_dim': 300,\n",
    "#     'max_seq_length': 512,\n",
    "#     'max_scan_num': 1000000,\n",
    "#     'inter_max_scan_num': 3000,\n",
    "#     # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "#     'train_file': './data/FN/sc_json_500/train.json',\n",
    "#     'eval_file': './data/FN/sc_test100/test.json',\n",
    "#     'test_file': './data/FN/sc_test100/test.json',\n",
    "#     'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "#     'tag_file': './data/FN/tags_list.txt',\n",
    "#     'loader_name': 'le_loader_zl',\n",
    "#     # 'loader_name': 'le_loader',\n",
    "#     'output_eval':True,\n",
    "#     \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "#     \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "#     \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "#     \"default_tag\":\"O\",\n",
    "#     'batch_size': 8,\n",
    "#     'eval_batch_size': 64,\n",
    "#     'do_shuffle': True,\n",
    "#     \"use_gpu\": True,\n",
    "#     \"debug\": True,\n",
    "#     'model_name': 'ZLEBert',\n",
    "#     'task_name': 'zl_sc_tx_inter_fn3000_ccks_1'\n",
    "# }\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#福建\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 3000,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'task_name': 'zl_fj_tx_inter_fn3000_ccks_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccks\n",
    "# ccks_baseline 0.8258 0.8197 \n",
    "# ccks_medicine_vocab_baseline 0.8149 0.8120\n",
    "# from CC.trainer import NERTrainer\n",
    "\n",
    "# args = {\n",
    "#     'num_epochs': 30,\n",
    "#     'num_gpus': [0, 1],\n",
    "#     'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "#     'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "#     'hidden_dim': 300,\n",
    "#     'max_seq_length': 150,\n",
    "#     'max_scan_num': 1000000,\n",
    "#     'inter_max_scan_num': 3000,\n",
    "#     'train_file': './data/ccks/train.json',\n",
    "#     'eval_file': './data/ccks/dev.json',\n",
    "#     'test_file': './data/ccks/test.json',\n",
    "#     'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "#     'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "#     'loader_name': 'le_loader_zl',\n",
    "#     # 'loader_name': 'le_loader',\n",
    "#     'output_eval':True,\n",
    "#     \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "#     \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "#     \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "#     \"default_tag\":\"O\",\n",
    "#     'batch_size': 8,\n",
    "#     'eval_batch_size': 64,\n",
    "#     'do_shuffle': True,\n",
    "#     \"use_gpu\": True,\n",
    "#     \"debug\": True,\n",
    "#     'model_name': 'ZLEBert',\n",
    "#     'task_name': 'zl_ccks_tx_inter_fn3000_base_1'\n",
    "# }\n",
    "\n",
    "# # Trainer\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn3000_ccks_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn3000_ccks_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn3000_ccks_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn3000_ccks_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四川\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v2',\n",
    "    'task_name': 'zl_v2_sc_tx_inter_fn3000_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#福建\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 3000,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v2',\n",
    "    'task_name': 'zl_v2_fj_tx_inter_fn3000_1'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj_json/train_400.json\",\n",
      "    \"eval_file\": \"./data/FN/fj_json/dev.json\",\n",
      "    \"test_file\": \"./data/FN/fj_json/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/FN_medicine_vocab.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 3000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"zl_v2_fj_tx_inter_fn3000_3\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj_json/train_400.json etag: 100%|██████████| 1.76M/1.76M [00:00<00:00, 282MB/s]\n",
      "calculate ./data/FN/fj_json/dev.json etag: 100%|██████████| 9.75M/9.75M [00:00<00:00, 317MB/s]\n",
      "calculate ./data/FN/fj_json/test.json etag: 100%|██████████| 9.64M/9.64M [00:00<00:00, 312MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 43.4kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 6449.47L/s]\n",
      "build line mapper: 3L [00:00, 6879.67L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 862.14it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 45590.26it/s]\n",
      "/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/fj_json/train_400.json: 100%|██████████| 400/400 [00:02<00:00, 138.16it/s]\n",
      "load dataset from ./data/FN/fj_json/dev.json: 100%|██████████| 2202/2202 [00:15<00:00, 138.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel_v2: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel_v2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel_v2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel_v2 were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.3.word_word_weight.bias', 'bert.encoder.layer.10.word_word_weight.weight', 'bert.encoder.layer.10.word_transform.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.4.word_word_weight.weight', 'bert.encoder.layer.5.word_transform.bias', 'bert.encoder.layer.3.word_transform.weight', 'bert.encoder.layer.5.word_transform.weight', 'bert.encoder.layer.9.word_transform.weight', 'bert.encoder.layer.11.word_word_weight.bias', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.11.word_transform.weight', 'bert.encoder.layer.3.word_transform.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.10.word_word_weight.bias', 'bert.encoder.layer.1.word_transform.bias', 'bert.encoder.layer.2.word_transform.weight', 'bert.encoder.layer.5.word_word_weight.bias', 'bert.encoder.layer.7.word_transform.bias', 'bert.encoder.layer.6.word_word_weight.bias', 'bert.encoder.layer.2.word_word_weight.weight', 'bert.encoder.layer.6.word_transform.weight', 'bert.encoder.layer.7.word_transform.weight', 'bert.encoder.layer.1.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.9.word_word_weight.weight', 'bert.encoder.layer.4.word_word_weight.bias', 'bert.encoder.layer.2.word_word_weight.bias', 'bert.encoder.layer.8.word_transform.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.2.word_transform.bias', 'bert.encoder.layer.1.word_transform.weight', 'bert.encoder.layer.4.word_transform.weight', 'bert.encoder.layer.6.word_transform.bias', 'bert.encoder.layer.10.word_transform.bias', 'bert.encoder.layer.5.word_word_weight.weight', 'bert.encoder.layer.3.word_word_weight.weight', 'bert.encoder.layer.8.word_transform.weight', 'bert.encoder.layer.7.word_word_weight.bias', 'bert.encoder.layer.7.word_word_weight.weight', 'bert.encoder.layer.4.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.1.word_word_weight.weight', 'bert.encoder.layer.11.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.11.word_word_weight.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.9.word_transform.bias', 'bert.encoder.layer.9.word_word_weight.bias', 'bert.encoder.layer.6.word_word_weight.weight', 'bert.encoder.layer.8.word_word_weight.bias', 'bert.encoder.layer.8.word_word_weight.weight', 'word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train:  10%|█         | 5/50 [00:30<04:28,  5.96s/it, F1=0, train_acc=0.572, train_loss=549, train_precision=0, train_recall=0]/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [04:22<00:00,  5.24s/it, F1=0.0389, train_acc=0.893, train_loss=181, train_precision=0.0267, train_recall=0.0739]     \n",
      "Eval Result: 100%|██████████| 35/35 [15:11<00:00, 26.05s/it, F1=0.181, eval_acc=0.963, eval_loss=56.5, eval_precision=0.124, eval_recall=0.331]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [04:33<00:00,  5.46s/it, F1=0.397, train_acc=0.976, train_loss=43.1, train_precision=0.301, train_recall=0.593]\n",
      "Eval Result: 100%|██████████| 35/35 [15:21<00:00, 26.34s/it, F1=0.519, eval_acc=0.979, eval_loss=35.2, eval_precision=0.405, eval_recall=0.727]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [04:34<00:00,  5.49s/it, F1=0.603, train_acc=0.986, train_loss=26.3, train_precision=0.501, train_recall=0.776]\n",
      "Eval Result: 100%|██████████| 35/35 [15:08<00:00, 25.96s/it, F1=0.688, eval_acc=0.984, eval_loss=28.6, eval_precision=0.569, eval_recall=0.872]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [04:35<00:00,  5.51s/it, F1=0.697, train_acc=0.99, train_loss=20.6, train_precision=0.606, train_recall=0.837] \n",
      "Eval Result: 100%|██████████| 35/35 [15:29<00:00, 26.55s/it, F1=0.685, eval_acc=0.985, eval_loss=27.6, eval_precision=0.588, eval_recall=0.823]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [04:44<00:00,  5.69s/it, F1=0.716, train_acc=0.991, train_loss=18.9, train_precision=0.631, train_recall=0.839]\n",
      "Eval Result: 100%|██████████| 35/35 [15:45<00:00, 27.00s/it, F1=0.745, eval_acc=0.986, eval_loss=24.7, eval_precision=0.636, eval_recall=0.9]  \n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [04:43<00:00,  5.67s/it, F1=0.844, train_acc=0.995, train_loss=10.7, train_precision=0.785, train_recall=0.921]\n",
      "Eval Result: 100%|██████████| 35/35 [16:30<00:00, 28.31s/it, F1=0.77, eval_acc=0.987, eval_loss=25.2, eval_precision=0.694, eval_recall=0.864] \n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [04:54<00:00,  5.90s/it, F1=0.885, train_acc=0.996, train_loss=7.95, train_precision=0.837, train_recall=0.944]\n",
      "Eval Result: 100%|██████████| 35/35 [17:01<00:00, 29.17s/it, F1=0.83, eval_acc=0.988, eval_loss=28.9, eval_precision=0.772, eval_recall=0.897] \n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [04:52<00:00,  5.84s/it, F1=0.89, train_acc=0.996, train_loss=7.65, train_precision=0.847, train_recall=0.941] \n",
      "Eval Result: 100%|██████████| 35/35 [17:07<00:00, 29.36s/it, F1=0.748, eval_acc=0.986, eval_loss=31.4, eval_precision=0.683, eval_recall=0.827]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [05:01<00:00,  6.02s/it, F1=0.881, train_acc=0.996, train_loss=7.82, train_precision=0.828, train_recall=0.948]\n",
      "Eval Result: 100%|██████████| 35/35 [16:53<00:00, 28.97s/it, F1=0.811, eval_acc=0.988, eval_loss=25.5, eval_precision=0.752, eval_recall=0.882]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [04:54<00:00,  5.90s/it, F1=0.923, train_acc=0.997, train_loss=5.49, train_precision=0.887, train_recall=0.966]\n",
      "Eval Result: 100%|██████████| 35/35 [17:07<00:00, 29.35s/it, F1=0.831, eval_acc=0.988, eval_loss=23.9, eval_precision=0.764, eval_recall=0.912]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [04:56<00:00,  5.92s/it, F1=0.934, train_acc=0.998, train_loss=4.92, train_precision=0.9, train_recall=0.974]  \n",
      "Eval Result: 100%|██████████| 35/35 [17:37<00:00, 30.22s/it, F1=0.844, eval_acc=0.988, eval_loss=26, eval_precision=0.782, eval_recall=0.917]  \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [05:03<00:00,  6.06s/it, F1=0.932, train_acc=0.998, train_loss=4.77, train_precision=0.9, train_recall=0.969]  \n",
      "Eval Result: 100%|██████████| 35/35 [17:01<00:00, 29.19s/it, F1=0.857, eval_acc=0.988, eval_loss=27.3, eval_precision=0.802, eval_recall=0.92] \n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [05:07<00:00,  6.15s/it, F1=0.958, train_acc=0.999, train_loss=3.51, train_precision=0.937, train_recall=0.982]\n",
      "Eval Result: 100%|██████████| 35/35 [17:06<00:00, 29.34s/it, F1=0.834, eval_acc=0.988, eval_loss=24.9, eval_precision=0.782, eval_recall=0.894]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [04:51<00:00,  5.83s/it, F1=0.959, train_acc=0.999, train_loss=2.65, train_precision=0.94, train_recall=0.98]  \n",
      "Eval Result: 100%|██████████| 35/35 [16:52<00:00, 28.94s/it, F1=0.828, eval_acc=0.988, eval_loss=24.6, eval_precision=0.768, eval_recall=0.9]  \n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [04:57<00:00,  5.96s/it, F1=0.964, train_acc=0.999, train_loss=3.15, train_precision=0.949, train_recall=0.981]\n",
      "Eval Result: 100%|██████████| 35/35 [17:05<00:00, 29.29s/it, F1=0.813, eval_acc=0.988, eval_loss=24.4, eval_precision=0.763, eval_recall=0.871]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [05:04<00:00,  6.09s/it, F1=0.966, train_acc=0.999, train_loss=3.3, train_precision=0.949, train_recall=0.984] \n",
      "Eval Result: 100%|██████████| 35/35 [17:29<00:00, 29.99s/it, F1=0.844, eval_acc=0.988, eval_loss=32.4, eval_precision=0.797, eval_recall=0.896]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [05:04<00:00,  6.09s/it, F1=0.958, train_acc=0.999, train_loss=3.82, train_precision=0.937, train_recall=0.98] \n",
      "Eval Result: 100%|██████████| 35/35 [17:26<00:00, 29.89s/it, F1=0.836, eval_acc=0.988, eval_loss=23.3, eval_precision=0.791, eval_recall=0.887]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [04:45<00:00,  5.72s/it, F1=0.976, train_acc=0.999, train_loss=1.87, train_precision=0.965, train_recall=0.987]\n",
      "Eval Result: 100%|██████████| 35/35 [16:59<00:00, 29.12s/it, F1=0.859, eval_acc=0.988, eval_loss=26.8, eval_precision=0.817, eval_recall=0.907]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [05:02<00:00,  6.05s/it, F1=0.982, train_acc=0.999, train_loss=1.48, train_precision=0.974, train_recall=0.991]\n",
      "Eval Result: 100%|██████████| 35/35 [16:57<00:00, 29.07s/it, F1=0.87, eval_acc=0.989, eval_loss=25.3, eval_precision=0.824, eval_recall=0.923] \n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [05:03<00:00,  6.08s/it, F1=0.988, train_acc=1, train_loss=1.03, train_precision=0.98, train_recall=0.995]      \n",
      "Eval Result: 100%|██████████| 35/35 [17:15<00:00, 29.57s/it, F1=0.877, eval_acc=0.988, eval_loss=30.4, eval_precision=0.84, eval_recall=0.917] \n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [05:12<00:00,  6.25s/it, F1=0.988, train_acc=0.999, train_loss=1.93, train_precision=0.98, train_recall=0.996] \n",
      "Eval Result: 100%|██████████| 35/35 [17:22<00:00, 29.80s/it, F1=0.865, eval_acc=0.988, eval_loss=26.6, eval_precision=0.826, eval_recall=0.909]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [04:46<00:00,  5.73s/it, F1=0.981, train_acc=0.999, train_loss=2.38, train_precision=0.972, train_recall=0.992]\n",
      "Eval Result: 100%|██████████| 35/35 [16:57<00:00, 29.07s/it, F1=0.889, eval_acc=0.988, eval_loss=34.9, eval_precision=0.841, eval_recall=0.944]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [01:29<00:00,  1.79s/it, F1=0.984, train_acc=0.999, train_loss=2.39, train_precision=0.975, train_recall=0.993]\n",
      "Eval Result: 100%|██████████| 35/35 [02:08<00:00,  3.66s/it, F1=0.879, eval_acc=0.988, eval_loss=27.1, eval_precision=0.84, eval_recall=0.921] \n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [01:26<00:00,  1.73s/it, F1=0.989, train_acc=0.999, train_loss=1.23, train_precision=0.984, train_recall=0.994]\n",
      "Eval Result: 100%|██████████| 35/35 [02:16<00:00,  3.89s/it, F1=0.856, eval_acc=0.988, eval_loss=26.7, eval_precision=0.82, eval_recall=0.896] \n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [01:23<00:00,  1.68s/it, F1=0.989, train_acc=1, train_loss=1.1, train_precision=0.982, train_recall=0.995]  \n",
      "Eval Result: 100%|██████████| 35/35 [02:10<00:00,  3.74s/it, F1=0.873, eval_acc=0.988, eval_loss=26.6, eval_precision=0.84, eval_recall=0.909] \n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.74s/it, F1=0.986, train_acc=0.999, train_loss=1.1, train_precision=0.98, train_recall=0.992]  \n",
      "Eval Result: 100%|██████████| 35/35 [02:15<00:00,  3.86s/it, F1=0.857, eval_acc=0.987, eval_loss=26, eval_precision=0.825, eval_recall=0.891]  \n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [01:25<00:00,  1.72s/it, F1=0.992, train_acc=1, train_loss=0.894, train_precision=0.988, train_recall=0.997]    \n",
      "Eval Result: 100%|██████████| 35/35 [02:10<00:00,  3.73s/it, F1=0.886, eval_acc=0.988, eval_loss=31.7, eval_precision=0.844, eval_recall=0.932]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [01:26<00:00,  1.73s/it, F1=0.992, train_acc=1, train_loss=0.966, train_precision=0.988, train_recall=0.997]  \n",
      "Eval Result: 100%|██████████| 35/35 [02:13<00:00,  3.80s/it, F1=0.897, eval_acc=0.989, eval_loss=31.6, eval_precision=0.865, eval_recall=0.931]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [01:25<00:00,  1.71s/it, F1=0.993, train_acc=1, train_loss=0.631, train_precision=0.987, train_recall=0.998]   \n",
      "Eval Result: 100%|██████████| 35/35 [02:14<00:00,  3.86s/it, F1=0.898, eval_acc=0.989, eval_loss=30.9, eval_precision=0.869, eval_recall=0.929]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.75s/it, F1=0.995, train_acc=1, train_loss=0.445, train_precision=0.991, train_recall=0.999]   \n",
      "Eval Result: 100%|██████████| 35/35 [02:18<00:00,  3.97s/it, F1=0.903, eval_acc=0.989, eval_loss=32.2, eval_precision=0.875, eval_recall=0.932]\n"
     ]
    }
   ],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj_json/train_400.json\",\n",
      "    \"eval_file\": \"./data/FN/fj_json/dev.json\",\n",
      "    \"test_file\": \"./data/FN/fj_json/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/FN_medicine_vocab.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 3000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"zl_v2_fj_tx_inter_fn3000_4\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj_json/train_400.json etag: 100%|██████████| 1.76M/1.76M [00:00<00:00, 304MB/s]\n",
      "calculate ./data/FN/fj_json/dev.json etag: 100%|██████████| 9.75M/9.75M [00:00<00:00, 342MB/s]\n",
      "calculate ./data/FN/fj_json/test.json etag: 100%|██████████| 9.64M/9.64M [00:00<00:00, 370MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 45.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 8780.82L/s]\n",
      "build line mapper: 3L [00:00, 4658.61L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 878.57it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 46431.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/fj_json/train_400.json: 100%|██████████| 400/400 [00:02<00:00, 139.53it/s]\n",
      "load dataset from ./data/FN/fj_json/dev.json: 100%|██████████| 2202/2202 [00:15<00:00, 138.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel_v2: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel_v2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel_v2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel_v2 were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.3.word_word_weight.bias', 'bert.encoder.layer.10.word_word_weight.weight', 'bert.encoder.layer.10.word_transform.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.4.word_word_weight.weight', 'bert.encoder.layer.5.word_transform.bias', 'bert.encoder.layer.3.word_transform.weight', 'bert.encoder.layer.5.word_transform.weight', 'bert.encoder.layer.9.word_transform.weight', 'bert.encoder.layer.11.word_word_weight.bias', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.11.word_transform.weight', 'bert.encoder.layer.3.word_transform.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.10.word_word_weight.bias', 'bert.encoder.layer.1.word_transform.bias', 'bert.encoder.layer.2.word_transform.weight', 'bert.encoder.layer.5.word_word_weight.bias', 'bert.encoder.layer.7.word_transform.bias', 'bert.encoder.layer.6.word_word_weight.bias', 'bert.encoder.layer.2.word_word_weight.weight', 'bert.encoder.layer.6.word_transform.weight', 'bert.encoder.layer.7.word_transform.weight', 'bert.encoder.layer.1.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.9.word_word_weight.weight', 'bert.encoder.layer.4.word_word_weight.bias', 'bert.encoder.layer.2.word_word_weight.bias', 'bert.encoder.layer.8.word_transform.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.2.word_transform.bias', 'bert.encoder.layer.1.word_transform.weight', 'bert.encoder.layer.4.word_transform.weight', 'bert.encoder.layer.6.word_transform.bias', 'bert.encoder.layer.10.word_transform.bias', 'bert.encoder.layer.5.word_word_weight.weight', 'bert.encoder.layer.3.word_word_weight.weight', 'bert.encoder.layer.8.word_transform.weight', 'bert.encoder.layer.7.word_word_weight.bias', 'bert.encoder.layer.7.word_word_weight.weight', 'bert.encoder.layer.4.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.1.word_word_weight.weight', 'bert.encoder.layer.11.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.11.word_word_weight.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.9.word_transform.bias', 'bert.encoder.layer.9.word_word_weight.bias', 'bert.encoder.layer.6.word_word_weight.weight', 'bert.encoder.layer.8.word_word_weight.bias', 'bert.encoder.layer.8.word_word_weight.weight', 'word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [01:25<00:00,  1.71s/it, F1=0.287, train_acc=0.931, train_loss=56.2, train_precision=0.333, train_recall=0.262]   \n",
      "Eval Result: 100%|██████████| 35/35 [02:13<00:00,  3.80s/it, F1=0.776, eval_acc=0.977, eval_loss=16.3, eval_precision=0.75, eval_recall=0.805] \n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [01:26<00:00,  1.74s/it, F1=0.834, train_acc=0.984, train_loss=12, train_precision=0.827, train_recall=0.844]  \n",
      "Eval Result: 100%|██████████| 35/35 [02:13<00:00,  3.80s/it, F1=0.857, eval_acc=0.979, eval_loss=14.9, eval_precision=0.805, eval_recall=0.917]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [01:26<00:00,  1.74s/it, F1=0.899, train_acc=0.989, train_loss=8.11, train_precision=0.887, train_recall=0.916]\n",
      "Eval Result: 100%|██████████| 35/35 [02:13<00:00,  3.82s/it, F1=0.89, eval_acc=0.983, eval_loss=10.6, eval_precision=0.846, eval_recall=0.939] \n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [01:31<00:00,  1.84s/it, F1=0.94, train_acc=0.993, train_loss=6.1, train_precision=0.927, train_recall=0.956]  \n",
      "Eval Result: 100%|██████████| 35/35 [02:23<00:00,  4.11s/it, F1=0.91, eval_acc=0.986, eval_loss=8.37, eval_precision=0.877, eval_recall=0.947] \n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.75s/it, F1=0.962, train_acc=0.995, train_loss=4.03, train_precision=0.951, train_recall=0.973]\n",
      "Eval Result: 100%|██████████| 35/35 [02:28<00:00,  4.23s/it, F1=0.906, eval_acc=0.986, eval_loss=8.64, eval_precision=0.872, eval_recall=0.943]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [01:26<00:00,  1.74s/it, F1=0.966, train_acc=0.996, train_loss=3.35, train_precision=0.959, train_recall=0.974]\n",
      "Eval Result: 100%|██████████| 35/35 [02:10<00:00,  3.72s/it, F1=0.904, eval_acc=0.984, eval_loss=12, eval_precision=0.859, eval_recall=0.954]  \n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [01:24<00:00,  1.70s/it, F1=0.965, train_acc=0.996, train_loss=3.07, train_precision=0.955, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 35/35 [02:17<00:00,  3.92s/it, F1=0.923, eval_acc=0.988, eval_loss=7.19, eval_precision=0.892, eval_recall=0.956]\n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [01:31<00:00,  1.83s/it, F1=0.973, train_acc=0.997, train_loss=2.46, train_precision=0.964, train_recall=0.983]\n",
      "Eval Result: 100%|██████████| 35/35 [02:22<00:00,  4.07s/it, F1=0.919, eval_acc=0.987, eval_loss=10.9, eval_precision=0.891, eval_recall=0.95] \n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [01:30<00:00,  1.80s/it, F1=0.98, train_acc=0.998, train_loss=2.45, train_precision=0.973, train_recall=0.988] \n",
      "Eval Result: 100%|██████████| 35/35 [02:16<00:00,  3.90s/it, F1=0.915, eval_acc=0.989, eval_loss=6.31, eval_precision=0.896, eval_recall=0.936]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.74s/it, F1=0.978, train_acc=0.997, train_loss=2.43, train_precision=0.975, train_recall=0.981]\n",
      "Eval Result: 100%|██████████| 35/35 [02:15<00:00,  3.88s/it, F1=0.923, eval_acc=0.988, eval_loss=8.5, eval_precision=0.9, eval_recall=0.948]   \n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [01:26<00:00,  1.73s/it, F1=0.983, train_acc=0.998, train_loss=1.65, train_precision=0.977, train_recall=0.99] \n",
      "Eval Result: 100%|██████████| 35/35 [02:10<00:00,  3.72s/it, F1=0.927, eval_acc=0.988, eval_loss=7.63, eval_precision=0.9, eval_recall=0.955]  \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.75s/it, F1=0.99, train_acc=0.999, train_loss=1.1, train_precision=0.984, train_recall=0.996]  \n",
      "Eval Result: 100%|██████████| 35/35 [02:13<00:00,  3.81s/it, F1=0.923, eval_acc=0.988, eval_loss=8.09, eval_precision=0.901, eval_recall=0.946]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [01:29<00:00,  1.79s/it, F1=0.987, train_acc=0.999, train_loss=1.29, train_precision=0.984, train_recall=0.99] \n",
      "Eval Result: 100%|██████████| 35/35 [02:14<00:00,  3.83s/it, F1=0.922, eval_acc=0.988, eval_loss=8.35, eval_precision=0.898, eval_recall=0.948]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [01:24<00:00,  1.69s/it, F1=0.989, train_acc=0.999, train_loss=1.12, train_precision=0.987, train_recall=0.992] \n",
      "Eval Result: 100%|██████████| 35/35 [02:18<00:00,  3.97s/it, F1=0.921, eval_acc=0.987, eval_loss=10.3, eval_precision=0.889, eval_recall=0.955]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [01:32<00:00,  1.86s/it, F1=0.991, train_acc=0.999, train_loss=0.894, train_precision=0.987, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 35/35 [02:18<00:00,  3.97s/it, F1=0.901, eval_acc=0.983, eval_loss=21.6, eval_precision=0.856, eval_recall=0.953]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [01:30<00:00,  1.82s/it, F1=0.994, train_acc=0.999, train_loss=0.851, train_precision=0.991, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 35/35 [02:18<00:00,  3.97s/it, F1=0.925, eval_acc=0.988, eval_loss=11.5, eval_precision=0.895, eval_recall=0.956]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.74s/it, F1=0.993, train_acc=0.999, train_loss=0.808, train_precision=0.991, train_recall=0.995]\n",
      "Eval Result: 100%|██████████| 35/35 [02:16<00:00,  3.89s/it, F1=0.924, eval_acc=0.989, eval_loss=7.62, eval_precision=0.901, eval_recall=0.949]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [01:26<00:00,  1.72s/it, F1=0.991, train_acc=0.999, train_loss=1.26, train_precision=0.988, train_recall=0.995] \n",
      "Eval Result: 100%|██████████| 35/35 [02:11<00:00,  3.74s/it, F1=0.915, eval_acc=0.985, eval_loss=13.8, eval_precision=0.878, eval_recall=0.955]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [01:25<00:00,  1.71s/it, F1=0.995, train_acc=1, train_loss=0.738, train_precision=0.993, train_recall=0.998]    \n",
      "Eval Result: 100%|██████████| 35/35 [02:08<00:00,  3.68s/it, F1=0.923, eval_acc=0.989, eval_loss=7.27, eval_precision=0.899, eval_recall=0.949]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [01:23<00:00,  1.68s/it, F1=0.994, train_acc=0.999, train_loss=0.8, train_precision=0.99, train_recall=0.997]   \n",
      "Eval Result: 100%|██████████| 35/35 [02:09<00:00,  3.71s/it, F1=0.92, eval_acc=0.989, eval_loss=8.78, eval_precision=0.891, eval_recall=0.95]  \n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [01:30<00:00,  1.81s/it, F1=0.994, train_acc=0.999, train_loss=0.693, train_precision=0.992, train_recall=0.995]\n",
      "Eval Result: 100%|██████████| 35/35 [02:12<00:00,  3.78s/it, F1=0.919, eval_acc=0.988, eval_loss=9.44, eval_precision=0.888, eval_recall=0.953]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [01:25<00:00,  1.70s/it, F1=0.995, train_acc=1, train_loss=0.695, train_precision=0.992, train_recall=0.998]    \n",
      "Eval Result: 100%|██████████| 35/35 [02:17<00:00,  3.93s/it, F1=0.916, eval_acc=0.987, eval_loss=12, eval_precision=0.881, eval_recall=0.955]  \n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [01:28<00:00,  1.77s/it, F1=0.994, train_acc=1, train_loss=0.594, train_precision=0.992, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 35/35 [02:14<00:00,  3.84s/it, F1=0.92, eval_acc=0.988, eval_loss=10.1, eval_precision=0.893, eval_recall=0.949] \n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.75s/it, F1=0.994, train_acc=0.999, train_loss=0.671, train_precision=0.992, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 35/35 [02:06<00:00,  3.63s/it, F1=0.914, eval_acc=0.989, eval_loss=8.76, eval_precision=0.891, eval_recall=0.937]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [01:27<00:00,  1.74s/it, F1=0.993, train_acc=0.999, train_loss=0.628, train_precision=0.991, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 35/35 [02:17<00:00,  3.92s/it, F1=0.915, eval_acc=0.986, eval_loss=13.8, eval_precision=0.881, eval_recall=0.951]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [01:29<00:00,  1.79s/it, F1=0.996, train_acc=1, train_loss=0.6, train_precision=0.993, train_recall=0.998]      \n",
      "Eval Result: 100%|██████████| 35/35 [02:13<00:00,  3.82s/it, F1=0.925, eval_acc=0.988, eval_loss=11.6, eval_precision=0.902, eval_recall=0.949]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [01:25<00:00,  1.72s/it, F1=0.994, train_acc=1, train_loss=0.897, train_precision=0.991, train_recall=0.998]    \n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.70s/it, F1=0.924, eval_acc=0.988, eval_loss=9.66, eval_precision=0.898, eval_recall=0.952]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.46s/it, F1=0.994, train_acc=0.999, train_loss=0.699, train_precision=0.992, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.71s/it, F1=0.928, eval_acc=0.989, eval_loss=11.5, eval_precision=0.902, eval_recall=0.955]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.46s/it, F1=0.993, train_acc=0.999, train_loss=0.526, train_precision=0.991, train_recall=0.995]\n",
      "Eval Result: 100%|██████████| 35/35 [01:33<00:00,  2.68s/it, F1=0.925, eval_acc=0.987, eval_loss=13.2, eval_precision=0.902, eval_recall=0.95] \n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.44s/it, F1=0.995, train_acc=1, train_loss=0.467, train_precision=0.994, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.69s/it, F1=0.905, eval_acc=0.984, eval_loss=21.5, eval_precision=0.858, eval_recall=0.957]\n"
     ]
    }
   ],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj_json/train_400.json\",\n",
      "    \"eval_file\": \"./data/FN/fj_json/dev.json\",\n",
      "    \"test_file\": \"./data/FN/fj_json/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/FN_medicine_vocab.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 3000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"zl_v2_fj_tx_inter_fn3000_5\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj_json/train_400.json etag: 100%|██████████| 1.76M/1.76M [00:00<00:00, 319MB/s]\n",
      "calculate ./data/FN/fj_json/dev.json etag: 100%|██████████| 9.75M/9.75M [00:00<00:00, 342MB/s]\n",
      "calculate ./data/FN/fj_json/test.json etag: 100%|██████████| 9.64M/9.64M [00:00<00:00, 373MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 51.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 7182.03L/s]\n",
      "build line mapper: 3L [00:00, 4813.66L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 792.03it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 8962.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/bd6f3bfd51d11354dbac9caa453be84d_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/fj_json/train_400.json: 100%|██████████| 400/400 [00:02<00:00, 140.64it/s]\n",
      "load dataset from ./data/FN/fj_json/dev.json: 100%|██████████| 2202/2202 [00:15<00:00, 139.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel_v2: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel_v2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel_v2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel_v2 were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.3.word_word_weight.bias', 'bert.encoder.layer.10.word_word_weight.weight', 'bert.encoder.layer.10.word_transform.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.4.word_word_weight.weight', 'bert.encoder.layer.5.word_transform.bias', 'bert.encoder.layer.3.word_transform.weight', 'bert.encoder.layer.5.word_transform.weight', 'bert.encoder.layer.9.word_transform.weight', 'bert.encoder.layer.11.word_word_weight.bias', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.11.word_transform.weight', 'bert.encoder.layer.3.word_transform.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.10.word_word_weight.bias', 'bert.encoder.layer.1.word_transform.bias', 'bert.encoder.layer.2.word_transform.weight', 'bert.encoder.layer.5.word_word_weight.bias', 'bert.encoder.layer.7.word_transform.bias', 'bert.encoder.layer.6.word_word_weight.bias', 'bert.encoder.layer.2.word_word_weight.weight', 'bert.encoder.layer.6.word_transform.weight', 'bert.encoder.layer.7.word_transform.weight', 'bert.encoder.layer.1.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.9.word_word_weight.weight', 'bert.encoder.layer.4.word_word_weight.bias', 'bert.encoder.layer.2.word_word_weight.bias', 'bert.encoder.layer.8.word_transform.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.2.word_transform.bias', 'bert.encoder.layer.1.word_transform.weight', 'bert.encoder.layer.4.word_transform.weight', 'bert.encoder.layer.6.word_transform.bias', 'bert.encoder.layer.10.word_transform.bias', 'bert.encoder.layer.5.word_word_weight.weight', 'bert.encoder.layer.3.word_word_weight.weight', 'bert.encoder.layer.8.word_transform.weight', 'bert.encoder.layer.7.word_word_weight.bias', 'bert.encoder.layer.7.word_word_weight.weight', 'bert.encoder.layer.4.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.1.word_word_weight.weight', 'bert.encoder.layer.11.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.11.word_word_weight.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.9.word_transform.bias', 'bert.encoder.layer.9.word_word_weight.bias', 'bert.encoder.layer.6.word_word_weight.weight', 'bert.encoder.layer.8.word_word_weight.bias', 'bert.encoder.layer.8.word_word_weight.weight', 'word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.43s/it, F1=0.14, train_acc=0.912, train_loss=82, train_precision=0.171, train_recall=0.124]        \n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.77s/it, F1=0.553, eval_acc=0.968, eval_loss=22.9, eval_precision=0.512, eval_recall=0.602]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.45s/it, F1=0.733, train_acc=0.979, train_loss=16.3, train_precision=0.719, train_recall=0.755]\n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.75s/it, F1=0.841, eval_acc=0.983, eval_loss=10.6, eval_precision=0.813, eval_recall=0.871]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.47s/it, F1=0.885, train_acc=0.989, train_loss=8.33, train_precision=0.862, train_recall=0.91] \n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.76s/it, F1=0.87, eval_acc=0.985, eval_loss=8.22, eval_precision=0.873, eval_recall=0.867] \n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.45s/it, F1=0.924, train_acc=0.992, train_loss=6.39, train_precision=0.904, train_recall=0.946]\n",
      "Eval Result: 100%|██████████| 35/35 [01:39<00:00,  2.83s/it, F1=0.903, eval_acc=0.987, eval_loss=8.08, eval_precision=0.877, eval_recall=0.931]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.47s/it, F1=0.932, train_acc=0.993, train_loss=5.12, train_precision=0.917, train_recall=0.949]\n",
      "Eval Result: 100%|██████████| 35/35 [01:37<00:00,  2.79s/it, F1=0.875, eval_acc=0.984, eval_loss=8.83, eval_precision=0.878, eval_recall=0.871]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.44s/it, F1=0.942, train_acc=0.995, train_loss=4.62, train_precision=0.93, train_recall=0.956] \n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.71s/it, F1=0.886, eval_acc=0.985, eval_loss=6.94, eval_precision=0.887, eval_recall=0.884]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.43s/it, F1=0.96, train_acc=0.996, train_loss=3.57, train_precision=0.947, train_recall=0.975] \n",
      "Eval Result: 100%|██████████| 35/35 [01:30<00:00,  2.59s/it, F1=0.912, eval_acc=0.988, eval_loss=6.06, eval_precision=0.904, eval_recall=0.921]\n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [01:10<00:00,  1.41s/it, F1=0.968, train_acc=0.996, train_loss=2.6, train_precision=0.959, train_recall=0.978] \n",
      "Eval Result: 100%|██████████| 35/35 [01:31<00:00,  2.62s/it, F1=0.921, eval_acc=0.988, eval_loss=6.98, eval_precision=0.909, eval_recall=0.934]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.45s/it, F1=0.974, train_acc=0.998, train_loss=1.86, train_precision=0.967, train_recall=0.982]\n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.71s/it, F1=0.917, eval_acc=0.989, eval_loss=8.88, eval_precision=0.897, eval_recall=0.937]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [01:14<00:00,  1.49s/it, F1=0.982, train_acc=0.998, train_loss=1.56, train_precision=0.973, train_recall=0.99] \n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.75s/it, F1=0.901, eval_acc=0.987, eval_loss=10.6, eval_precision=0.896, eval_recall=0.906]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.47s/it, F1=0.982, train_acc=0.998, train_loss=1.46, train_precision=0.973, train_recall=0.991]\n",
      "Eval Result: 100%|██████████| 35/35 [01:35<00:00,  2.73s/it, F1=0.921, eval_acc=0.989, eval_loss=8.14, eval_precision=0.896, eval_recall=0.947]\n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.43s/it, F1=0.989, train_acc=0.999, train_loss=1.08, train_precision=0.982, train_recall=0.996] \n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.74s/it, F1=0.919, eval_acc=0.989, eval_loss=8.33, eval_precision=0.895, eval_recall=0.946]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [01:14<00:00,  1.49s/it, F1=0.989, train_acc=0.999, train_loss=0.905, train_precision=0.984, train_recall=0.994]\n",
      "Eval Result: 100%|██████████| 35/35 [01:35<00:00,  2.72s/it, F1=0.912, eval_acc=0.987, eval_loss=10.6, eval_precision=0.915, eval_recall=0.908]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.46s/it, F1=0.99, train_acc=0.999, train_loss=0.755, train_precision=0.985, train_recall=0.995] \n",
      "Eval Result: 100%|██████████| 35/35 [01:35<00:00,  2.74s/it, F1=0.922, eval_acc=0.989, eval_loss=12.2, eval_precision=0.899, eval_recall=0.946]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.48s/it, F1=0.993, train_acc=0.999, train_loss=0.586, train_precision=0.988, train_recall=0.999]\n",
      "Eval Result: 100%|██████████| 35/35 [01:31<00:00,  2.61s/it, F1=0.911, eval_acc=0.987, eval_loss=11.5, eval_precision=0.905, eval_recall=0.917]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.44s/it, F1=0.994, train_acc=0.999, train_loss=0.484, train_precision=0.991, train_recall=0.998]\n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.77s/it, F1=0.914, eval_acc=0.988, eval_loss=11.6, eval_precision=0.894, eval_recall=0.934]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.46s/it, F1=0.994, train_acc=0.999, train_loss=0.924, train_precision=0.991, train_recall=0.998]\n",
      "Eval Result: 100%|██████████| 35/35 [01:38<00:00,  2.82s/it, F1=0.878, eval_acc=0.985, eval_loss=11, eval_precision=0.89, eval_recall=0.866]   \n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.47s/it, F1=0.992, train_acc=0.999, train_loss=1.47, train_precision=0.988, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 35/35 [01:35<00:00,  2.73s/it, F1=0.909, eval_acc=0.987, eval_loss=7.88, eval_precision=0.899, eval_recall=0.92] \n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.47s/it, F1=0.992, train_acc=0.999, train_loss=1.19, train_precision=0.989, train_recall=0.995]\n",
      "Eval Result: 100%|██████████| 35/35 [01:31<00:00,  2.62s/it, F1=0.909, eval_acc=0.987, eval_loss=8.63, eval_precision=0.903, eval_recall=0.916]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.42s/it, F1=0.991, train_acc=0.999, train_loss=1.09, train_precision=0.988, train_recall=0.995] \n",
      "Eval Result: 100%|██████████| 35/35 [01:31<00:00,  2.60s/it, F1=0.917, eval_acc=0.988, eval_loss=12.1, eval_precision=0.889, eval_recall=0.947]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.44s/it, F1=0.993, train_acc=0.999, train_loss=0.814, train_precision=0.989, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 35/35 [01:33<00:00,  2.66s/it, F1=0.917, eval_acc=0.988, eval_loss=9.33, eval_precision=0.905, eval_recall=0.929]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.47s/it, F1=0.993, train_acc=0.999, train_loss=0.659, train_precision=0.99, train_recall=0.996] \n",
      "Eval Result: 100%|██████████| 35/35 [01:35<00:00,  2.74s/it, F1=0.915, eval_acc=0.989, eval_loss=10.7, eval_precision=0.906, eval_recall=0.925]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [01:16<00:00,  1.53s/it, F1=0.994, train_acc=0.999, train_loss=0.954, train_precision=0.991, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 35/35 [01:35<00:00,  2.72s/it, F1=0.908, eval_acc=0.987, eval_loss=12, eval_precision=0.916, eval_recall=0.9]    \n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.46s/it, F1=0.996, train_acc=1, train_loss=0.581, train_precision=0.993, train_recall=0.998]\n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.70s/it, F1=0.922, eval_acc=0.989, eval_loss=11.4, eval_precision=0.909, eval_recall=0.935]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [01:11<00:00,  1.43s/it, F1=0.993, train_acc=0.999, train_loss=0.795, train_precision=0.99, train_recall=0.996] \n",
      "Eval Result: 100%|██████████| 35/35 [01:35<00:00,  2.71s/it, F1=0.917, eval_acc=0.987, eval_loss=13.1, eval_precision=0.881, eval_recall=0.956]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.45s/it, F1=0.996, train_acc=1, train_loss=0.535, train_precision=0.994, train_recall=0.998]    \n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.75s/it, F1=0.924, eval_acc=0.989, eval_loss=10.8, eval_precision=0.905, eval_recall=0.944]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.46s/it, F1=0.997, train_acc=1, train_loss=0.253, train_precision=0.995, train_recall=1]\n",
      "Eval Result: 100%|██████████| 35/35 [01:36<00:00,  2.75s/it, F1=0.921, eval_acc=0.989, eval_loss=13.5, eval_precision=0.895, eval_recall=0.949]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.45s/it, F1=0.996, train_acc=1, train_loss=0.255, train_precision=0.995, train_recall=0.998]\n",
      "Eval Result: 100%|██████████| 35/35 [01:32<00:00,  2.65s/it, F1=0.926, eval_acc=0.989, eval_loss=12.7, eval_precision=0.907, eval_recall=0.945]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [01:12<00:00,  1.46s/it, F1=0.995, train_acc=1, train_loss=0.362, train_precision=0.993, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.70s/it, F1=0.921, eval_acc=0.988, eval_loss=14.9, eval_precision=0.894, eval_recall=0.951]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [01:13<00:00,  1.46s/it, F1=0.997, train_acc=1, train_loss=0.179, train_precision=0.994, train_recall=0.999]\n",
      "Eval Result: 100%|██████████| 35/35 [01:34<00:00,  2.69s/it, F1=0.923, eval_acc=0.989, eval_loss=16.3, eval_precision=0.9, eval_recall=0.948]  \n"
     ]
    }
   ],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccks\n",
    "# ccks_baseline 0.8258 0.8197 \n",
    "# ccks_medicine_vocab_baseline 0.8149 0.8120\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v2',\n",
    "    'task_name': 'zl_v2_ccks_tx_inter_fn3000_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b5eafe76d1bce12a884379452caff4a891b716fe38e21da9ac26a4bcd47e2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('bert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
