{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/FN_medicine_vocab.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 3470,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"zl_sc_tx_inter_fn_base\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 320MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 364MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 364MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 63.5kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 7525.67L/s]\n",
      "build line mapper: 3L [00:00, 23003.50L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 866.23it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 33916.20it/s]\n",
      "/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:04<00:00, 97.91it/s] \n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:01<00:00, 97.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['inter_word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.embeddings.position_ids', 'word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train:   2%|▏         | 1/50 [00:09<07:31,  9.22s/it, F1=0, train_acc=0.933, train_loss=341, train_precision=0, train_recall=0]/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.00872, train_acc=0.907, train_loss=176, train_precision=0.00975, train_recall=0.00804]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.0343, eval_acc=0.937, eval_loss=95.7, eval_precision=0.0411, eval_recall=0.0295]\n",
      "Epoch: 2/30 Train:  32%|███▏      | 16/50 [00:14<00:31,  1.07it/s, F1=0.0937, train_acc=0.93, train_loss=99.6, train_precision=0.116, train_recall=0.081]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1672415/4226505901.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNERTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/trainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **arg)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_path, resume_step, lr1, lr2)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update learning rate schedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NER/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NER/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NER/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NER/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                     eps=group['eps'])\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NER/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 四川\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3470,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'task_name': 'zl_sc_tx_inter_fn_base'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj_json/train_400.json\",\n",
      "    \"eval_file\": \"./data/FN/fj_json/dev.json\",\n",
      "    \"test_file\": \"./data/FN/fj_json/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/FN_medicine_vocab.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 3470,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"zl_fj_tx_inter_fn_base_1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj_json/train_400.json etag: 100%|██████████| 1.76M/1.76M [00:00<00:00, 304MB/s]\n",
      "calculate ./data/FN/fj_json/dev.json etag: 100%|██████████| 9.75M/9.75M [00:00<00:00, 348MB/s]\n",
      "calculate ./data/FN/fj_json/test.json etag: 100%|██████████| 9.64M/9.64M [00:00<00:00, 373MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 60.2kB/s]\n",
      "count line size ./data/tencent/tencent_vocab.txt: 8824306L [00:00, 27854889.23L/s]\n",
      "build line mapper: 8824306L [00:03, 2865391.82L/s][00:00<?, ?it/s]\n",
      "load vocabs into trie: 100%|██████████| 1000000/1000000 [00:14<00:00, 69092.99it/s]\n",
      "build trie: 100%|██████████| 1000000/1000000 [00:17<00:00, 58338.80it/s]\n",
      "count line size ./data/FN/fj_json/train_400.json: 401L [00:00, 258477.93L/s]\n",
      "load dataset matched word: 100%|█████████▉| 400/401 [00:00<00:00, 601.18it/s]\n",
      "count line size ./data/FN/fj_json/dev.json: 2203L [00:00, 316050.48L/s]\n",
      "load dataset matched word: 100%|█████████▉| 2202/2203 [00:03<00:00, 602.09it/s]\n",
      "count line size ./data/FN/fj_json/test.json: 2204L [00:00, 321684.45L/s]\n",
      "load dataset matched word: 100%|█████████▉| 2203/2204 [00:03<00:00, 608.40it/s]\n",
      "load vocab from list: 100%|██████████| 8118/8118 [00:00<00:00, 747335.66it/s]\n",
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 9184.61L/s]\n",
      "build line mapper: 3L [00:00, 5271.43L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 815.27it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 29676.68it/s]\n",
      "count line size ./data/tencent/word_embedding.txt: 8824332L [00:11, 774555.47L/s]\n",
      "build line mapper: 8824332L [00:25, 349304.49L/s]0 [00:00<?, ?it/s]\n",
      "load word embedding...:   1%|          | 6091/1000000 [00:25<40:09, 412.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.073052', '-0.454791', '-0.136675', '0.258515', '0.011885', '0.041317', '0.041896', '0.128609', '-0.163152', '0.246874', '0.092835', '-0.126660', '0.178079', '-0.286865', '0.027544', '-0.119339', '-0.007087', '-0.189252', '0.377924', '0.023270', '-0.367283', '-0.070051', '0.030247', '0.319996', '0.058990', '0.139447', '-0.126480', '0.268859', '-0.023930', '0.227637', '0.044645', '0.301265', '-0.234713', '0.074171', '-0.306097', '-0.403798', '0.140713', '-0.101598', '0.047656', '-0.276438', '-0.080730', '0.143143', '0.189617', '0.129756', '0.185805', '0.133780', '-0.318322', '-0.483692', '0.408157', '0.031308', '-0.311013', '0.059570', '-0.224590', '0.414112', '0.384909', '-0.305530', '0.131048', '0.119147', '0.059128', '-0.248051', '-0.138340', '-0.000503', '-0.100918', '0.304257', '-0.368741', '0.131073', '-0.237684', '-0.029870', '-0.244446', '-0.151965', '-0.221822', '0.489918', '0.114530', '0.175746', '-0.040124', '-0.295019', '-0.063676', '0.157994', '-0.011577', '-0.144796', '0.072814', '0.670234', '0.158069', '0.444338', '0.187207', '-0.178194', '0.441387', '-0.113229', '-0.311998', '-0.262850', '0.166731', '-0.074662', '-0.120234', '-0.557770', '0.217205', '-0.100354', '-0.315967', '-0.199034', '0.044077', '0.072797', '-0.041775', '0.057079', '0.252978', '0.155448', '-0.151973', '0.304786', '0.286322', '-0.461290', '-0.391043', '-0.356367', '0.050163', '-0.301053', '-0.258514', '0.085645', '0.219664', '-0.164307', '0.024513', '-0.077458', '-0.200825', '-0.418520', '0.258938', '-0.140829', '0.152272', '0.176600', '0.340460', '0.357031', '-0.039533', '0.113248', '0.152953', '-0.136520', '-0.167888', '-0.033082', '0.532537', '-0.086386', '-0.234421', '-0.337486', '-0.016323', '-0.018524', '-0.000276', '-0.252389', '-0.395252', '-0.324214', '-0.259109', '0.308163', '0.353517', '-0.195429', '0.319425', '-0.014722', '0.196231', '0.051138', '-0.161133', '-0.056247', '0.262345', '-0.192682', '0.272561', '0.010028', '0.139295', '-0.224138', '0.507957', '0.109516', '0.273037', '0.105556', '-0.182561', '0.069247', '0.102232', '-0.009683', '-0.127744', '0.224196', '-0.027222', '-0.057221', '0.096212', '0.126842', '-0.309821', '-0.031813', '0.135091', '-0.046443', '0.000263', '0.098762', '0.394687', '-0.081866', '0.021604', '0.033224', '-0.121693', '0.001640', '-0.080564', '0.109319', '-0.128558', '0.327593', '0.000599', '0.335171', '0.104823', '0.229730', '0.158929', '-0.203982', '0.355367', '0.507303', '-0.173355', '-0.388700', '-0.324610', '0.296036'] embedding error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load word embedding...: 100%|██████████| 1000000/1000000 [00:57<00:00, 17506.45it/s]\n",
      "load vocab embedding: 8125it [00:00, 12212.49it/s]\n",
      "count line size ./data/tencent/FN_medicine_vocab.txt: 3470L [00:00, 8632405.03L/s]\n",
      "build line mapper: 3470L [00:00, 2048161.40L/s][00:00<?, ?it/s]\n",
      "load vocabs into trie:  64%|██████▍   | 2227/3470 [00:00<00:00, 74819.09it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1567289/4107521337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m'task_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'zl_fj_tx_inter_fn_base_1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m }\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNERTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_gpus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/trainer.py\u001b[0m in \u001b[0;36mdataloader_init\u001b[0;34m(self, **args)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataloader_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **args)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataloader not found name '{}'.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/loaders/le_loader_zl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./temp/{self.cache_key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# self.inter_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/loaders/le_loader_zl.py\u001b[0m in \u001b[0;36mread_data_set\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# 外部知识\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         self.inter_lexicon_tree = cache.load(\"inter_lexicon_tree\",lambda: TrieFactory.get_trie_from_vocabs(\n\u001b[0m\u001b[1;32m     77\u001b[0m             [self.inter_knowledge_file], self.inter_max_scan_num))\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/loaders/utils/cache_manager.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, key, construct)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconstruct\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/loaders/le_loader_zl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# 外部知识\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         self.inter_lexicon_tree = cache.load(\"inter_lexicon_tree\",lambda: TrieFactory.get_trie_from_vocabs(\n\u001b[0;32m---> 77\u001b[0;31m             [self.inter_knowledge_file], self.inter_max_scan_num))\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.inter_matched_words = cache.load(\"inter_matched_words\",lambda: TrieFactory.get_all_matched_word_from_dataset(\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/loaders/utils/lexicon_factory.py\u001b[0m in \u001b[0;36mget_trie_from_vocabs\u001b[0;34m(vocab_files, max_line)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlexicon_tree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#福建\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'task_name': 'zl_fj_tx_inter_fn_base_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn_base_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn_base_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn_base_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_fj_tx_inter_fn_base_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b5eafe76d1bce12a884379452caff4a891b716fe38e21da9ac26a4bcd47e2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('bert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
