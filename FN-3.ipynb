{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 四川\n",
    "# from CC.trainer import NERTrainer\n",
    "# args = {\n",
    "#     'num_epochs': 30,\n",
    "#     'num_gpus': [0, 1],\n",
    "#     'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "#     # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "#     'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "#     # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "#     'hidden_dim': 300,\n",
    "#     'max_seq_length': 512,\n",
    "#     'max_scan_num': 1000000,\n",
    "#     'inter_max_scan_num': 3000,\n",
    "#     # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "#     'train_file': './data/FN/sc_json_500/train.json',\n",
    "#     'eval_file': './data/FN/sc_test100/test.json',\n",
    "#     'test_file': './data/FN/sc_test100/test.json',\n",
    "#     'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "#     'tag_file': './data/FN/tags_list.txt',\n",
    "#     'loader_name': 'le_loader_zl',\n",
    "#     # 'loader_name': 'le_loader',\n",
    "#     'output_eval':True,\n",
    "#     \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "#     \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "#     \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "#     \"default_tag\":\"O\",\n",
    "#     'batch_size': 8,\n",
    "#     'eval_batch_size': 64,\n",
    "#     'do_shuffle': True,\n",
    "#     \"use_gpu\": True,\n",
    "#     \"debug\": True,\n",
    "#     'model_name': 'ZLEBert',\n",
    "#     'task_name': 'zl_sc_tx_inter_fn3000_ccks_1'\n",
    "# }\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #福建\n",
    "# from CC.trainer import NERTrainer\n",
    "# args = {\n",
    "#     'num_epochs': 30,\n",
    "#     'num_gpus': [0, 1],\n",
    "#     'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "#     # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "#     'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "#     # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "#     'hidden_dim': 3000,\n",
    "#     'max_seq_length': 512,\n",
    "#     'max_scan_num': 1000000,\n",
    "#     'inter_max_scan_num': 3000,\n",
    "#     # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "#     'train_file': './data/FN/fj_json/train_400.json',\n",
    "#     'eval_file': './data/FN/fj_json/dev.json',\n",
    "#     'test_file': './data/FN/fj_json/test.json',\n",
    "#     'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "#     'tag_file': './data/FN/tags_list.txt',\n",
    "#     'loader_name': 'le_loader_zl',\n",
    "#     # 'loader_name': 'le_loader',\n",
    "#     'output_eval':True,\n",
    "#     \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "#     \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "#     \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "#     # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "#     \"default_tag\":\"O\",\n",
    "#     'batch_size': 8,\n",
    "#     'eval_batch_size': 64,\n",
    "#     'do_shuffle': True,\n",
    "#     \"use_gpu\": True,\n",
    "#     \"debug\": True,\n",
    "#     'model_name': 'ZLEBert',\n",
    "#     'task_name': 'zl_fj_tx_inter_fn3000_ccks_1'\n",
    "# }\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/subtask1_train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'task_name': 'v1_ccks_tx_fn3000_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"v1_ccks_tx_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"v1_ccks_tx_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"v1_ccks_tx_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"v1_ccks_tx_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四川\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v2',\n",
    "    'task_name': 'zl_v2_sc_tx_inter_fn3000_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_sc_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#福建\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 3000,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v2',\n",
    "    'task_name': 'zl_v2_fj_tx_inter_fn3000_1'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_fj_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccks\n",
    "# ccks_baseline 0.8258 0.8197 \n",
    "# ccks_medicine_vocab_baseline 0.8149 0.8120\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v2',\n",
    "    'task_name': 'zl_v2_ccks_tx_inter_fn3000_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v2_ccks_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四川\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v3',\n",
    "    'task_name': 'zl_v3_sc_tx_inter_fn_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_sc_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_sc_tx_inter_fn_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_sc_tx_inter_fn_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_sc_tx_inter_fn_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#福建\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 3000,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v3',\n",
    "    'task_name': 'zl_v3_fj_tx_inter_fn_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_fj_tx_inter_fn_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_fj_tx_inter_fn_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_fj_tx_inter_fn_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_fj_tx_inter_fn_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccks\n",
    "# ccks_baseline 0.8258 0.8197 \n",
    "# ccks_medicine_vocab_baseline 0.8149 0.8120\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v3',\n",
    "    'task_name': 'zl_v3_ccks_tx_inter_fn3000_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_ccks_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_ccks_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_ccks_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v3_ccks_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四川\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v4',\n",
    "    'task_name': 'zl_v4_sc_tx_inter_fn_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_sc_tx_inter_fn_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_sc_tx_inter_fn_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_sc_tx_inter_fn_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_sc_tx_inter_fn_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#福建\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 3000,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v4',\n",
    "    'task_name': 'zl_v4_fj_tx_inter_fn_1'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_fj_tx_inter_fn_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_fj_tx_inter_fn_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_fj_tx_inter_fn_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_fj_tx_inter_fn_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccks\n",
    "# ccks_baseline 0.8258 0.8197 \n",
    "# ccks_medicine_vocab_baseline 0.8149 0.8120\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    # 'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert_v4',\n",
    "    'task_name': 'zl_v4_ccks_tx_inter_fn3000_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_ccks_tx_inter_fn3000_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_ccks_tx_inter_fn3000_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_ccks_tx_inter_fn3000_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"task_name\"] = \"zl_v4_ccks_tx_inter_fn3000_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_medicine_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"tx_fn_1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 319MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 365MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 379MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 125kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 9503.71L/s]\n",
      "build line mapper: 3L [00:00, 26602.35L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 900.65it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 47482.69it/s]\n",
      "/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:02<00:00, 151.38it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:00<00:00, 148.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:44<00:00,  1.13it/s, F1=0.0116, train_acc=0.91, train_loss=164, train_precision=0.0155, train_recall=0.00949]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.0433, eval_acc=0.937, eval_loss=82.3, eval_precision=0.055, eval_recall=0.0357]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.235, train_acc=0.932, train_loss=78.9, train_precision=0.254, train_recall=0.229]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.427, eval_acc=0.947, eval_loss=58.1, eval_precision=0.414, eval_recall=0.441]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.479, train_acc=0.941, train_loss=56.4, train_precision=0.485, train_recall=0.483]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.521, eval_acc=0.945, eval_loss=45.1, eval_precision=0.491, eval_recall=0.556]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.542, train_acc=0.943, train_loss=43.4, train_precision=0.568, train_recall=0.533]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.54, eval_acc=0.944, eval_loss=38.4, eval_precision=0.508, eval_recall=0.575] \n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.593, train_acc=0.946, train_loss=32.1, train_precision=0.613, train_recall=0.585]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.604, eval_acc=0.955, eval_loss=30, eval_precision=0.627, eval_recall=0.583] \n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.632, train_acc=0.954, train_loss=23.5, train_precision=0.65, train_recall=0.621] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.615, eval_acc=0.956, eval_loss=24.6, eval_precision=0.623, eval_recall=0.608]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.657, train_acc=0.958, train_loss=18.6, train_precision=0.673, train_recall=0.645]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.623, eval_acc=0.955, eval_loss=23.7, eval_precision=0.646, eval_recall=0.602]\n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.679, train_acc=0.964, train_loss=15.5, train_precision=0.69, train_recall=0.673] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.626, eval_acc=0.954, eval_loss=25.3, eval_precision=0.657, eval_recall=0.597]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.719, train_acc=0.969, train_loss=12.9, train_precision=0.729, train_recall=0.712]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.616, eval_acc=0.952, eval_loss=22.8, eval_precision=0.667, eval_recall=0.573]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.727, train_acc=0.97, train_loss=11.1, train_precision=0.737, train_recall=0.721] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.649, eval_acc=0.957, eval_loss=22.4, eval_precision=0.662, eval_recall=0.638]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.756, train_acc=0.973, train_loss=9.75, train_precision=0.763, train_recall=0.752]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.667, eval_acc=0.957, eval_loss=20.4, eval_precision=0.707, eval_recall=0.632]\n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.793, train_acc=0.978, train_loss=8.49, train_precision=0.796, train_recall=0.792]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.67, eval_acc=0.959, eval_loss=22.4, eval_precision=0.662, eval_recall=0.679] \n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.773, train_acc=0.974, train_loss=8.84, train_precision=0.785, train_recall=0.766]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.661, eval_acc=0.957, eval_loss=18.8, eval_precision=0.685, eval_recall=0.639]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.802, train_acc=0.979, train_loss=7.25, train_precision=0.803, train_recall=0.803]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.652, eval_acc=0.955, eval_loss=21.3, eval_precision=0.695, eval_recall=0.614]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.831, train_acc=0.98, train_loss=6.46, train_precision=0.836, train_recall=0.829] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.674, eval_acc=0.96, eval_loss=16.8, eval_precision=0.679, eval_recall=0.668]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.841, train_acc=0.983, train_loss=6.06, train_precision=0.842, train_recall=0.841]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.65, eval_acc=0.957, eval_loss=18.5, eval_precision=0.67, eval_recall=0.631]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.845, train_acc=0.985, train_loss=5.54, train_precision=0.846, train_recall=0.845]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.659, eval_acc=0.959, eval_loss=20.4, eval_precision=0.668, eval_recall=0.65] \n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.852, train_acc=0.983, train_loss=5.75, train_precision=0.853, train_recall=0.853]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.658, eval_acc=0.957, eval_loss=17.7, eval_precision=0.705, eval_recall=0.617]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.856, train_acc=0.985, train_loss=4.97, train_precision=0.859, train_recall=0.855]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.649, eval_acc=0.956, eval_loss=21.5, eval_precision=0.651, eval_recall=0.646]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.854, train_acc=0.984, train_loss=4.97, train_precision=0.861, train_recall=0.85] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.676, eval_acc=0.96, eval_loss=18.5, eval_precision=0.661, eval_recall=0.691] \n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.877, train_acc=0.986, train_loss=4.3, train_precision=0.873, train_recall=0.882] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.666, eval_acc=0.959, eval_loss=23.8, eval_precision=0.685, eval_recall=0.648]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.869, train_acc=0.985, train_loss=5.56, train_precision=0.878, train_recall=0.863]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.667, eval_acc=0.961, eval_loss=16.9, eval_precision=0.699, eval_recall=0.639]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.883, train_acc=0.987, train_loss=4.56, train_precision=0.889, train_recall=0.878]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.68, eval_acc=0.961, eval_loss=19.2, eval_precision=0.687, eval_recall=0.673] \n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.878, train_acc=0.986, train_loss=4.42, train_precision=0.882, train_recall=0.877]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.672, eval_acc=0.96, eval_loss=17.2, eval_precision=0.67, eval_recall=0.674]  \n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.903, train_acc=0.989, train_loss=3.68, train_precision=0.909, train_recall=0.898]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.683, eval_acc=0.962, eval_loss=19.6, eval_precision=0.681, eval_recall=0.686]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.913, train_acc=0.989, train_loss=3.27, train_precision=0.913, train_recall=0.915]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.667, eval_acc=0.959, eval_loss=22.2, eval_precision=0.689, eval_recall=0.647]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.901, train_acc=0.988, train_loss=3.55, train_precision=0.903, train_recall=0.901]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.683, eval_acc=0.959, eval_loss=25.1, eval_precision=0.708, eval_recall=0.661]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.903, train_acc=0.989, train_loss=3.71, train_precision=0.908, train_recall=0.899]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.668, eval_acc=0.954, eval_loss=27.8, eval_precision=0.717, eval_recall=0.627]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.908, train_acc=0.987, train_loss=3.58, train_precision=0.912, train_recall=0.906]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.663, eval_acc=0.957, eval_loss=20, eval_precision=0.686, eval_recall=0.642]  \n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.909, train_acc=0.99, train_loss=3.48, train_precision=0.91, train_recall=0.909]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.645, eval_acc=0.957, eval_loss=20.6, eval_precision=0.708, eval_recall=0.592]\n"
     ]
    }
   ],
   "source": [
    "# 四川\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'inter_max_scan_num': 3000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    # 'loader_name': 'le_loader_zl',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    # \"inter_knowledge_file\":\"./data/tencent/FN_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'tx_fn_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_medicine_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"tx_fn_2\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 367MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 366MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 378MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 118kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 8805.40L/s]\n",
      "build line mapper: 3L [00:00, 5675.65L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 777.06it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 34473.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:02<00:00, 147.69it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:00<00:00, 148.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.000194, train_acc=0.841, train_loss=324, train_precision=0.000128, train_recall=0.0012] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.0012, eval_acc=0.907, eval_loss=173, eval_precision=0.000689, eval_recall=0.00462] \n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.0255, train_acc=0.917, train_loss=158, train_precision=0.0164, train_recall=0.0614]     \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.094, eval_acc=0.942, eval_loss=105, eval_precision=0.0621, eval_recall=0.194]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.134, train_acc=0.935, train_loss=110, train_precision=0.0859, train_recall=0.319] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.271, eval_acc=0.948, eval_loss=84, eval_precision=0.183, eval_recall=0.524]  \n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.264, train_acc=0.944, train_loss=81.8, train_precision=0.188, train_recall=0.47]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.416, eval_acc=0.953, eval_loss=63.9, eval_precision=0.327, eval_recall=0.574]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.49, train_acc=0.954, train_loss=58.2, train_precision=0.439, train_recall=0.563] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.569, eval_acc=0.96, eval_loss=49.7, eval_precision=0.538, eval_recall=0.603] \n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.58, train_acc=0.961, train_loss=42.1, train_precision=0.559, train_recall=0.606] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.598, eval_acc=0.959, eval_loss=41.7, eval_precision=0.575, eval_recall=0.624]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.628, train_acc=0.963, train_loss=32.3, train_precision=0.616, train_recall=0.644]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.638, eval_acc=0.961, eval_loss=36.5, eval_precision=0.627, eval_recall=0.65] \n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.684, train_acc=0.968, train_loss=25.4, train_precision=0.678, train_recall=0.694]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s, F1=0.627, eval_acc=0.959, eval_loss=33.9, eval_precision=0.595, eval_recall=0.664]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.696, train_acc=0.969, train_loss=21.7, train_precision=0.697, train_recall=0.699]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.647, eval_acc=0.958, eval_loss=35.6, eval_precision=0.616, eval_recall=0.681]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.725, train_acc=0.973, train_loss=17.9, train_precision=0.726, train_recall=0.728]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.635, eval_acc=0.96, eval_loss=32.7, eval_precision=0.614, eval_recall=0.657]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.734, train_acc=0.973, train_loss=16.7, train_precision=0.736, train_recall=0.737]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.641, eval_acc=0.956, eval_loss=32.1, eval_precision=0.631, eval_recall=0.651]\n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.739, train_acc=0.971, train_loss=15.2, train_precision=0.75, train_recall=0.736] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.652, eval_acc=0.956, eval_loss=30.6, eval_precision=0.624, eval_recall=0.682]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.752, train_acc=0.971, train_loss=14, train_precision=0.765, train_recall=0.745]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.639, eval_acc=0.959, eval_loss=29.5, eval_precision=0.61, eval_recall=0.671] \n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.766, train_acc=0.976, train_loss=11.5, train_precision=0.772, train_recall=0.764]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.676, eval_acc=0.962, eval_loss=32.1, eval_precision=0.658, eval_recall=0.696]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.802, train_acc=0.981, train_loss=9.09, train_precision=0.802, train_recall=0.804]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.678, eval_acc=0.963, eval_loss=34.3, eval_precision=0.656, eval_recall=0.7]  \n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.81, train_acc=0.981, train_loss=8.65, train_precision=0.819, train_recall=0.805] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.682, eval_acc=0.964, eval_loss=32.9, eval_precision=0.663, eval_recall=0.702]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:44<00:00,  1.13it/s, F1=0.835, train_acc=0.985, train_loss=7.29, train_precision=0.837, train_recall=0.836]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.666, eval_acc=0.959, eval_loss=34.1, eval_precision=0.676, eval_recall=0.656]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.833, train_acc=0.984, train_loss=6.89, train_precision=0.837, train_recall=0.832]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.67, eval_acc=0.959, eval_loss=28.6, eval_precision=0.67, eval_recall=0.669]  \n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.855, train_acc=0.987, train_loss=6.03, train_precision=0.854, train_recall=0.858]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.666, eval_acc=0.959, eval_loss=33.9, eval_precision=0.678, eval_recall=0.654]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.854, train_acc=0.986, train_loss=5.94, train_precision=0.857, train_recall=0.854]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.667, eval_acc=0.96, eval_loss=25.8, eval_precision=0.667, eval_recall=0.668]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.872, train_acc=0.987, train_loss=5.02, train_precision=0.872, train_recall=0.873]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.67, eval_acc=0.959, eval_loss=31, eval_precision=0.683, eval_recall=0.658]   \n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.882, train_acc=0.99, train_loss=4.47, train_precision=0.882, train_recall=0.882] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.674, eval_acc=0.956, eval_loss=37.5, eval_precision=0.686, eval_recall=0.662]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.892, train_acc=0.989, train_loss=4.29, train_precision=0.895, train_recall=0.889]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.671, eval_acc=0.962, eval_loss=36.6, eval_precision=0.689, eval_recall=0.655]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.903, train_acc=0.991, train_loss=3.98, train_precision=0.903, train_recall=0.903]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.676, eval_acc=0.963, eval_loss=34.3, eval_precision=0.699, eval_recall=0.656]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.909, train_acc=0.991, train_loss=3.73, train_precision=0.909, train_recall=0.91] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.684, eval_acc=0.964, eval_loss=35.1, eval_precision=0.68, eval_recall=0.688]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.919, train_acc=0.993, train_loss=3.31, train_precision=0.923, train_recall=0.916]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.66, eval_acc=0.959, eval_loss=40.4, eval_precision=0.677, eval_recall=0.645] \n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.916, train_acc=0.993, train_loss=3.2, train_precision=0.916, train_recall=0.917] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.686, eval_acc=0.963, eval_loss=42.4, eval_precision=0.69, eval_recall=0.682]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.916, train_acc=0.992, train_loss=3.4, train_precision=0.917, train_recall=0.915] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.683, eval_acc=0.961, eval_loss=33.4, eval_precision=0.696, eval_recall=0.671]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.906, train_acc=0.99, train_loss=3.65, train_precision=0.905, train_recall=0.908] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.677, eval_acc=0.959, eval_loss=34.6, eval_precision=0.694, eval_recall=0.662]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.91, train_acc=0.992, train_loss=3.06, train_precision=0.912, train_recall=0.909] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.683, eval_acc=0.961, eval_loss=35.3, eval_precision=0.683, eval_recall=0.684]\n"
     ]
    }
   ],
   "source": [
    "args[\"task_name\"] = \"tx_fn_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_medicine_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"tx_fn_3\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 335MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 335MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 366MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 37.7kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 9539.74L/s]\n",
      "build line mapper: 3L [00:00, 28339.89L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 924.81it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 43996.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:02<00:00, 150.86it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:00<00:00, 151.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.000634, train_acc=0.808, train_loss=362, train_precision=0.000554, train_recall=0.000971]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0, eval_acc=0.915, eval_loss=183, eval_precision=0, eval_recall=0]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.0217, train_acc=0.915, train_loss=155, train_precision=0.0141, train_recall=0.052]     \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.028, eval_acc=0.941, eval_loss=106, eval_precision=0.0193, eval_recall=0.0508]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.124, train_acc=0.934, train_loss=106, train_precision=0.0812, train_recall=0.273]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.037, eval_acc=0.934, eval_loss=97.4, eval_precision=0.0242, eval_recall=0.0781] \n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.302, train_acc=0.946, train_loss=76.7, train_precision=0.23, train_recall=0.459]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.289, eval_acc=0.95, eval_loss=68.2, eval_precision=0.273, eval_recall=0.308]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.49, train_acc=0.954, train_loss=56.5, train_precision=0.441, train_recall=0.559] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.495, eval_acc=0.958, eval_loss=50.2, eval_precision=0.502, eval_recall=0.489]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.579, train_acc=0.96, train_loss=42.3, train_precision=0.554, train_recall=0.61]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.518, eval_acc=0.954, eval_loss=45.5, eval_precision=0.549, eval_recall=0.492]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.618, train_acc=0.963, train_loss=32.6, train_precision=0.605, train_recall=0.637]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.545, eval_acc=0.952, eval_loss=38.9, eval_precision=0.573, eval_recall=0.52] \n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.646, train_acc=0.965, train_loss=27.5, train_precision=0.64, train_recall=0.658] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.535, eval_acc=0.948, eval_loss=37.6, eval_precision=0.613, eval_recall=0.474]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.681, train_acc=0.966, train_loss=24.3, train_precision=0.675, train_recall=0.693]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.62, eval_acc=0.957, eval_loss=28.7, eval_precision=0.659, eval_recall=0.585] \n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.684, train_acc=0.966, train_loss=22.6, train_precision=0.688, train_recall=0.687]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.606, eval_acc=0.955, eval_loss=27, eval_precision=0.627, eval_recall=0.587]  \n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.708, train_acc=0.968, train_loss=19.3, train_precision=0.715, train_recall=0.709]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.591, eval_acc=0.94, eval_loss=29.3, eval_precision=0.577, eval_recall=0.606] \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.729, train_acc=0.97, train_loss=16.4, train_precision=0.744, train_recall=0.722] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.644, eval_acc=0.957, eval_loss=29.6, eval_precision=0.658, eval_recall=0.631]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.768, train_acc=0.977, train_loss=12.2, train_precision=0.772, train_recall=0.768]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.656, eval_acc=0.962, eval_loss=28.5, eval_precision=0.673, eval_recall=0.64] \n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.8, train_acc=0.978, train_loss=10.5, train_precision=0.803, train_recall=0.8]    \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.654, eval_acc=0.963, eval_loss=27.6, eval_precision=0.67, eval_recall=0.638]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.81, train_acc=0.98, train_loss=9.58, train_precision=0.815, train_recall=0.81]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.659, eval_acc=0.958, eval_loss=25.2, eval_precision=0.65, eval_recall=0.668]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.807, train_acc=0.981, train_loss=8.95, train_precision=0.816, train_recall=0.802]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.682, eval_acc=0.962, eval_loss=25.6, eval_precision=0.695, eval_recall=0.67]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.833, train_acc=0.983, train_loss=7.42, train_precision=0.833, train_recall=0.836]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.636, eval_acc=0.952, eval_loss=29.2, eval_precision=0.623, eval_recall=0.649]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.836, train_acc=0.984, train_loss=7.51, train_precision=0.84, train_recall=0.836] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.652, eval_acc=0.954, eval_loss=32.5, eval_precision=0.635, eval_recall=0.671]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.816, train_acc=0.981, train_loss=7.9, train_precision=0.824, train_recall=0.812] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.649, eval_acc=0.954, eval_loss=31.4, eval_precision=0.624, eval_recall=0.676]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.837, train_acc=0.984, train_loss=6.65, train_precision=0.84, train_recall=0.836] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.631, eval_acc=0.947, eval_loss=29.5, eval_precision=0.599, eval_recall=0.666]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.846, train_acc=0.983, train_loss=6.5, train_precision=0.851, train_recall=0.845] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.67, eval_acc=0.957, eval_loss=33.8, eval_precision=0.639, eval_recall=0.705] \n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.846, train_acc=0.982, train_loss=6.72, train_precision=0.849, train_recall=0.845]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.653, eval_acc=0.957, eval_loss=26.9, eval_precision=0.636, eval_recall=0.671]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.849, train_acc=0.983, train_loss=6.23, train_precision=0.856, train_recall=0.846]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.662, eval_acc=0.957, eval_loss=27.5, eval_precision=0.654, eval_recall=0.671]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.843, train_acc=0.983, train_loss=6.56, train_precision=0.847, train_recall=0.841]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.669, eval_acc=0.96, eval_loss=24.6, eval_precision=0.647, eval_recall=0.693] \n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.835, train_acc=0.982, train_loss=7.05, train_precision=0.841, train_recall=0.831]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.666, eval_acc=0.963, eval_loss=28.2, eval_precision=0.691, eval_recall=0.642]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.859, train_acc=0.984, train_loss=5.51, train_precision=0.864, train_recall=0.856]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.675, eval_acc=0.961, eval_loss=28.5, eval_precision=0.717, eval_recall=0.637]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.883, train_acc=0.988, train_loss=4.41, train_precision=0.883, train_recall=0.885]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.661, eval_acc=0.96, eval_loss=28.9, eval_precision=0.654, eval_recall=0.668]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.902, train_acc=0.991, train_loss=3.33, train_precision=0.905, train_recall=0.9]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.656, eval_acc=0.955, eval_loss=32, eval_precision=0.654, eval_recall=0.657]  \n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.92, train_acc=0.991, train_loss=3.14, train_precision=0.919, train_recall=0.921] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.648, eval_acc=0.955, eval_loss=29.6, eval_precision=0.641, eval_recall=0.655]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.926, train_acc=0.993, train_loss=2.66, train_precision=0.928, train_recall=0.926]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.642, eval_acc=0.954, eval_loss=36, eval_precision=0.628, eval_recall=0.656]  \n"
     ]
    }
   ],
   "source": [
    "args[\"task_name\"] = \"tx_fn_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_medicine_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"tx_fn_4\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 363MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 349MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 379MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 66.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 9171.22L/s]\n",
      "build line mapper: 3L [00:00, 6093.42L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 902.32it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 43389.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:02<00:00, 150.18it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:00<00:00, 150.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.0376, train_acc=0.914, train_loss=129, train_precision=0.0425, train_recall=0.0358]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.0502, eval_acc=0.935, eval_loss=85.5, eval_precision=0.0653, eval_recall=0.0408]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.325, train_acc=0.935, train_loss=74.9, train_precision=0.301, train_recall=0.365]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.472, eval_acc=0.949, eval_loss=56.7, eval_precision=0.477, eval_recall=0.466]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:43<00:00,  1.14it/s, F1=0.495, train_acc=0.944, train_loss=54.2, train_precision=0.483, train_recall=0.527]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.592, eval_acc=0.957, eval_loss=40.2, eval_precision=0.591, eval_recall=0.594]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.589, train_acc=0.955, train_loss=37, train_precision=0.579, train_recall=0.605]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.618, eval_acc=0.962, eval_loss=30.4, eval_precision=0.641, eval_recall=0.597]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.622, train_acc=0.957, train_loss=28.4, train_precision=0.626, train_recall=0.626]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.603, eval_acc=0.958, eval_loss=26, eval_precision=0.591, eval_recall=0.614]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.677, train_acc=0.965, train_loss=21, train_precision=0.68, train_recall=0.68]    \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.607, eval_acc=0.954, eval_loss=24, eval_precision=0.591, eval_recall=0.624]  \n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.705, train_acc=0.966, train_loss=16.9, train_precision=0.711, train_recall=0.705]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.649, eval_acc=0.961, eval_loss=21.6, eval_precision=0.649, eval_recall=0.649]\n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.729, train_acc=0.971, train_loss=13.6, train_precision=0.737, train_recall=0.725]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.648, eval_acc=0.963, eval_loss=21.1, eval_precision=0.665, eval_recall=0.633]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.778, train_acc=0.975, train_loss=11.3, train_precision=0.78, train_recall=0.777] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.651, eval_acc=0.963, eval_loss=19.7, eval_precision=0.672, eval_recall=0.631]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.784, train_acc=0.977, train_loss=9.6, train_precision=0.792, train_recall=0.778] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.635, eval_acc=0.96, eval_loss=21.7, eval_precision=0.655, eval_recall=0.617] \n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.802, train_acc=0.978, train_loss=8.91, train_precision=0.803, train_recall=0.804]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.642, eval_acc=0.958, eval_loss=21.7, eval_precision=0.663, eval_recall=0.622]\n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.832, train_acc=0.983, train_loss=7.7, train_precision=0.836, train_recall=0.83]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.629, eval_acc=0.955, eval_loss=20.9, eval_precision=0.646, eval_recall=0.612]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.84, train_acc=0.984, train_loss=7.08, train_precision=0.848, train_recall=0.834] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.636, eval_acc=0.959, eval_loss=20.9, eval_precision=0.648, eval_recall=0.625]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.84, train_acc=0.984, train_loss=6.88, train_precision=0.846, train_recall=0.836] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.651, eval_acc=0.962, eval_loss=18.5, eval_precision=0.655, eval_recall=0.647]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.84, train_acc=0.985, train_loss=6.36, train_precision=0.845, train_recall=0.838] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.669, eval_acc=0.96, eval_loss=18.1, eval_precision=0.672, eval_recall=0.666]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.855, train_acc=0.984, train_loss=6.09, train_precision=0.862, train_recall=0.849]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.672, eval_acc=0.963, eval_loss=18.5, eval_precision=0.667, eval_recall=0.677]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.866, train_acc=0.985, train_loss=6.22, train_precision=0.871, train_recall=0.862]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.649, eval_acc=0.962, eval_loss=14.6, eval_precision=0.645, eval_recall=0.653]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.881, train_acc=0.987, train_loss=5.16, train_precision=0.888, train_recall=0.875]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.697, eval_acc=0.964, eval_loss=15.3, eval_precision=0.707, eval_recall=0.687]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.9, train_acc=0.99, train_loss=4.27, train_precision=0.905, train_recall=0.896]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.672, eval_acc=0.962, eval_loss=15.5, eval_precision=0.692, eval_recall=0.654]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.896, train_acc=0.989, train_loss=4.01, train_precision=0.901, train_recall=0.891]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.667, eval_acc=0.963, eval_loss=16.1, eval_precision=0.688, eval_recall=0.647]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.91, train_acc=0.991, train_loss=3.58, train_precision=0.911, train_recall=0.91]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.685, eval_acc=0.961, eval_loss=17.7, eval_precision=0.701, eval_recall=0.669]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.922, train_acc=0.993, train_loss=3.33, train_precision=0.928, train_recall=0.917]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.679, eval_acc=0.963, eval_loss=17, eval_precision=0.702, eval_recall=0.658]  \n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.928, train_acc=0.993, train_loss=3.45, train_precision=0.932, train_recall=0.926]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.659, eval_acc=0.961, eval_loss=17.2, eval_precision=0.679, eval_recall=0.64]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.921, train_acc=0.992, train_loss=3.58, train_precision=0.921, train_recall=0.922]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.666, eval_acc=0.959, eval_loss=18, eval_precision=0.706, eval_recall=0.631] \n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.922, train_acc=0.993, train_loss=3.56, train_precision=0.923, train_recall=0.922]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.673, eval_acc=0.96, eval_loss=16.8, eval_precision=0.687, eval_recall=0.659] \n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.926, train_acc=0.992, train_loss=3.47, train_precision=0.929, train_recall=0.923]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.655, eval_acc=0.959, eval_loss=19.4, eval_precision=0.677, eval_recall=0.635]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.921, train_acc=0.992, train_loss=3.34, train_precision=0.924, train_recall=0.919]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.673, eval_acc=0.962, eval_loss=18.4, eval_precision=0.702, eval_recall=0.647]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.942, train_acc=0.993, train_loss=3.03, train_precision=0.944, train_recall=0.94] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.687, eval_acc=0.961, eval_loss=18.4, eval_precision=0.717, eval_recall=0.661]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.942, train_acc=0.994, train_loss=2.97, train_precision=0.943, train_recall=0.941]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.641, eval_acc=0.954, eval_loss=19.8, eval_precision=0.731, eval_recall=0.572]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.948, train_acc=0.994, train_loss=3.1, train_precision=0.947, train_recall=0.949] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.67, eval_acc=0.959, eval_loss=21.1, eval_precision=0.721, eval_recall=0.625] \n"
     ]
    }
   ],
   "source": [
    "args[\"task_name\"] = \"tx_fn_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_medicine_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"tx_fn_5\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 361MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 363MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 380MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 65.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 8456.26L/s]\n",
      "build line mapper: 3L [00:00, 27962.03L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 909.83it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 42799.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:02<00:00, 149.50it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:00<00:00, 147.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.000254, train_acc=0.829, train_loss=193, train_precision=0.000131, train_recall=0.00589]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.00505, eval_acc=0.922, eval_loss=99.9, eval_precision=0.0122, eval_recall=0.00318]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.135, train_acc=0.925, train_loss=87.4, train_precision=0.149, train_recall=0.133]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.4, eval_acc=0.948, eval_loss=59.5, eval_precision=0.366, eval_recall=0.442]  \n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.437, train_acc=0.94, train_loss=56.1, train_precision=0.413, train_recall=0.474] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.588, eval_acc=0.953, eval_loss=44.4, eval_precision=0.581, eval_recall=0.595]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.554, train_acc=0.948, train_loss=41.1, train_precision=0.552, train_recall=0.565]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.61, eval_acc=0.952, eval_loss=37.2, eval_precision=0.649, eval_recall=0.575] \n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.596, train_acc=0.947, train_loss=31.6, train_precision=0.603, train_recall=0.601]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.622, eval_acc=0.954, eval_loss=29.4, eval_precision=0.623, eval_recall=0.621]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.647, train_acc=0.955, train_loss=24.6, train_precision=0.66, train_recall=0.642] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.629, eval_acc=0.956, eval_loss=24.9, eval_precision=0.628, eval_recall=0.63] \n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:44<00:00,  1.13it/s, F1=0.669, train_acc=0.96, train_loss=18.8, train_precision=0.675, train_recall=0.668] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.63, eval_acc=0.957, eval_loss=22, eval_precision=0.625, eval_recall=0.636] \n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.688, train_acc=0.964, train_loss=15.5, train_precision=0.699, train_recall=0.683]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.643, eval_acc=0.958, eval_loss=19.1, eval_precision=0.639, eval_recall=0.646]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.721, train_acc=0.967, train_loss=13.4, train_precision=0.731, train_recall=0.717]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.621, eval_acc=0.947, eval_loss=21.4, eval_precision=0.678, eval_recall=0.573]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.72, train_acc=0.967, train_loss=12.4, train_precision=0.732, train_recall=0.716] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.603, eval_acc=0.954, eval_loss=20.1, eval_precision=0.608, eval_recall=0.598]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.748, train_acc=0.97, train_loss=10.8, train_precision=0.764, train_recall=0.738] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.653, eval_acc=0.952, eval_loss=19, eval_precision=0.685, eval_recall=0.623] \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.76, train_acc=0.97, train_loss=10.6, train_precision=0.782, train_recall=0.748]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.636, eval_acc=0.949, eval_loss=18.1, eval_precision=0.745, eval_recall=0.555]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.777, train_acc=0.974, train_loss=8.95, train_precision=0.789, train_recall=0.771]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.655, eval_acc=0.958, eval_loss=17.5, eval_precision=0.657, eval_recall=0.654]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.784, train_acc=0.975, train_loss=8.09, train_precision=0.79, train_recall=0.781] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.601, eval_acc=0.947, eval_loss=20.3, eval_precision=0.591, eval_recall=0.611]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.811, train_acc=0.976, train_loss=7.28, train_precision=0.82, train_recall=0.807] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.579, eval_acc=0.942, eval_loss=19.3, eval_precision=0.588, eval_recall=0.57] \n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.825, train_acc=0.981, train_loss=6.48, train_precision=0.833, train_recall=0.821]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.572, eval_acc=0.939, eval_loss=23.7, eval_precision=0.561, eval_recall=0.584]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.816, train_acc=0.979, train_loss=6.83, train_precision=0.824, train_recall=0.813]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.628, eval_acc=0.948, eval_loss=20.5, eval_precision=0.614, eval_recall=0.643]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.807, train_acc=0.979, train_loss=6.81, train_precision=0.82, train_recall=0.802] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.655, eval_acc=0.956, eval_loss=18.2, eval_precision=0.664, eval_recall=0.647]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, F1=0.828, train_acc=0.981, train_loss=6.12, train_precision=0.842, train_recall=0.822]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.666, eval_acc=0.959, eval_loss=22.3, eval_precision=0.668, eval_recall=0.664]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.819, train_acc=0.981, train_loss=5.87, train_precision=0.827, train_recall=0.816]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.657, eval_acc=0.961, eval_loss=23.4, eval_precision=0.644, eval_recall=0.671]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.862, train_acc=0.985, train_loss=4.42, train_precision=0.865, train_recall=0.86] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.674, eval_acc=0.957, eval_loss=24, eval_precision=0.684, eval_recall=0.665]  \n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.879, train_acc=0.986, train_loss=3.89, train_precision=0.88, train_recall=0.88]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.688, eval_acc=0.961, eval_loss=20.9, eval_precision=0.694, eval_recall=0.682]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.897, train_acc=0.988, train_loss=3.46, train_precision=0.898, train_recall=0.897]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.675, eval_acc=0.96, eval_loss=20.3, eval_precision=0.688, eval_recall=0.663] \n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.905, train_acc=0.99, train_loss=3.23, train_precision=0.908, train_recall=0.903] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.659, eval_acc=0.961, eval_loss=21.8, eval_precision=0.694, eval_recall=0.628]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.91, train_acc=0.99, train_loss=3, train_precision=0.913, train_recall=0.909]     \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.677, eval_acc=0.962, eval_loss=20.9, eval_precision=0.704, eval_recall=0.653]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.919, train_acc=0.992, train_loss=2.59, train_precision=0.92, train_recall=0.919] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.684, eval_acc=0.962, eval_loss=23.9, eval_precision=0.685, eval_recall=0.683]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.932, train_acc=0.993, train_loss=2.31, train_precision=0.932, train_recall=0.933]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.684, eval_acc=0.96, eval_loss=23.5, eval_precision=0.703, eval_recall=0.666]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.942, train_acc=0.994, train_loss=2.17, train_precision=0.94, train_recall=0.944] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.683, eval_acc=0.96, eval_loss=25.7, eval_precision=0.663, eval_recall=0.705] \n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, F1=0.927, train_acc=0.992, train_loss=2.9, train_precision=0.929, train_recall=0.925] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.652, eval_acc=0.956, eval_loss=20.9, eval_precision=0.674, eval_recall=0.633]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s, F1=0.922, train_acc=0.991, train_loss=2.73, train_precision=0.925, train_recall=0.921]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.678, eval_acc=0.959, eval_loss=23.6, eval_precision=0.704, eval_recall=0.654]\n"
     ]
    }
   ],
   "source": [
    "args[\"task_name\"] = \"tx_fn_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b5eafe76d1bce12a884379452caff4a891b716fe38e21da9ac26a4bcd47e2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('bert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
