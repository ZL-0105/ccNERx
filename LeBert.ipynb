{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FN\n",
    "### LeBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "from CC.trainer import NERTrainer\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj+sc/train(json-400).csv etag: 100%|██████████| 6.47M/6.47M [00:00<00:00, 541MB/s]\n",
      "calculate ./data/FN/sc-json/dev.csv etag: 100%|██████████| 346k/346k [00:00<00:00, 430MB/s]\n",
      "calculate ./data/FN/sc-json/test.csv etag: 100%|██████████| 318k/318k [00:00<00:00, 405MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 38.6kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 4,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj+sc/train(json-400).csv\",\n",
      "    \"eval_file\": \"./data/FN/sc-json/dev.csv\",\n",
      "    \"test_file\": \"./data/FN/sc-json/test.csv\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"FN-fj+sc(400)-sc-LeBert\"\n",
      "}\n",
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 10082.46L/s]\n",
      "build line mapper: 3L [00:00, 30690.03L/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 1005.43it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 35746.91it/s]\n",
      "/home/zl/miniconda3/envs/FN/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "load dataset from ./data/FN/fj+sc/train(json-400).csv:   2%|▏         | 19/800 [00:00<00:04, 185.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n",
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/fj+sc/train(json-400).csv: 100%|██████████| 800/800 [00:03<00:00, 264.83it/s]\n",
      "load dataset from ./data/FN/sc-json/dev.csv: 100%|██████████| 30/30 [00:00<00:00, 200.34it/s]\n",
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_transform.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.fuse_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train:  15%|█▌        | 30/200 [00:36<02:45,  1.03it/s, F1=8.09e-5, train_acc=0.117, train_loss=978, train_precision=4.08e-5, train_recall=0.00444]/home/zl/miniconda3/envs/FN/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 200/200 [03:27<00:00,  1.04s/it, F1=0.0502, train_acc=0.797, train_loss=264, train_precision=0.0439, train_recall=0.0635]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s, F1=0.0773, eval_acc=0.93, eval_loss=106, eval_precision=0.0584, eval_recall=0.114]\n",
      "Epoch: 2/30 Train: 100%|██████████| 200/200 [03:21<00:00,  1.01s/it, F1=0.252, train_acc=0.948, train_loss=65.7, train_precision=0.216, train_recall=0.31]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, F1=0.244, eval_acc=0.947, eval_loss=66.9, eval_precision=0.182, eval_recall=0.371]\n",
      "Epoch: 3/30 Train: 100%|██████████| 200/200 [03:26<00:00,  1.03s/it, F1=0.543, train_acc=0.963, train_loss=42.3, train_precision=0.489, train_recall=0.621]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, F1=0.369, eval_acc=0.954, eval_loss=53.7, eval_precision=0.324, eval_recall=0.429]\n",
      "Epoch: 4/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.642, train_acc=0.969, train_loss=32.9, train_precision=0.6, train_recall=0.699]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, F1=0.419, eval_acc=0.96, eval_loss=46.4, eval_precision=0.382, eval_recall=0.464]\n",
      "Epoch: 5/30 Train: 100%|██████████| 200/200 [03:26<00:00,  1.03s/it, F1=0.706, train_acc=0.973, train_loss=26, train_precision=0.673, train_recall=0.75]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, F1=0.49, eval_acc=0.968, eval_loss=38.1, eval_precision=0.473, eval_recall=0.507]\n",
      "Epoch: 6/30 Train: 100%|██████████| 200/200 [03:23<00:00,  1.02s/it, F1=0.732, train_acc=0.976, train_loss=22.2, train_precision=0.71, train_recall=0.761]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, F1=0.505, eval_acc=0.966, eval_loss=37.1, eval_precision=0.504, eval_recall=0.507]\n",
      "Epoch: 7/30 Train: 100%|██████████| 200/200 [03:23<00:00,  1.02s/it, F1=0.762, train_acc=0.979, train_loss=18.9, train_precision=0.743, train_recall=0.788]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, F1=0.551, eval_acc=0.964, eval_loss=30.3, eval_precision=0.537, eval_recall=0.564]\n",
      "Epoch: 8/30 Train: 100%|██████████| 200/200 [03:21<00:00,  1.01s/it, F1=0.793, train_acc=0.982, train_loss=15, train_precision=0.78, train_recall=0.811]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, F1=0.61, eval_acc=0.968, eval_loss=34.4, eval_precision=0.629, eval_recall=0.593]\n",
      "Epoch: 9/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.807, train_acc=0.982, train_loss=13.7, train_precision=0.794, train_recall=0.826]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, F1=0.58, eval_acc=0.973, eval_loss=26.2, eval_precision=0.605, eval_recall=0.557]\n",
      "Epoch: 10/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.815, train_acc=0.982, train_loss=13.3, train_precision=0.811, train_recall=0.824]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, F1=0.688, eval_acc=0.972, eval_loss=23.1, eval_precision=0.699, eval_recall=0.679]\n",
      "Epoch: 11/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.837, train_acc=0.984, train_loss=10.7, train_precision=0.833, train_recall=0.845]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, F1=0.71, eval_acc=0.976, eval_loss=19.8, eval_precision=0.712, eval_recall=0.707]\n",
      "Epoch: 12/30 Train: 100%|██████████| 200/200 [03:26<00:00,  1.03s/it, F1=0.853, train_acc=0.985, train_loss=9.67, train_precision=0.851, train_recall=0.859]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, F1=0.708, eval_acc=0.971, eval_loss=18.3, eval_precision=0.715, eval_recall=0.7]\n",
      "Epoch: 13/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.861, train_acc=0.986, train_loss=8.62, train_precision=0.862, train_recall=0.865]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, F1=0.765, eval_acc=0.979, eval_loss=15.1, eval_precision=0.774, eval_recall=0.757]\n",
      "Epoch: 14/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.875, train_acc=0.987, train_loss=7.74, train_precision=0.875, train_recall=0.878]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, F1=0.731, eval_acc=0.976, eval_loss=12.6, eval_precision=0.734, eval_recall=0.729]\n",
      "Epoch: 15/30 Train: 100%|██████████| 200/200 [03:23<00:00,  1.02s/it, F1=0.885, train_acc=0.989, train_loss=6.28, train_precision=0.884, train_recall=0.889]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, F1=0.761, eval_acc=0.982, eval_loss=12.1, eval_precision=0.772, eval_recall=0.75]\n",
      "Epoch: 16/30 Train: 100%|██████████| 200/200 [03:25<00:00,  1.03s/it, F1=0.9, train_acc=0.991, train_loss=5.54, train_precision=0.901, train_recall=0.901]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, F1=0.758, eval_acc=0.981, eval_loss=12.8, eval_precision=0.791, eval_recall=0.729]\n",
      "Epoch: 17/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.91, train_acc=0.992, train_loss=4.85, train_precision=0.911, train_recall=0.911]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, F1=0.8, eval_acc=0.985, eval_loss=11.9, eval_precision=0.8, eval_recall=0.8]\n",
      "Epoch: 18/30 Train: 100%|██████████| 200/200 [03:25<00:00,  1.03s/it, F1=0.908, train_acc=0.99, train_loss=5.36, train_precision=0.907, train_recall=0.911]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, F1=0.789, eval_acc=0.979, eval_loss=11.4, eval_precision=0.778, eval_recall=0.8]\n",
      "Epoch: 19/30 Train: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it, F1=0.907, train_acc=0.991, train_loss=4.9, train_precision=0.905, train_recall=0.91]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, F1=0.781, eval_acc=0.975, eval_loss=12.3, eval_precision=0.814, eval_recall=0.75]\n",
      "Epoch: 20/30 Train: 100%|██████████| 200/200 [03:23<00:00,  1.02s/it, F1=0.92, train_acc=0.992, train_loss=3.98, train_precision=0.922, train_recall=0.921]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, F1=0.803, eval_acc=0.98, eval_loss=11.8, eval_precision=0.806, eval_recall=0.8]\n",
      "Epoch: 21/30 Train: 100%|██████████| 200/200 [03:25<00:00,  1.03s/it, F1=0.917, train_acc=0.993, train_loss=3.99, train_precision=0.917, train_recall=0.918]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, F1=0.817, eval_acc=0.982, eval_loss=11.5, eval_precision=0.82, eval_recall=0.814]\n",
      "Epoch: 22/30 Train: 100%|██████████| 200/200 [03:23<00:00,  1.02s/it, F1=0.926, train_acc=0.992, train_loss=3.61, train_precision=0.929, train_recall=0.926]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, F1=0.842, eval_acc=0.983, eval_loss=8.76, eval_precision=0.865, eval_recall=0.821]\n",
      "Epoch: 23/30 Train: 100%|██████████| 200/200 [03:19<00:00,  1.00it/s, F1=0.927, train_acc=0.992, train_loss=3.6, train_precision=0.927, train_recall=0.928]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, F1=0.799, eval_acc=0.985, eval_loss=9.34, eval_precision=0.804, eval_recall=0.793]\n",
      "Epoch: 24/30 Train: 100%|██████████| 200/200 [02:56<00:00,  1.13it/s, F1=0.936, train_acc=0.994, train_loss=2.99, train_precision=0.936, train_recall=0.937]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s, F1=0.866, eval_acc=0.987, eval_loss=9.44, eval_precision=0.876, eval_recall=0.857]\n",
      "Epoch: 25/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.937, train_acc=0.994, train_loss=2.67, train_precision=0.939, train_recall=0.937]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s, F1=0.768, eval_acc=0.984, eval_loss=10.2, eval_precision=0.779, eval_recall=0.757]\n",
      "Epoch: 26/30 Train: 100%|██████████| 200/200 [02:05<00:00,  1.59it/s, F1=0.946, train_acc=0.995, train_loss=2.18, train_precision=0.946, train_recall=0.947]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s, F1=0.78, eval_acc=0.984, eval_loss=11, eval_precision=0.775, eval_recall=0.786]\n",
      "Epoch: 27/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.56it/s, F1=0.951, train_acc=0.995, train_loss=2.02, train_precision=0.953, train_recall=0.951]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s, F1=0.77, eval_acc=0.982, eval_loss=10.7, eval_precision=0.775, eval_recall=0.764]\n",
      "Epoch: 28/30 Train: 100%|██████████| 200/200 [02:06<00:00,  1.58it/s, F1=0.946, train_acc=0.995, train_loss=2.19, train_precision=0.95, train_recall=0.943]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s, F1=0.76, eval_acc=0.977, eval_loss=9.13, eval_precision=0.763, eval_recall=0.757]\n",
      "Epoch: 29/30 Train: 100%|██████████| 200/200 [02:08<00:00,  1.56it/s, F1=0.946, train_acc=0.995, train_loss=2.1, train_precision=0.947, train_recall=0.947]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s, F1=0.852, eval_acc=0.988, eval_loss=8.37, eval_precision=0.84, eval_recall=0.864]\n",
      "Epoch: 30/30 Train: 100%|██████████| 200/200 [02:05<00:00,  1.59it/s, F1=0.949, train_acc=0.995, train_loss=2.19, train_precision=0.95, train_recall=0.949]\n",
      "Eval Result: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s, F1=0.826, eval_acc=0.986, eval_loss=9.95, eval_precision=0.838, eval_recall=0.814]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_3/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_4/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_5/Bert_95250/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj+sc/train(json-400).csv',\n",
    "    'eval_file': './data/FN/sc-json/dev.csv',\n",
    "    'test_file': './data/FN/sc-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN-fj+sc(400)-sc-LeBert'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj+sc/train(json-400).csv etag: 100%|██████████| 6.47M/6.47M [00:00<00:00, 613MB/s]\n",
      "calculate ./data/FN/fj-json/dev.csv etag: 100%|██████████| 9.75M/9.75M [00:00<00:00, 618MB/s]\n",
      "calculate ./data/FN/fj-json/test.csv etag: 100%|██████████| 9.64M/9.64M [00:00<00:00, 626MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 69.5kB/s]\n",
      "count line size ./data/tencent/tencent_vocab.txt: 8824306L [00:00, 66571018.40L/s]\n",
      "load vocabs into trie:   0%|          | 0/1000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 4,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj+sc/train(json-400).csv\",\n",
      "    \"eval_file\": \"./data/FN/fj-json/dev.csv\",\n",
      "    \"test_file\": \"./data/FN/fj-json/test.csv\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"FN-fj+sc(400)-fj-LeBert\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build line mapper: 8824306L [00:02, 3372552.58L/s]\n",
      "load vocabs into trie: 100%|██████████| 1000000/1000000 [00:09<00:00, 108310.79it/s]\n",
      "build trie: 100%|██████████| 1000000/1000000 [00:11<00:00, 90406.76it/s]\n",
      "count line size ./data/FN/fj+sc/train(json-400).csv: 801L [00:00, 231157.11L/s]\n",
      "load dataset matched word: 100%|█████████▉| 800/801 [00:01<00:00, 455.05it/s]\n",
      "count line size ./data/FN/fj-json/dev.csv: 2203L [00:00, 253152.10L/s]\n",
      "load dataset matched word: 100%|█████████▉| 2202/2203 [00:02<00:00, 1019.81it/s]\n",
      "count line size ./data/FN/fj-json/test.csv: 2204L [00:00, 449382.43L/s]\n",
      "load dataset matched word: 100%|█████████▉| 2203/2204 [00:02<00:00, 1069.03it/s]\n",
      "load vocab from list: 100%|██████████| 9645/9645 [00:00<00:00, 1226884.48it/s]\n",
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 31068.92L/s]\n",
      "build line mapper: 3L [00:00, 48395.82L/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 1116.89it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 59634.65it/s]\n",
      "count line size ./data/tencent/word_embedding.txt: 8824332L [00:06, 1272378.69L/s]\n",
      "build line mapper: 8824332L [00:15, 573717.07L/s]\n",
      "load word embedding...:   1%|          | 10308/1000000 [00:15<14:30, 1137.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.073052', '-0.454791', '-0.136675', '0.258515', '0.011885', '0.041317', '0.041896', '0.128609', '-0.163152', '0.246874', '0.092835', '-0.126660', '0.178079', '-0.286865', '0.027544', '-0.119339', '-0.007087', '-0.189252', '0.377924', '0.023270', '-0.367283', '-0.070051', '0.030247', '0.319996', '0.058990', '0.139447', '-0.126480', '0.268859', '-0.023930', '0.227637', '0.044645', '0.301265', '-0.234713', '0.074171', '-0.306097', '-0.403798', '0.140713', '-0.101598', '0.047656', '-0.276438', '-0.080730', '0.143143', '0.189617', '0.129756', '0.185805', '0.133780', '-0.318322', '-0.483692', '0.408157', '0.031308', '-0.311013', '0.059570', '-0.224590', '0.414112', '0.384909', '-0.305530', '0.131048', '0.119147', '0.059128', '-0.248051', '-0.138340', '-0.000503', '-0.100918', '0.304257', '-0.368741', '0.131073', '-0.237684', '-0.029870', '-0.244446', '-0.151965', '-0.221822', '0.489918', '0.114530', '0.175746', '-0.040124', '-0.295019', '-0.063676', '0.157994', '-0.011577', '-0.144796', '0.072814', '0.670234', '0.158069', '0.444338', '0.187207', '-0.178194', '0.441387', '-0.113229', '-0.311998', '-0.262850', '0.166731', '-0.074662', '-0.120234', '-0.557770', '0.217205', '-0.100354', '-0.315967', '-0.199034', '0.044077', '0.072797', '-0.041775', '0.057079', '0.252978', '0.155448', '-0.151973', '0.304786', '0.286322', '-0.461290', '-0.391043', '-0.356367', '0.050163', '-0.301053', '-0.258514', '0.085645', '0.219664', '-0.164307', '0.024513', '-0.077458', '-0.200825', '-0.418520', '0.258938', '-0.140829', '0.152272', '0.176600', '0.340460', '0.357031', '-0.039533', '0.113248', '0.152953', '-0.136520', '-0.167888', '-0.033082', '0.532537', '-0.086386', '-0.234421', '-0.337486', '-0.016323', '-0.018524', '-0.000276', '-0.252389', '-0.395252', '-0.324214', '-0.259109', '0.308163', '0.353517', '-0.195429', '0.319425', '-0.014722', '0.196231', '0.051138', '-0.161133', '-0.056247', '0.262345', '-0.192682', '0.272561', '0.010028', '0.139295', '-0.224138', '0.507957', '0.109516', '0.273037', '0.105556', '-0.182561', '0.069247', '0.102232', '-0.009683', '-0.127744', '0.224196', '-0.027222', '-0.057221', '0.096212', '0.126842', '-0.309821', '-0.031813', '0.135091', '-0.046443', '0.000263', '0.098762', '0.394687', '-0.081866', '0.021604', '0.033224', '-0.121693', '0.001640', '-0.080564', '0.109319', '-0.128558', '0.327593', '0.000599', '0.335171', '0.104823', '0.229730', '0.158929', '-0.203982', '0.355367', '0.507303', '-0.173355', '-0.388700', '-0.324610', '0.296036'] embedding error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load word embedding...: 100%|██████████| 1000000/1000000 [00:36<00:00, 27685.69it/s]\n",
      "load vocab embedding: 9652it [00:00, 19987.00it/s]\n",
      "load dataset from ./data/FN/fj+sc/train(json-400).csv: 100%|██████████| 800/800 [00:02<00:00, 344.51it/s]\n",
      "load dataset from ./data/FN/fj-json/dev.csv: 100%|██████████| 2202/2202 [00:05<00:00, 421.04it/s]\n",
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_transform.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.fuse_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.293, train_acc=0.933, train_loss=38.3, train_precision=0.403, train_recall=0.256]\n",
      "Eval Result: 100%|██████████| 35/35 [00:38<00:00,  1.11s/it, F1=0.83, eval_acc=0.974, eval_loss=10.7, eval_precision=0.786, eval_recall=0.879]\n",
      "Epoch: 2/30 Train: 100%|██████████| 200/200 [02:06<00:00,  1.58it/s, F1=0.694, train_acc=0.957, train_loss=17.1, train_precision=0.732, train_recall=0.679]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.89, eval_acc=0.982, eval_loss=5.84, eval_precision=0.887, eval_recall=0.895]\n",
      "Epoch: 3/30 Train: 100%|██████████| 200/200 [02:02<00:00,  1.63it/s, F1=0.751, train_acc=0.964, train_loss=12, train_precision=0.768, train_recall=0.744]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.913, eval_acc=0.985, eval_loss=4.78, eval_precision=0.91, eval_recall=0.916]\n",
      "Epoch: 4/30 Train: 100%|██████████| 200/200 [02:06<00:00,  1.58it/s, F1=0.777, train_acc=0.969, train_loss=9.61, train_precision=0.79, train_recall=0.772]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.07s/it, F1=0.914, eval_acc=0.986, eval_loss=5.19, eval_precision=0.902, eval_recall=0.926]\n",
      "Epoch: 5/30 Train: 100%|██████████| 200/200 [02:02<00:00,  1.63it/s, F1=0.795, train_acc=0.972, train_loss=8.15, train_precision=0.806, train_recall=0.793]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.07s/it, F1=0.914, eval_acc=0.986, eval_loss=4.71, eval_precision=0.908, eval_recall=0.921]\n",
      "Epoch: 6/30 Train: 100%|██████████| 200/200 [02:08<00:00,  1.56it/s, F1=0.816, train_acc=0.975, train_loss=6.97, train_precision=0.823, train_recall=0.816]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.917, eval_acc=0.986, eval_loss=4.34, eval_precision=0.91, eval_recall=0.926]\n",
      "Epoch: 7/30 Train: 100%|██████████| 200/200 [02:02<00:00,  1.63it/s, F1=0.844, train_acc=0.98, train_loss=5.9, train_precision=0.849, train_recall=0.845]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.07s/it, F1=0.916, eval_acc=0.987, eval_loss=5.57, eval_precision=0.897, eval_recall=0.936]\n",
      "Epoch: 8/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.841, train_acc=0.979, train_loss=5.54, train_precision=0.847, train_recall=0.841]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.07s/it, F1=0.915, eval_acc=0.987, eval_loss=5.26, eval_precision=0.891, eval_recall=0.94]\n",
      "Epoch: 9/30 Train: 100%|██████████| 200/200 [02:02<00:00,  1.64it/s, F1=0.863, train_acc=0.982, train_loss=4.68, train_precision=0.868, train_recall=0.865]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.921, eval_acc=0.987, eval_loss=5.22, eval_precision=0.908, eval_recall=0.935]\n",
      "Epoch: 10/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.881, train_acc=0.984, train_loss=4.07, train_precision=0.881, train_recall=0.886]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.913, eval_acc=0.986, eval_loss=5.68, eval_precision=0.919, eval_recall=0.909]\n",
      "Epoch: 11/30 Train: 100%|██████████| 200/200 [02:03<00:00,  1.62it/s, F1=0.889, train_acc=0.985, train_loss=4.02, train_precision=0.895, train_recall=0.886]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.08s/it, F1=0.914, eval_acc=0.985, eval_loss=5.3, eval_precision=0.908, eval_recall=0.919]\n",
      "Epoch: 12/30 Train: 100%|██████████| 200/200 [02:08<00:00,  1.56it/s, F1=0.894, train_acc=0.987, train_loss=3.5, train_precision=0.897, train_recall=0.896]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.915, eval_acc=0.986, eval_loss=5.63, eval_precision=0.918, eval_recall=0.913]\n",
      "Epoch: 13/30 Train: 100%|██████████| 200/200 [02:03<00:00,  1.62it/s, F1=0.895, train_acc=0.987, train_loss=3.14, train_precision=0.897, train_recall=0.895]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.914, eval_acc=0.986, eval_loss=6.56, eval_precision=0.918, eval_recall=0.911]\n",
      "Epoch: 14/30 Train: 100%|██████████| 200/200 [02:06<00:00,  1.58it/s, F1=0.917, train_acc=0.989, train_loss=2.78, train_precision=0.918, train_recall=0.918]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.07s/it, F1=0.912, eval_acc=0.986, eval_loss=5.37, eval_precision=0.92, eval_recall=0.904]\n",
      "Epoch: 15/30 Train: 100%|██████████| 200/200 [02:04<00:00,  1.60it/s, F1=0.924, train_acc=0.989, train_loss=2.58, train_precision=0.924, train_recall=0.926]\n",
      "Eval Result: 100%|██████████| 35/35 [00:41<00:00,  1.18s/it, F1=0.919, eval_acc=0.987, eval_loss=5.48, eval_precision=0.912, eval_recall=0.927]\n",
      "Epoch: 16/30 Train: 100%|██████████| 200/200 [02:04<00:00,  1.61it/s, F1=0.929, train_acc=0.991, train_loss=2.15, train_precision=0.932, train_recall=0.928]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.07s/it, F1=0.919, eval_acc=0.986, eval_loss=5.81, eval_precision=0.914, eval_recall=0.924]\n",
      "Epoch: 17/30 Train: 100%|██████████| 200/200 [02:05<00:00,  1.60it/s, F1=0.935, train_acc=0.992, train_loss=1.95, train_precision=0.936, train_recall=0.936]\n",
      "Eval Result: 100%|██████████| 35/35 [00:39<00:00,  1.11s/it, F1=0.917, eval_acc=0.986, eval_loss=6.04, eval_precision=0.923, eval_recall=0.912]\n",
      "Epoch: 18/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.56it/s, F1=0.939, train_acc=0.993, train_loss=1.83, train_precision=0.941, train_recall=0.939]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.08s/it, F1=0.903, eval_acc=0.985, eval_loss=7.64, eval_precision=0.905, eval_recall=0.902]\n",
      "Epoch: 19/30 Train: 100%|██████████| 200/200 [02:04<00:00,  1.61it/s, F1=0.943, train_acc=0.992, train_loss=2, train_precision=0.946, train_recall=0.942]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.917, eval_acc=0.986, eval_loss=5.99, eval_precision=0.923, eval_recall=0.912]\n",
      "Epoch: 20/30 Train: 100%|██████████| 200/200 [02:06<00:00,  1.58it/s, F1=0.95, train_acc=0.994, train_loss=1.47, train_precision=0.95, train_recall=0.951]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.918, eval_acc=0.986, eval_loss=7.36, eval_precision=0.921, eval_recall=0.915]\n",
      "Epoch: 21/30 Train: 100%|██████████| 200/200 [02:03<00:00,  1.62it/s, F1=0.945, train_acc=0.993, train_loss=1.7, train_precision=0.947, train_recall=0.944]\n",
      "Eval Result: 100%|██████████| 35/35 [00:42<00:00,  1.21s/it, F1=0.922, eval_acc=0.987, eval_loss=7.08, eval_precision=0.915, eval_recall=0.928]\n",
      "Epoch: 22/30 Train: 100%|██████████| 200/200 [02:03<00:00,  1.61it/s, F1=0.957, train_acc=0.995, train_loss=1.34, train_precision=0.958, train_recall=0.957]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.918, eval_acc=0.986, eval_loss=7.65, eval_precision=0.929, eval_recall=0.907]\n",
      "Epoch: 23/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.96, train_acc=0.995, train_loss=1.14, train_precision=0.962, train_recall=0.96]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.08s/it, F1=0.915, eval_acc=0.986, eval_loss=6.66, eval_precision=0.928, eval_recall=0.902]\n",
      "Epoch: 24/30 Train: 100%|██████████| 200/200 [02:03<00:00,  1.62it/s, F1=0.966, train_acc=0.996, train_loss=1.04, train_precision=0.968, train_recall=0.966]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.922, eval_acc=0.987, eval_loss=7.85, eval_precision=0.919, eval_recall=0.925]\n",
      "Epoch: 25/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.966, train_acc=0.996, train_loss=1.13, train_precision=0.968, train_recall=0.965]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.07s/it, F1=0.915, eval_acc=0.986, eval_loss=7.35, eval_precision=0.918, eval_recall=0.911]\n",
      "Epoch: 26/30 Train: 100%|██████████| 200/200 [02:03<00:00,  1.62it/s, F1=0.958, train_acc=0.994, train_loss=1.27, train_precision=0.962, train_recall=0.957]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.914, eval_acc=0.986, eval_loss=5.99, eval_precision=0.928, eval_recall=0.901]\n",
      "Epoch: 27/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.965, train_acc=0.995, train_loss=1.14, train_precision=0.966, train_recall=0.964]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.08s/it, F1=0.919, eval_acc=0.986, eval_loss=7.74, eval_precision=0.919, eval_recall=0.92]\n",
      "Epoch: 28/30 Train: 100%|██████████| 200/200 [02:04<00:00,  1.60it/s, F1=0.972, train_acc=0.996, train_loss=0.952, train_precision=0.974, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 35/35 [00:38<00:00,  1.09s/it, F1=0.913, eval_acc=0.985, eval_loss=6.97, eval_precision=0.931, eval_recall=0.896]\n",
      "Epoch: 29/30 Train: 100%|██████████| 200/200 [02:07<00:00,  1.57it/s, F1=0.974, train_acc=0.996, train_loss=0.801, train_precision=0.976, train_recall=0.971]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.08s/it, F1=0.919, eval_acc=0.987, eval_loss=7.26, eval_precision=0.925, eval_recall=0.914]\n",
      "Epoch: 30/30 Train: 100%|██████████| 200/200 [02:04<00:00,  1.61it/s, F1=0.972, train_acc=0.997, train_loss=0.723, train_precision=0.973, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it, F1=0.919, eval_acc=0.986, eval_loss=5.21, eval_precision=0.923, eval_recall=0.915]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_3/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_4/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_5/Bert_95250/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj+sc/train(json-400).csv',\n",
    "    'eval_file': './data/FN/fj-json/dev.csv',\n",
    "    'test_file': './data/FN/fj-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN-fj+sc(400)-fj-LeBert'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj-json/train.csv etag: 100%|██████████| 77.8M/77.8M [00:00<00:00, 595MB/s]\n",
      "calculate ./data/FN/fj-json/dev.csv etag: 100%|██████████| 9.75M/9.75M [00:00<00:00, 498MB/s]\n",
      "calculate ./data/FN/fj-json/test.csv etag: 100%|██████████| 9.64M/9.64M [00:00<00:00, 532MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 23.8kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 16,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj-json/train.csv\",\n",
      "    \"eval_file\": \"./data/FN/fj-json/dev.csv\",\n",
      "    \"test_file\": \"./data/FN/fj-json/test.csv\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"FN-fj-LeBert\"\n",
      "}\n",
      "load cached ./temp/86febf5809301cc61412128a9e7daf95-16_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 21147.75L/s]\n",
      "build line mapper: 3L [00:00, 28024.30L/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 1055.52it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 67650.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/86febf5809301cc61412128a9e7daf95-16_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/86febf5809301cc61412128a9e7daf95-16_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n",
      "load cached ./temp/86febf5809301cc61412128a9e7daf95-16_b33e995d7216dd5ffcae7f0ab39c4b70-2_f1f8f06bec25f4644916ad9ee910da2a-2_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/zl/miniconda3/envs/FN/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "load dataset from ./data/FN/fj-json/train.csv: 100%|██████████| 17615/17615 [00:40<00:00, 439.87it/s]\n",
      "load dataset from ./data/FN/fj-json/dev.csv: 100%|██████████| 2202/2202 [00:05<00:00, 435.49it/s]\n",
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.bias', 'word_embeddings.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.fuse_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train:   3%|▎         | 33/1101 [00:36<16:48,  1.06it/s, F1=0, train_acc=0.521, train_loss=581, train_precision=0, train_recall=0]/home/zl/miniconda3/envs/FN/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 1101/1101 [17:20<00:00,  1.06it/s, F1=0.792, train_acc=0.97, train_loss=31.1, train_precision=0.773, train_recall=0.814]\n",
      "Eval Result: 100%|██████████| 35/35 [00:37<00:00,  1.08s/it, F1=0.941, eval_acc=0.993, eval_loss=4.99, eval_precision=0.923, eval_recall=0.959]\n",
      "Epoch: 2/30 Train: 100%|██████████| 1101/1101 [17:17<00:00,  1.06it/s, F1=0.945, train_acc=0.993, train_loss=4.21, train_precision=0.933, train_recall=0.958]\n",
      "Eval Result: 100%|██████████| 35/35 [00:39<00:00,  1.12s/it, F1=0.95, eval_acc=0.994, eval_loss=3.42, eval_precision=0.94, eval_recall=0.961]\n",
      "Epoch: 3/30 Train: 100%|██████████| 1101/1101 [18:54<00:00,  1.03s/it, F1=0.956, train_acc=0.994, train_loss=2.74, train_precision=0.947, train_recall=0.964]\n",
      "Eval Result: 100%|██████████| 35/35 [00:39<00:00,  1.14s/it, F1=0.959, eval_acc=0.994, eval_loss=2.49, eval_precision=0.948, eval_recall=0.97]\n",
      "Epoch: 4/30 Train: 100%|██████████| 1101/1101 [19:05<00:00,  1.04s/it, F1=0.963, train_acc=0.995, train_loss=1.89, train_precision=0.957, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 35/35 [00:39<00:00,  1.14s/it, F1=0.96, eval_acc=0.994, eval_loss=2.29, eval_precision=0.949, eval_recall=0.972]\n",
      "Epoch: 5/30 Train: 100%|██████████| 1101/1101 [19:01<00:00,  1.04s/it, F1=0.966, train_acc=0.995, train_loss=1.38, train_precision=0.962, train_recall=0.971]\n",
      "Eval Result: 100%|██████████| 35/35 [00:40<00:00,  1.14s/it, F1=0.963, eval_acc=0.994, eval_loss=1.76, eval_precision=0.955, eval_recall=0.972]\n",
      "Epoch: 6/30 Train: 100%|██████████| 1101/1101 [19:01<00:00,  1.04s/it, F1=0.97, train_acc=0.996, train_loss=1.06, train_precision=0.967, train_recall=0.974]\n",
      "Eval Result: 100%|██████████| 35/35 [00:39<00:00,  1.13s/it, F1=0.962, eval_acc=0.994, eval_loss=1.66, eval_precision=0.954, eval_recall=0.97]\n",
      "Epoch: 7/30 Train: 100%|██████████| 1101/1101 [19:01<00:00,  1.04s/it, F1=0.973, train_acc=0.996, train_loss=0.831, train_precision=0.97, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 35/35 [00:41<00:00,  1.19s/it, F1=0.963, eval_acc=0.994, eval_loss=1.83, eval_precision=0.951, eval_recall=0.974]\n",
      "Epoch: 8/30 Train: 100%|██████████| 1101/1101 [19:01<00:00,  1.04s/it, F1=0.976, train_acc=0.997, train_loss=0.671, train_precision=0.974, train_recall=0.978]\n",
      "Eval Result: 100%|██████████| 35/35 [00:39<00:00,  1.14s/it, F1=0.962, eval_acc=0.994, eval_loss=1.57, eval_precision=0.95, eval_recall=0.974]\n",
      "Epoch: 9/30 Train: 100%|██████████| 1101/1101 [19:04<00:00,  1.04s/it, F1=0.977, train_acc=0.997, train_loss=0.583, train_precision=0.976, train_recall=0.979]\n",
      "Eval Result: 100%|██████████| 35/35 [00:39<00:00,  1.14s/it, F1=0.962, eval_acc=0.994, eval_loss=1.74, eval_precision=0.95, eval_recall=0.975]\n",
      "Epoch: 10/30 Train:  33%|███▎      | 358/1101 [06:12<12:54,  1.04s/it, F1=0.98, train_acc=0.997, train_loss=0.489, train_precision=0.978, train_recall=0.981]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11383/2341748808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNERTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/trainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **arg)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_path, resume_step, lr1, lr2)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mtrain_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbirnncrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/birnncrf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeds, masks)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Get the emission scores from the BiLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/crf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, masks)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \"\"\"\n\u001b[1;32m     44\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__viterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NER/ccNERx-main/CC/crf.py\u001b[0m in \u001b[0;36m__viterbi_decode\u001b[0;34m(self, features, masks)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0macc_score_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_score_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0macc_score_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0memit_score_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_score_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_score\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmask_t\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# max_score or acc_score_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Transition to STOP_TAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_3/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_4/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_5/Bert_95250/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj-json/train.csv',\n",
    "    'eval_file': './data/FN/fj-json/dev.csv',\n",
    "    'test_file': './data/FN/fj-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN-fj-LeBert'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "from CC.loaders.label_le_loader import LabelLLoader\n",
    "from CC.predicter import NERPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_3/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_4/Bert_95250/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_Pretrained_5/Bert_95250/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj+sc/train(json-400).csv',\n",
    "    'eval_file': './data/FN/sc-json/dev.csv',\n",
    "    'test_file': './data/FN/sc-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN-fj+sc(400)-sc-LeBert',\n",
    "    'lstm_crf_model_file':'./save_model/FN-fj+sc(400)-sc-LeBert/lstm_crf/lstm_crf_6000.pth',\n",
    "    'bert_model_file':'./save_model/FN-fj+sc(400)-sc-LeBert/LEBert/LEBert_6000.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/fj+sc/train(json-400).csv etag: 100%|██████████| 6.47M/6.47M [00:00<00:00, 399MB/s]\n",
      "calculate ./data/FN/sc-json/dev.csv etag: 100%|██████████| 346k/346k [00:00<00:00, 260MB/s]\n",
      "calculate ./data/FN/sc-json/test.csv etag: 100%|██████████| 318k/318k [00:00<00:00, 375MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 22.3kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"use_gpu\": true,\n",
      "    \"loader_name\": \"le_loader\",\n",
      "    \"model_name\": \"LEBert\",\n",
      "    \"lstm_crf_model_file\": \"./save_model/FN-fj+sc(400)-sc-LeBert/lstm_crf/lstm_crf_6000.pth\",\n",
      "    \"bert_model_file\": \"./save_model/FN-fj+sc(400)-sc-LeBert/LEBert/LEBert_6000.pth\",\n",
      "    \"hidden_dim\": 300,\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"bert_config_file_name\": \"./model/chinese_wwm_ext/bert_config.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"padding_length\": 512,\n",
      "    \"num_gpus\": [\n",
      "        0,\n",
      "        1,\n",
      "        2,\n",
      "        3\n",
      "    ]\n",
      "}\n",
      "kwargs parser: {\n",
      "    \"batch_size\": 4,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/fj+sc/train(json-400).csv\",\n",
      "    \"eval_file\": \"./data/FN/sc-json/dev.csv\",\n",
      "    \"test_file\": \"./data/FN/sc-json/test.csv\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": true,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": true,\n",
      "    \"task_name\": \"FN-fj+sc(400)-sc-LeBert\"\n",
      "}\n",
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 43539.49L/s]\n",
      "build line mapper: 3L [00:00, 50942.96L/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 1039.05it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 44150.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n",
      "load cached ./temp/3ea3284374ca8786f9547229fe3b41c3-2_7934888989a34292bc50ec19762699e9_6c966819d6db1103e1e8f221e408821e_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['B',\n",
       "   'M',\n",
       "   'I',\n",
       "   ':',\n",
       "   '2',\n",
       "   '3',\n",
       "   '.',\n",
       "   '1',\n",
       "   '8',\n",
       "   'k',\n",
       "   'g',\n",
       "   '/',\n",
       "   '㎡',\n",
       "   '体',\n",
       "   '重',\n",
       "   '正',\n",
       "   '常'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " (['胆', '囊', '胆', '固', '醇', '沉', '着'],\n",
       "  ['B-KEYWORD',\n",
       "   'I-KEYWORD',\n",
       "   'I-KEYWORD',\n",
       "   'I-KEYWORD',\n",
       "   'I-KEYWORD',\n",
       "   'I-KEYWORD',\n",
       "   'I-KEYWORD']),\n",
       " (['胆', '囊', '结', '石'], ['B-KEYWORD', 'I-KEYWORD', 'I-KEYWORD', 'I-KEYWORD']),\n",
       " (['心', '肌', '损', '伤', '酶', '谱', '：', '正', '常', ';', '右', '肾', '囊', '肿'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-KEYWORD',\n",
       "   'I-KEYWORD',\n",
       "   'I-KEYWORD',\n",
       "   'I-KEYWORD'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = NERPredict(**args)\n",
    "predict([\"BMI:23.18kg/㎡体重正常\",\"胆囊胆固醇沉着\",\"胆囊结石\", \"心肌损伤酶谱：正常;右肾囊肿\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9251cfec689d76e08d0a8937412ac104f3bf079996eeda446f48ff04929e0b8b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('FN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
