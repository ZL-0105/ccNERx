{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### daga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json\",\n",
      "    \"eval_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"test_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"tag_file\": \"data/ccks/ccks_tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"ccks_daga_resnet_drop_bi_2_1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json etag: 100%|██████████| 5.49M/5.49M [00:00<00:00, 290MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 46.5MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 304MB/s]\n",
      "calculate data/ccks/ccks_tags_list.txt etag: 100%|██████████| 85.0/85.0 [00:00<00:00, 151kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size data/ccks/ccks_tags_list.txt: 13L [00:00, 26468.91L/s]\n",
      "build line mapper: 13L [00:00, 17348.38L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 13/13 [00:00<00:00, 2831.05it/s]\n",
      "load vocab from list: 100%|██████████| 13/13 [00:00<00:00, 98422.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/vocab_embedding\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_word_vocab\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json: 100%|██████████| 2812/2812 [00:05<00:00, 542.90it/s] \n",
      "load dataset from ./data/ccks/subtask1_test.json: 100%|██████████| 379/379 [00:01<00:00, 255.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'inter_word_embeddings.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train:   4%|▍         | 14/352 [00:06<02:23,  2.36it/s, F1=0, train_acc=0.116, train_loss=180, train_precision=0, train_recall=0]  /home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.56, train_acc=0.887, train_loss=23.3, train_precision=0.568, train_recall=0.56]         \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.769, eval_acc=0.957, eval_loss=13.6, eval_precision=0.73, eval_recall=0.813] \n",
      "Epoch: 2/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.44it/s, F1=0.813, train_acc=0.961, train_loss=5.23, train_precision=0.818, train_recall=0.81] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.90it/s, F1=0.799, eval_acc=0.962, eval_loss=10.2, eval_precision=0.772, eval_recall=0.827]\n",
      "Epoch: 3/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.36it/s, F1=0.845, train_acc=0.969, train_loss=3.67, train_precision=0.85, train_recall=0.842] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.784, eval_acc=0.958, eval_loss=11, eval_precision=0.757, eval_recall=0.813]  \n",
      "Epoch: 4/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.869, train_acc=0.974, train_loss=2.85, train_precision=0.871, train_recall=0.867]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.809, eval_acc=0.96, eval_loss=12.7, eval_precision=0.827, eval_recall=0.793] \n",
      "Epoch: 5/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.892, train_acc=0.979, train_loss=2.32, train_precision=0.895, train_recall=0.89] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.815, eval_acc=0.962, eval_loss=11.1, eval_precision=0.811, eval_recall=0.819]\n",
      "Epoch: 6/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.908, train_acc=0.983, train_loss=2.01, train_precision=0.911, train_recall=0.906]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.811, eval_acc=0.962, eval_loss=11, eval_precision=0.796, eval_recall=0.827]  \n",
      "Epoch: 7/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.916, train_acc=0.984, train_loss=1.85, train_precision=0.919, train_recall=0.915]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.89it/s, F1=0.816, eval_acc=0.965, eval_loss=12.2, eval_precision=0.799, eval_recall=0.835]\n",
      "Epoch: 8/30 Train: 100%|██████████| 352/352 [02:23<00:00,  2.45it/s, F1=0.919, train_acc=0.984, train_loss=1.81, train_precision=0.921, train_recall=0.917]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.828, eval_acc=0.968, eval_loss=11.1, eval_precision=0.812, eval_recall=0.844]\n",
      "Epoch: 9/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.928, train_acc=0.986, train_loss=1.54, train_precision=0.929, train_recall=0.928]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s, F1=0.827, eval_acc=0.969, eval_loss=11, eval_precision=0.817, eval_recall=0.838]  \n",
      "Epoch: 10/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.932, train_acc=0.988, train_loss=1.38, train_precision=0.932, train_recall=0.932]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.824, eval_acc=0.969, eval_loss=13.2, eval_precision=0.821, eval_recall=0.827]\n",
      "Epoch: 11/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.44it/s, F1=0.939, train_acc=0.989, train_loss=1.34, train_precision=0.939, train_recall=0.939]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.821, eval_acc=0.967, eval_loss=12.1, eval_precision=0.813, eval_recall=0.83] \n",
      "Epoch: 12/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.948, train_acc=0.99, train_loss=1.17, train_precision=0.949, train_recall=0.947] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.82, eval_acc=0.966, eval_loss=13.5, eval_precision=0.805, eval_recall=0.837] \n",
      "Epoch: 13/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.949, train_acc=0.99, train_loss=1.19, train_precision=0.95, train_recall=0.949]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.83it/s, F1=0.81, eval_acc=0.963, eval_loss=12.7, eval_precision=0.796, eval_recall=0.826] \n",
      "Epoch: 14/30 Train: 100%|██████████| 352/352 [02:23<00:00,  2.46it/s, F1=0.949, train_acc=0.99, train_loss=1.09, train_precision=0.95, train_recall=0.949]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.813, eval_acc=0.964, eval_loss=13.4, eval_precision=0.796, eval_recall=0.831]\n",
      "Epoch: 15/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.957, train_acc=0.993, train_loss=0.878, train_precision=0.957, train_recall=0.956]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.818, eval_acc=0.967, eval_loss=12.8, eval_precision=0.815, eval_recall=0.822]\n",
      "Epoch: 16/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.959, train_acc=0.994, train_loss=0.833, train_precision=0.96, train_recall=0.958] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.824, eval_acc=0.968, eval_loss=12.8, eval_precision=0.819, eval_recall=0.829]\n",
      "Epoch: 17/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.44it/s, F1=0.956, train_acc=0.992, train_loss=0.959, train_precision=0.957, train_recall=0.956]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.95it/s, F1=0.818, eval_acc=0.967, eval_loss=13.2, eval_precision=0.811, eval_recall=0.825]\n",
      "Epoch: 18/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.961, train_acc=0.993, train_loss=0.864, train_precision=0.962, train_recall=0.96] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.82, eval_acc=0.964, eval_loss=12.9, eval_precision=0.814, eval_recall=0.827] \n",
      "Epoch: 19/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.971, train_acc=0.995, train_loss=0.61, train_precision=0.971, train_recall=0.971] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.816, eval_acc=0.964, eval_loss=14.5, eval_precision=0.822, eval_recall=0.812]\n",
      "Epoch: 20/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.41it/s, F1=0.967, train_acc=0.994, train_loss=0.758, train_precision=0.968, train_recall=0.967]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.89it/s, F1=0.821, eval_acc=0.967, eval_loss=14.4, eval_precision=0.816, eval_recall=0.828]\n",
      "Epoch: 21/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.41it/s, F1=0.968, train_acc=0.994, train_loss=0.719, train_precision=0.968, train_recall=0.968]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.816, eval_acc=0.965, eval_loss=15, eval_precision=0.808, eval_recall=0.825]  \n",
      "Epoch: 22/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.969, train_acc=0.994, train_loss=0.749, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.91it/s, F1=0.811, eval_acc=0.964, eval_loss=15.6, eval_precision=0.796, eval_recall=0.828]\n",
      "Epoch: 23/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.44it/s, F1=0.969, train_acc=0.994, train_loss=0.695, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.824, eval_acc=0.965, eval_loss=14.3, eval_precision=0.833, eval_recall=0.815]\n",
      "Epoch: 24/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.971, train_acc=0.995, train_loss=0.621, train_precision=0.971, train_recall=0.97] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.821, eval_acc=0.965, eval_loss=17.4, eval_precision=0.813, eval_recall=0.829]\n",
      "Epoch: 25/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.97, train_acc=0.995, train_loss=0.692, train_precision=0.97, train_recall=0.969]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.824, eval_acc=0.965, eval_loss=16.1, eval_precision=0.823, eval_recall=0.825]\n",
      "Epoch: 26/30 Train: 100%|██████████| 352/352 [02:23<00:00,  2.45it/s, F1=0.97, train_acc=0.995, train_loss=0.69, train_precision=0.971, train_recall=0.969]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.83it/s, F1=0.808, eval_acc=0.964, eval_loss=20.3, eval_precision=0.798, eval_recall=0.819]\n",
      "Epoch: 27/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.43it/s, F1=0.968, train_acc=0.995, train_loss=0.778, train_precision=0.968, train_recall=0.968]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.819, eval_acc=0.966, eval_loss=15.6, eval_precision=0.826, eval_recall=0.814]\n",
      "Epoch: 28/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.37it/s, F1=0.976, train_acc=0.996, train_loss=0.545, train_precision=0.977, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.815, eval_acc=0.964, eval_loss=16.5, eval_precision=0.802, eval_recall=0.828]\n",
      "Epoch: 29/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.976, train_acc=0.996, train_loss=0.516, train_precision=0.976, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.814, eval_acc=0.965, eval_loss=18, eval_precision=0.813, eval_recall=0.816]  \n",
      "Epoch: 30/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.43it/s, F1=0.976, train_acc=0.996, train_loss=0.543, train_precision=0.977, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.89it/s, F1=0.829, eval_acc=0.968, eval_loss=15.6, eval_precision=0.825, eval_recall=0.833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json\",\n",
      "    \"eval_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"test_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"tag_file\": \"data/ccks/ccks_tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"ccks_daga_resnet_drop_bi_2_2\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json etag: 100%|██████████| 5.49M/5.49M [00:00<00:00, 94.8MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 40.5MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 351MB/s]\n",
      "calculate data/ccks/ccks_tags_list.txt etag: 100%|██████████| 85.0/85.0 [00:00<00:00, 191kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/lexicon_tree\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size data/ccks/ccks_tags_list.txt: 13L [00:00, 97892.19L/s]\n",
      "build line mapper: 13L [00:00, 61750.80L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 13/13 [00:00<00:00, 2740.00it/s]\n",
      "load vocab from list: 100%|██████████| 13/13 [00:00<00:00, 141994.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_lexicon_tree\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_word_vocab\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json: 100%|██████████| 2812/2812 [00:05<00:00, 556.29it/s] \n",
      "load dataset from ./data/ccks/subtask1_test.json: 100%|██████████| 379/379 [00:01<00:00, 254.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'inter_word_embeddings.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.57, train_acc=0.906, train_loss=19.6, train_precision=0.579, train_recall=0.569]        \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.788, eval_acc=0.96, eval_loss=13.3, eval_precision=0.792, eval_recall=0.785] \n",
      "Epoch: 2/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.822, train_acc=0.965, train_loss=5.2, train_precision=0.828, train_recall=0.818] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.811, eval_acc=0.964, eval_loss=10.4, eval_precision=0.813, eval_recall=0.809]\n",
      "Epoch: 3/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.855, train_acc=0.972, train_loss=3.61, train_precision=0.862, train_recall=0.85] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s, F1=0.807, eval_acc=0.961, eval_loss=12.1, eval_precision=0.812, eval_recall=0.803]\n",
      "Epoch: 4/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.42it/s, F1=0.877, train_acc=0.977, train_loss=2.92, train_precision=0.883, train_recall=0.872]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.8, eval_acc=0.96, eval_loss=12.4, eval_precision=0.802, eval_recall=0.8]     \n",
      "Epoch: 5/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.89, train_acc=0.98, train_loss=2.43, train_precision=0.894, train_recall=0.888]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.77it/s, F1=0.789, eval_acc=0.96, eval_loss=12.5, eval_precision=0.781, eval_recall=0.797] \n",
      "Epoch: 6/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.908, train_acc=0.983, train_loss=2.07, train_precision=0.911, train_recall=0.905]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.83it/s, F1=0.802, eval_acc=0.964, eval_loss=11.5, eval_precision=0.801, eval_recall=0.804]\n",
      "Epoch: 7/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.923, train_acc=0.986, train_loss=1.83, train_precision=0.926, train_recall=0.922]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.824, eval_acc=0.968, eval_loss=10.2, eval_precision=0.82, eval_recall=0.827] \n",
      "Epoch: 8/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.928, train_acc=0.986, train_loss=1.72, train_precision=0.93, train_recall=0.928] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.808, eval_acc=0.961, eval_loss=11.9, eval_precision=0.802, eval_recall=0.813]\n",
      "Epoch: 9/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.932, train_acc=0.988, train_loss=1.61, train_precision=0.933, train_recall=0.93] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.819, eval_acc=0.967, eval_loss=11.4, eval_precision=0.82, eval_recall=0.818] \n",
      "Epoch: 10/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.41it/s, F1=0.935, train_acc=0.988, train_loss=1.55, train_precision=0.937, train_recall=0.934]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.826, eval_acc=0.966, eval_loss=11.8, eval_precision=0.843, eval_recall=0.809]\n",
      "Epoch: 11/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.946, train_acc=0.99, train_loss=1.44, train_precision=0.947, train_recall=0.946] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.811, eval_acc=0.962, eval_loss=15.5, eval_precision=0.846, eval_recall=0.78] \n",
      "Epoch: 12/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.942, train_acc=0.99, train_loss=1.39, train_precision=0.944, train_recall=0.941] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.83, eval_acc=0.969, eval_loss=10.4, eval_precision=0.843, eval_recall=0.818] \n",
      "Epoch: 13/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.946, train_acc=0.991, train_loss=1.25, train_precision=0.948, train_recall=0.945]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.823, eval_acc=0.968, eval_loss=12.4, eval_precision=0.832, eval_recall=0.815]\n",
      "Epoch: 14/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.943, train_acc=0.99, train_loss=1.39, train_precision=0.944, train_recall=0.942] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s, F1=0.818, eval_acc=0.966, eval_loss=13.1, eval_precision=0.821, eval_recall=0.816]\n",
      "Epoch: 15/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.945, train_acc=0.99, train_loss=1.13, train_precision=0.946, train_recall=0.944] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s, F1=0.823, eval_acc=0.967, eval_loss=12.4, eval_precision=0.83, eval_recall=0.817] \n",
      "Epoch: 16/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.42it/s, F1=0.958, train_acc=0.993, train_loss=0.94, train_precision=0.959, train_recall=0.958] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.814, eval_acc=0.965, eval_loss=12.2, eval_precision=0.821, eval_recall=0.806]\n",
      "Epoch: 17/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.44it/s, F1=0.96, train_acc=0.993, train_loss=0.961, train_precision=0.961, train_recall=0.96]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.828, eval_acc=0.968, eval_loss=11.3, eval_precision=0.838, eval_recall=0.818]\n",
      "Epoch: 18/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.964, train_acc=0.994, train_loss=0.781, train_precision=0.966, train_recall=0.963]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.821, eval_acc=0.967, eval_loss=13, eval_precision=0.835, eval_recall=0.808]  \n",
      "Epoch: 19/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.965, train_acc=0.993, train_loss=0.879, train_precision=0.966, train_recall=0.965]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.823, eval_acc=0.967, eval_loss=13.2, eval_precision=0.823, eval_recall=0.822]\n",
      "Epoch: 20/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.43it/s, F1=0.962, train_acc=0.993, train_loss=0.852, train_precision=0.962, train_recall=0.963]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.821, eval_acc=0.965, eval_loss=13.7, eval_precision=0.853, eval_recall=0.792]\n",
      "Epoch: 21/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.964, train_acc=0.995, train_loss=0.738, train_precision=0.966, train_recall=0.964]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.77it/s, F1=0.816, eval_acc=0.965, eval_loss=14.2, eval_precision=0.825, eval_recall=0.808]\n",
      "Epoch: 22/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.41it/s, F1=0.971, train_acc=0.995, train_loss=0.689, train_precision=0.972, train_recall=0.97] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.816, eval_acc=0.965, eval_loss=14.5, eval_precision=0.835, eval_recall=0.799]\n",
      "Epoch: 23/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.43it/s, F1=0.974, train_acc=0.996, train_loss=0.622, train_precision=0.974, train_recall=0.974]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.814, eval_acc=0.964, eval_loss=17.1, eval_precision=0.834, eval_recall=0.796]\n",
      "Epoch: 24/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.97, train_acc=0.995, train_loss=0.721, train_precision=0.971, train_recall=0.969] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.83it/s, F1=0.823, eval_acc=0.967, eval_loss=15.4, eval_precision=0.833, eval_recall=0.813]\n",
      "Epoch: 25/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.971, train_acc=0.995, train_loss=0.707, train_precision=0.971, train_recall=0.97] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.826, eval_acc=0.969, eval_loss=15.8, eval_precision=0.83, eval_recall=0.822] \n",
      "Epoch: 26/30 Train: 100%|██████████| 352/352 [02:24<00:00,  2.43it/s, F1=0.971, train_acc=0.995, train_loss=0.656, train_precision=0.971, train_recall=0.97] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.815, eval_acc=0.965, eval_loss=16.8, eval_precision=0.827, eval_recall=0.803]\n",
      "Epoch: 27/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.38it/s, F1=0.974, train_acc=0.996, train_loss=0.663, train_precision=0.975, train_recall=0.973]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s, F1=0.809, eval_acc=0.965, eval_loss=20.4, eval_precision=0.826, eval_recall=0.793]\n",
      "Epoch: 28/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.36it/s, F1=0.973, train_acc=0.996, train_loss=0.668, train_precision=0.974, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s, F1=0.816, eval_acc=0.966, eval_loss=16.1, eval_precision=0.813, eval_recall=0.82] \n",
      "Epoch: 29/30 Train: 100%|██████████| 352/352 [02:23<00:00,  2.45it/s, F1=0.949, train_acc=0.991, train_loss=1.48, train_precision=0.951, train_recall=0.947] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.818, eval_acc=0.964, eval_loss=15.5, eval_precision=0.816, eval_recall=0.82] \n",
      "Epoch: 30/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.969, train_acc=0.995, train_loss=0.681, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.74it/s, F1=0.829, eval_acc=0.968, eval_loss=15.6, eval_precision=0.829, eval_recall=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json\",\n",
      "    \"eval_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"test_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"tag_file\": \"data/ccks/ccks_tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"ccks_daga_resnet_drop_bi_2_3\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json etag: 100%|██████████| 5.49M/5.49M [00:00<00:00, 312MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 335MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 349MB/s]\n",
      "calculate data/ccks/ccks_tags_list.txt etag: 100%|██████████| 85.0/85.0 [00:00<00:00, 220kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/lexicon_tree\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size data/ccks/ccks_tags_list.txt: 13L [00:00, 55809.57L/s]\n",
      "build line mapper: 13L [00:00, 62386.67L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 13/13 [00:00<00:00, 2353.60it/s]\n",
      "load vocab from list: 100%|██████████| 13/13 [00:00<00:00, 82615.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/vocab_embedding\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_word_vocab\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json: 100%|██████████| 2812/2812 [00:05<00:00, 478.75it/s] \n",
      "load dataset from ./data/ccks/subtask1_test.json: 100%|██████████| 379/379 [00:01<00:00, 242.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'inter_word_embeddings.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.474, train_acc=0.886, train_loss=30.3, train_precision=0.445, train_recall=0.518]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.714, eval_acc=0.956, eval_loss=17.4, eval_precision=0.73, eval_recall=0.701] \n",
      "Epoch: 2/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.42it/s, F1=0.785, train_acc=0.959, train_loss=7.01, train_precision=0.782, train_recall=0.79] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.803, eval_acc=0.966, eval_loss=11.8, eval_precision=0.815, eval_recall=0.793]\n",
      "Epoch: 3/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.33it/s, F1=0.84, train_acc=0.969, train_loss=4.36, train_precision=0.844, train_recall=0.839] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.73it/s, F1=0.816, eval_acc=0.967, eval_loss=9.93, eval_precision=0.822, eval_recall=0.81] \n",
      "Epoch: 4/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.34it/s, F1=0.871, train_acc=0.976, train_loss=3.11, train_precision=0.876, train_recall=0.869]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.809, eval_acc=0.966, eval_loss=10.5, eval_precision=0.825, eval_recall=0.795]\n",
      "Epoch: 5/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.879, train_acc=0.977, train_loss=2.92, train_precision=0.882, train_recall=0.878]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s, F1=0.829, eval_acc=0.969, eval_loss=9.85, eval_precision=0.827, eval_recall=0.831]\n",
      "Epoch: 6/30 Train: 100%|██████████| 352/352 [02:33<00:00,  2.29it/s, F1=0.899, train_acc=0.982, train_loss=2.23, train_precision=0.902, train_recall=0.898]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.825, eval_acc=0.965, eval_loss=10.8, eval_precision=0.841, eval_recall=0.811]\n",
      "Epoch: 7/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.911, train_acc=0.984, train_loss=1.88, train_precision=0.914, train_recall=0.91] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.829, eval_acc=0.968, eval_loss=10.6, eval_precision=0.845, eval_recall=0.814]\n",
      "Epoch: 8/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.38it/s, F1=0.92, train_acc=0.986, train_loss=1.75, train_precision=0.922, train_recall=0.92]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.818, eval_acc=0.966, eval_loss=11.3, eval_precision=0.839, eval_recall=0.798]\n",
      "Epoch: 9/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.36it/s, F1=0.929, train_acc=0.988, train_loss=1.54, train_precision=0.93, train_recall=0.929] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.77it/s, F1=0.824, eval_acc=0.967, eval_loss=11.2, eval_precision=0.822, eval_recall=0.826]\n",
      "Epoch: 10/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.936, train_acc=0.989, train_loss=1.41, train_precision=0.936, train_recall=0.936]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.814, eval_acc=0.963, eval_loss=12.5, eval_precision=0.838, eval_recall=0.792]\n",
      "Epoch: 11/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.939, train_acc=0.99, train_loss=1.32, train_precision=0.939, train_recall=0.94]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.65it/s, F1=0.818, eval_acc=0.962, eval_loss=12.7, eval_precision=0.831, eval_recall=0.806]\n",
      "Epoch: 12/30 Train: 100%|██████████| 352/352 [02:33<00:00,  2.29it/s, F1=0.945, train_acc=0.991, train_loss=1.21, train_precision=0.945, train_recall=0.946]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.71it/s, F1=0.821, eval_acc=0.968, eval_loss=11, eval_precision=0.832, eval_recall=0.811]  \n",
      "Epoch: 13/30 Train: 100%|██████████| 352/352 [02:34<00:00,  2.28it/s, F1=0.946, train_acc=0.991, train_loss=1.16, train_precision=0.946, train_recall=0.945]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s, F1=0.8, eval_acc=0.962, eval_loss=13.2, eval_precision=0.811, eval_recall=0.79]   \n",
      "Epoch: 14/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.951, train_acc=0.991, train_loss=1.11, train_precision=0.952, train_recall=0.951]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.76it/s, F1=0.814, eval_acc=0.962, eval_loss=12.4, eval_precision=0.829, eval_recall=0.799]\n",
      "Epoch: 15/30 Train: 100%|██████████| 352/352 [02:33<00:00,  2.29it/s, F1=0.956, train_acc=0.992, train_loss=0.994, train_precision=0.956, train_recall=0.957]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.822, eval_acc=0.966, eval_loss=13.3, eval_precision=0.835, eval_recall=0.809]\n",
      "Epoch: 16/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.30it/s, F1=0.957, train_acc=0.993, train_loss=0.891, train_precision=0.958, train_recall=0.957]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.70it/s, F1=0.814, eval_acc=0.966, eval_loss=13.2, eval_precision=0.812, eval_recall=0.816]\n",
      "Epoch: 17/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.30it/s, F1=0.963, train_acc=0.994, train_loss=0.831, train_precision=0.963, train_recall=0.963]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.804, eval_acc=0.963, eval_loss=15.3, eval_precision=0.811, eval_recall=0.797]\n",
      "Epoch: 18/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.30it/s, F1=0.96, train_acc=0.993, train_loss=0.957, train_precision=0.961, train_recall=0.96]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.807, eval_acc=0.962, eval_loss=15, eval_precision=0.819, eval_recall=0.796]  \n",
      "Epoch: 19/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.33it/s, F1=0.96, train_acc=0.994, train_loss=0.829, train_precision=0.962, train_recall=0.96]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.815, eval_acc=0.965, eval_loss=16.7, eval_precision=0.829, eval_recall=0.803]\n",
      "Epoch: 20/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.33it/s, F1=0.963, train_acc=0.994, train_loss=0.828, train_precision=0.963, train_recall=0.963]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.812, eval_acc=0.964, eval_loss=15.9, eval_precision=0.829, eval_recall=0.797]\n",
      "Epoch: 21/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.965, train_acc=0.995, train_loss=0.668, train_precision=0.965, train_recall=0.966]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.66it/s, F1=0.816, eval_acc=0.965, eval_loss=20.6, eval_precision=0.828, eval_recall=0.806]\n",
      "Epoch: 22/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.967, train_acc=0.995, train_loss=0.725, train_precision=0.968, train_recall=0.966]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.823, eval_acc=0.966, eval_loss=13.7, eval_precision=0.828, eval_recall=0.818]\n",
      "Epoch: 23/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.38it/s, F1=0.97, train_acc=0.995, train_loss=0.656, train_precision=0.97, train_recall=0.97]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.807, eval_acc=0.963, eval_loss=14.1, eval_precision=0.825, eval_recall=0.791]\n",
      "Epoch: 24/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.34it/s, F1=0.969, train_acc=0.994, train_loss=0.698, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s, F1=0.8, eval_acc=0.965, eval_loss=14.3, eval_precision=0.81, eval_recall=0.791]   \n",
      "Epoch: 25/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.968, train_acc=0.994, train_loss=0.69, train_precision=0.969, train_recall=0.968] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.808, eval_acc=0.965, eval_loss=14.2, eval_precision=0.822, eval_recall=0.796]\n",
      "Epoch: 26/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.36it/s, F1=0.969, train_acc=0.996, train_loss=0.6, train_precision=0.97, train_recall=0.968]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.83it/s, F1=0.819, eval_acc=0.966, eval_loss=14.6, eval_precision=0.823, eval_recall=0.814]\n",
      "Epoch: 27/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.972, train_acc=0.996, train_loss=0.598, train_precision=0.972, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.777, eval_acc=0.956, eval_loss=16.9, eval_precision=0.791, eval_recall=0.764]\n",
      "Epoch: 28/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.33it/s, F1=0.969, train_acc=0.995, train_loss=0.601, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.70it/s, F1=0.801, eval_acc=0.962, eval_loss=16.9, eval_precision=0.801, eval_recall=0.801]\n",
      "Epoch: 29/30 Train: 100%|██████████| 352/352 [02:59<00:00,  1.96it/s, F1=0.972, train_acc=0.995, train_loss=0.569, train_precision=0.972, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.813, eval_acc=0.964, eval_loss=15.7, eval_precision=0.823, eval_recall=0.804]\n",
      "Epoch: 30/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.972, train_acc=0.995, train_loss=0.625, train_precision=0.973, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.818, eval_acc=0.965, eval_loss=16.1, eval_precision=0.831, eval_recall=0.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json\",\n",
      "    \"eval_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"test_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"tag_file\": \"data/ccks/ccks_tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"ccks_daga_resnet_drop_bi_2_4\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json etag: 100%|██████████| 5.49M/5.49M [00:00<00:00, 105MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 47.0MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 300MB/s]\n",
      "calculate data/ccks/ccks_tags_list.txt etag: 100%|██████████| 85.0/85.0 [00:00<00:00, 150kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/lexicon_tree\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size data/ccks/ccks_tags_list.txt: 13L [00:00, 30529.65L/s]\n",
      "build line mapper: 13L [00:00, 20475.39L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 13/13 [00:00<00:00, 3080.04it/s]\n",
      "load vocab from list: 100%|██████████| 13/13 [00:00<00:00, 116508.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/vocab_embedding\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_word_vocab\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json: 100%|██████████| 2812/2812 [00:05<00:00, 538.94it/s] \n",
      "load dataset from ./data/ccks/subtask1_test.json: 100%|██████████| 379/379 [00:01<00:00, 244.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'inter_word_embeddings.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.493, train_acc=0.891, train_loss=29.8, train_precision=0.464, train_recall=0.536]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.83it/s, F1=0.757, eval_acc=0.955, eval_loss=15.3, eval_precision=0.747, eval_recall=0.767]\n",
      "Epoch: 2/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.37it/s, F1=0.787, train_acc=0.96, train_loss=6.54, train_precision=0.787, train_recall=0.791] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.787, eval_acc=0.962, eval_loss=11.4, eval_precision=0.778, eval_recall=0.796]\n",
      "Epoch: 3/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.84, train_acc=0.968, train_loss=4.22, train_precision=0.845, train_recall=0.837] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.806, eval_acc=0.966, eval_loss=10.5, eval_precision=0.793, eval_recall=0.82] \n",
      "Epoch: 4/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.866, train_acc=0.975, train_loss=3.09, train_precision=0.87, train_recall=0.864] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.92it/s, F1=0.822, eval_acc=0.969, eval_loss=9.72, eval_precision=0.817, eval_recall=0.827]\n",
      "Epoch: 5/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.889, train_acc=0.979, train_loss=2.47, train_precision=0.89, train_recall=0.888] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.76it/s, F1=0.825, eval_acc=0.967, eval_loss=11.1, eval_precision=0.818, eval_recall=0.832]\n",
      "Epoch: 6/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.36it/s, F1=0.895, train_acc=0.981, train_loss=2.21, train_precision=0.897, train_recall=0.895]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.82, eval_acc=0.966, eval_loss=10.9, eval_precision=0.82, eval_recall=0.82]   \n",
      "Epoch: 7/30 Train: 100%|██████████| 352/352 [02:26<00:00,  2.40it/s, F1=0.901, train_acc=0.981, train_loss=2.24, train_precision=0.903, train_recall=0.9]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.66it/s, F1=0.823, eval_acc=0.968, eval_loss=10, eval_precision=0.815, eval_recall=0.831]  \n",
      "Epoch: 8/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.33it/s, F1=0.91, train_acc=0.983, train_loss=1.89, train_precision=0.912, train_recall=0.91]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s, F1=0.823, eval_acc=0.967, eval_loss=10.8, eval_precision=0.832, eval_recall=0.813]\n",
      "Epoch: 9/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.37it/s, F1=0.921, train_acc=0.985, train_loss=1.58, train_precision=0.922, train_recall=0.922]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.89it/s, F1=0.837, eval_acc=0.969, eval_loss=11.1, eval_precision=0.832, eval_recall=0.842]\n",
      "Epoch: 10/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.39it/s, F1=0.937, train_acc=0.987, train_loss=1.35, train_precision=0.938, train_recall=0.937]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.827, eval_acc=0.967, eval_loss=11.7, eval_precision=0.82, eval_recall=0.833] \n",
      "Epoch: 11/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.34it/s, F1=0.941, train_acc=0.989, train_loss=1.31, train_precision=0.941, train_recall=0.941]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.837, eval_acc=0.97, eval_loss=11.3, eval_precision=0.835, eval_recall=0.84]  \n",
      "Epoch: 12/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.36it/s, F1=0.945, train_acc=0.991, train_loss=1.13, train_precision=0.946, train_recall=0.945]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.836, eval_acc=0.969, eval_loss=11.9, eval_precision=0.832, eval_recall=0.84] \n",
      "Epoch: 13/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.42it/s, F1=0.95, train_acc=0.991, train_loss=1.11, train_precision=0.951, train_recall=0.949] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.828, eval_acc=0.967, eval_loss=13.2, eval_precision=0.832, eval_recall=0.825]\n",
      "Epoch: 14/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.953, train_acc=0.992, train_loss=1.01, train_precision=0.953, train_recall=0.953] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.83, eval_acc=0.968, eval_loss=14.6, eval_precision=0.828, eval_recall=0.832] \n",
      "Epoch: 15/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.36it/s, F1=0.956, train_acc=0.992, train_loss=0.984, train_precision=0.957, train_recall=0.956]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.813, eval_acc=0.966, eval_loss=13.2, eval_precision=0.806, eval_recall=0.82] \n",
      "Epoch: 16/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.958, train_acc=0.993, train_loss=0.846, train_precision=0.958, train_recall=0.958]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.826, eval_acc=0.967, eval_loss=14.5, eval_precision=0.835, eval_recall=0.817]\n",
      "Epoch: 17/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.33it/s, F1=0.954, train_acc=0.992, train_loss=0.961, train_precision=0.955, train_recall=0.954]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.817, eval_acc=0.965, eval_loss=14.3, eval_precision=0.81, eval_recall=0.825] \n",
      "Epoch: 18/30 Train: 100%|██████████| 352/352 [02:27<00:00,  2.38it/s, F1=0.962, train_acc=0.993, train_loss=0.897, train_precision=0.963, train_recall=0.962]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.807, eval_acc=0.964, eval_loss=13, eval_precision=0.822, eval_recall=0.793]  \n",
      "Epoch: 19/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.41it/s, F1=0.96, train_acc=0.993, train_loss=0.878, train_precision=0.96, train_recall=0.961]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.82, eval_acc=0.966, eval_loss=14, eval_precision=0.833, eval_recall=0.808]   \n",
      "Epoch: 20/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.33it/s, F1=0.964, train_acc=0.994, train_loss=0.902, train_precision=0.965, train_recall=0.964]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.809, eval_acc=0.966, eval_loss=14.4, eval_precision=0.818, eval_recall=0.801]\n",
      "Epoch: 21/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.37it/s, F1=0.966, train_acc=0.994, train_loss=0.731, train_precision=0.965, train_recall=0.966]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.91it/s, F1=0.813, eval_acc=0.966, eval_loss=15.9, eval_precision=0.808, eval_recall=0.818]\n",
      "Epoch: 22/30 Train: 100%|██████████| 352/352 [02:25<00:00,  2.42it/s, F1=0.967, train_acc=0.995, train_loss=0.699, train_precision=0.967, train_recall=0.968]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s, F1=0.821, eval_acc=0.966, eval_loss=15.7, eval_precision=0.822, eval_recall=0.82] \n",
      "Epoch: 23/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.34it/s, F1=0.964, train_acc=0.994, train_loss=0.821, train_precision=0.964, train_recall=0.965]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.814, eval_acc=0.966, eval_loss=15.3, eval_precision=0.813, eval_recall=0.816]\n",
      "Epoch: 24/30 Train: 100%|██████████| 352/352 [02:34<00:00,  2.27it/s, F1=0.967, train_acc=0.994, train_loss=0.796, train_precision=0.967, train_recall=0.967]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.805, eval_acc=0.963, eval_loss=13.4, eval_precision=0.807, eval_recall=0.804]\n",
      "Epoch: 25/30 Train: 100%|██████████| 352/352 [02:46<00:00,  2.12it/s, F1=0.97, train_acc=0.995, train_loss=0.623, train_precision=0.971, train_recall=0.969] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.77it/s, F1=0.807, eval_acc=0.963, eval_loss=16.3, eval_precision=0.815, eval_recall=0.799]\n",
      "Epoch: 26/30 Train: 100%|██████████| 352/352 [02:48<00:00,  2.09it/s, F1=0.968, train_acc=0.995, train_loss=0.726, train_precision=0.967, train_recall=0.968]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.60it/s, F1=0.814, eval_acc=0.964, eval_loss=17.2, eval_precision=0.823, eval_recall=0.806]\n",
      "Epoch: 27/30 Train: 100%|██████████| 352/352 [02:51<00:00,  2.06it/s, F1=0.969, train_acc=0.995, train_loss=0.673, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.69it/s, F1=0.823, eval_acc=0.966, eval_loss=14.8, eval_precision=0.831, eval_recall=0.816]\n",
      "Epoch: 28/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.33it/s, F1=0.971, train_acc=0.995, train_loss=0.657, train_precision=0.971, train_recall=0.971]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.77it/s, F1=0.83, eval_acc=0.967, eval_loss=15.5, eval_precision=0.844, eval_recall=0.817] \n",
      "Epoch: 29/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.973, train_acc=0.996, train_loss=0.553, train_precision=0.973, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.75it/s, F1=0.825, eval_acc=0.967, eval_loss=16.4, eval_precision=0.823, eval_recall=0.826]\n",
      "Epoch: 30/30 Train: 100%|██████████| 352/352 [02:34<00:00,  2.28it/s, F1=0.969, train_acc=0.995, train_loss=0.634, train_precision=0.97, train_recall=0.969] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.822, eval_acc=0.967, eval_loss=15.4, eval_precision=0.827, eval_recall=0.817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json\",\n",
      "    \"eval_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"test_file\": \"./data/ccks/subtask1_test.json\",\n",
      "    \"tag_file\": \"data/ccks/ccks_tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"ccks_daga_resnet_drop_bi_2_5\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json etag: 100%|██████████| 5.49M/5.49M [00:00<00:00, 323MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 338MB/s]\n",
      "calculate ./data/ccks/subtask1_test.json etag: 100%|██████████| 1.57M/1.57M [00:00<00:00, 325MB/s]\n",
      "calculate data/ccks/ccks_tags_list.txt etag: 100%|██████████| 85.0/85.0 [00:00<00:00, 160kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/lexicon_tree\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size data/ccks/ccks_tags_list.txt: 13L [00:00, 30701.55L/s]\n",
      "build line mapper: 13L [00:00, 66576.25L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 13/13 [00:00<00:00, 3460.65it/s]\n",
      "load vocab from list: 100%|██████████| 13/13 [00:00<00:00, 113124.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/vocab_embedding\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_matched_words\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_word_vocab\n",
      "load cached ./temp/012a130bce33701277b91bb72fc094b9-2_cda3479a91a9c4fc8b0c534e972e99e8_cda3479a91a9c4fc8b0c534e972e99e8_9c02c6b5f9f31c0f8b66d34ba80dcf4e/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json: 100%|██████████| 2812/2812 [00:05<00:00, 489.66it/s] \n",
      "load dataset from ./data/ccks/subtask1_test.json: 100%|██████████| 379/379 [00:01<00:00, 193.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'inter_word_embeddings.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 352/352 [02:28<00:00,  2.38it/s, F1=0.583, train_acc=0.899, train_loss=19.2, train_precision=0.593, train_recall=0.587]    \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.85it/s, F1=0.776, eval_acc=0.958, eval_loss=14.7, eval_precision=0.787, eval_recall=0.766]\n",
      "Epoch: 2/30 Train: 100%|██████████| 352/352 [02:34<00:00,  2.28it/s, F1=0.822, train_acc=0.964, train_loss=5, train_precision=0.828, train_recall=0.819]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.65it/s, F1=0.825, eval_acc=0.968, eval_loss=9.91, eval_precision=0.826, eval_recall=0.824]\n",
      "Epoch: 3/30 Train: 100%|██████████| 352/352 [02:34<00:00,  2.28it/s, F1=0.859, train_acc=0.971, train_loss=3.32, train_precision=0.863, train_recall=0.858]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.76it/s, F1=0.806, eval_acc=0.96, eval_loss=11.4, eval_precision=0.813, eval_recall=0.799] \n",
      "Epoch: 4/30 Train: 100%|██████████| 352/352 [02:33<00:00,  2.30it/s, F1=0.885, train_acc=0.978, train_loss=2.63, train_precision=0.89, train_recall=0.882] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.70it/s, F1=0.826, eval_acc=0.968, eval_loss=9.31, eval_precision=0.809, eval_recall=0.844]\n",
      "Epoch: 5/30 Train: 100%|██████████| 352/352 [02:36<00:00,  2.25it/s, F1=0.898, train_acc=0.98, train_loss=2.32, train_precision=0.9, train_recall=0.897]   \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.67it/s, F1=0.83, eval_acc=0.968, eval_loss=10.1, eval_precision=0.83, eval_recall=0.829]  \n",
      "Epoch: 6/30 Train: 100%|██████████| 352/352 [02:35<00:00,  2.27it/s, F1=0.916, train_acc=0.984, train_loss=1.83, train_precision=0.917, train_recall=0.915]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.76it/s, F1=0.819, eval_acc=0.965, eval_loss=11.9, eval_precision=0.816, eval_recall=0.823]\n",
      "Epoch: 7/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.36it/s, F1=0.923, train_acc=0.985, train_loss=1.73, train_precision=0.924, train_recall=0.923]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.831, eval_acc=0.968, eval_loss=11.4, eval_precision=0.832, eval_recall=0.83] \n",
      "Epoch: 8/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.929, train_acc=0.987, train_loss=1.66, train_precision=0.93, train_recall=0.929] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s, F1=0.834, eval_acc=0.97, eval_loss=10.6, eval_precision=0.83, eval_recall=0.837]  \n",
      "Epoch: 9/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.33it/s, F1=0.927, train_acc=0.986, train_loss=1.62, train_precision=0.929, train_recall=0.927]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.832, eval_acc=0.967, eval_loss=11, eval_precision=0.825, eval_recall=0.839]  \n",
      "Epoch: 10/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.941, train_acc=0.989, train_loss=1.3, train_precision=0.942, train_recall=0.941] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.834, eval_acc=0.969, eval_loss=12.6, eval_precision=0.83, eval_recall=0.839] \n",
      "Epoch: 11/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.36it/s, F1=0.946, train_acc=0.989, train_loss=1.3, train_precision=0.948, train_recall=0.946] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.821, eval_acc=0.968, eval_loss=13.7, eval_precision=0.812, eval_recall=0.83] \n",
      "Epoch: 12/30 Train: 100%|██████████| 352/352 [02:33<00:00,  2.30it/s, F1=0.949, train_acc=0.991, train_loss=1.19, train_precision=0.951, train_recall=0.948]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.838, eval_acc=0.969, eval_loss=11.5, eval_precision=0.838, eval_recall=0.838]\n",
      "Epoch: 13/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.956, train_acc=0.992, train_loss=1.04, train_precision=0.957, train_recall=0.956]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s, F1=0.809, eval_acc=0.963, eval_loss=12.3, eval_precision=0.798, eval_recall=0.82] \n",
      "Epoch: 14/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.34it/s, F1=0.954, train_acc=0.992, train_loss=1.1, train_precision=0.956, train_recall=0.954] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.824, eval_acc=0.968, eval_loss=13.7, eval_precision=0.818, eval_recall=0.83] \n",
      "Epoch: 15/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.956, train_acc=0.992, train_loss=1.03, train_precision=0.957, train_recall=0.954]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.82, eval_acc=0.966, eval_loss=12, eval_precision=0.821, eval_recall=0.818]   \n",
      "Epoch: 16/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.958, train_acc=0.992, train_loss=0.986, train_precision=0.959, train_recall=0.957]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.828, eval_acc=0.969, eval_loss=12.6, eval_precision=0.833, eval_recall=0.823]\n",
      "Epoch: 17/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.33it/s, F1=0.958, train_acc=0.991, train_loss=0.988, train_precision=0.96, train_recall=0.956] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.821, eval_acc=0.965, eval_loss=13.6, eval_precision=0.808, eval_recall=0.836]\n",
      "Epoch: 18/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.30it/s, F1=0.964, train_acc=0.994, train_loss=0.748, train_precision=0.964, train_recall=0.964]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.90it/s, F1=0.819, eval_acc=0.965, eval_loss=14.8, eval_precision=0.808, eval_recall=0.831]\n",
      "Epoch: 19/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.963, train_acc=0.993, train_loss=0.725, train_precision=0.964, train_recall=0.962]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.829, eval_acc=0.967, eval_loss=14.1, eval_precision=0.822, eval_recall=0.836]\n",
      "Epoch: 20/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.34it/s, F1=0.969, train_acc=0.994, train_loss=0.699, train_precision=0.97, train_recall=0.969] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s, F1=0.823, eval_acc=0.966, eval_loss=14.7, eval_precision=0.813, eval_recall=0.834]\n",
      "Epoch: 21/30 Train: 100%|██████████| 352/352 [02:30<00:00,  2.34it/s, F1=0.967, train_acc=0.994, train_loss=0.72, train_precision=0.969, train_recall=0.967] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.75it/s, F1=0.82, eval_acc=0.967, eval_loss=16.5, eval_precision=0.812, eval_recall=0.828] \n",
      "Epoch: 22/30 Train: 100%|██████████| 352/352 [02:32<00:00,  2.31it/s, F1=0.965, train_acc=0.994, train_loss=0.756, train_precision=0.966, train_recall=0.965]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.818, eval_acc=0.964, eval_loss=15, eval_precision=0.806, eval_recall=0.83]   \n",
      "Epoch: 23/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.33it/s, F1=0.965, train_acc=0.994, train_loss=0.7, train_precision=0.965, train_recall=0.965]  \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s, F1=0.816, eval_acc=0.964, eval_loss=15.2, eval_precision=0.809, eval_recall=0.822]\n",
      "Epoch: 24/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.973, train_acc=0.996, train_loss=0.571, train_precision=0.974, train_recall=0.973]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.814, eval_acc=0.964, eval_loss=17, eval_precision=0.803, eval_recall=0.825]  \n",
      "Epoch: 25/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.975, train_acc=0.996, train_loss=0.572, train_precision=0.976, train_recall=0.975]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.81it/s, F1=0.821, eval_acc=0.965, eval_loss=17, eval_precision=0.81, eval_recall=0.833]   \n",
      "Epoch: 26/30 Train: 100%|██████████| 352/352 [02:33<00:00,  2.30it/s, F1=0.969, train_acc=0.995, train_loss=0.676, train_precision=0.97, train_recall=0.968] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.88it/s, F1=0.815, eval_acc=0.965, eval_loss=18, eval_precision=0.798, eval_recall=0.833]  \n",
      "Epoch: 27/30 Train: 100%|██████████| 352/352 [02:29<00:00,  2.35it/s, F1=0.972, train_acc=0.995, train_loss=0.624, train_precision=0.974, train_recall=0.97] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s, F1=0.815, eval_acc=0.965, eval_loss=16.5, eval_precision=0.797, eval_recall=0.833]\n",
      "Epoch: 28/30 Train: 100%|██████████| 352/352 [02:31<00:00,  2.32it/s, F1=0.972, train_acc=0.996, train_loss=0.597, train_precision=0.973, train_recall=0.971]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.86it/s, F1=0.82, eval_acc=0.965, eval_loss=16.8, eval_precision=0.805, eval_recall=0.836] \n",
      "Epoch: 29/30 Train: 100%|██████████| 352/352 [02:38<00:00,  2.22it/s, F1=0.971, train_acc=0.995, train_loss=0.652, train_precision=0.971, train_recall=0.971]\n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.87it/s, F1=0.798, eval_acc=0.959, eval_loss=17.6, eval_precision=0.783, eval_recall=0.814]\n",
      "Epoch: 30/30 Train: 100%|██████████| 352/352 [02:34<00:00,  2.27it/s, F1=0.973, train_acc=0.995, train_loss=0.64, train_precision=0.974, train_recall=0.973] \n",
      "Eval Result: 100%|██████████| 6/6 [00:03<00:00,  1.77it/s, F1=0.816, eval_acc=0.966, eval_loss=17.7, eval_precision=0.808, eval_recall=0.826]\n"
     ]
    }
   ],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pre_4/Bert_19215/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/thuocl_key_pre_1/Bert_13890/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/tmul_pre_1/Bert_3535/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 20000,\n",
    "    'train_file': './data/ccks/daga/bilstm/renet/resnet_drop_bi_2.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'ccks_daga_resnet_drop_bi_2_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"ccks_daga_resnet_drop_bi_2_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_daga_resnet_drop_bi_2_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_daga_resnet_drop_bi_2_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_daga_resnet_drop_bi_2_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert预训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_2/Bert_8960/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/before/subtask1_train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'ccks_pre_3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        # \"KEYWORD\": \"关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "        # \"CHECK\":\"检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pre_4/Bert_19215/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'train_file': './data/ccks/conll/train_1000.txt',\n",
    "    'eval_file': './data/ccks/conll/subtask1_test_set_with_answer.txt',\n",
    "    'test_file': './data/ccks/conll/subtask1_test_set_with_answer.txt',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'output_eval':True,\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'Bert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'ccks_bert_pro_4_1',\n",
    "\n",
    "}\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_pro_4_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"ccks_bert_pro_4_3\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"ccks_bert_pro_4_4\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"ccks_bert_pro_4_5\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pre_3/Bert_4375/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 80,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/before/subtask1_train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    # 'loader_name': 'le_loader_zl',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'classify':'crf',\n",
    "    'task_name': 'ccks_LEBert_crf_pro_80_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_crf_pro_80_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_crf_pro_80_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_crf_pro_80_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_crf_pro_80_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pre_4/Bert_19215/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/before/subtask1_train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    # 'loader_name': 'le_loader_zl',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'classify':'crf',\n",
    "    'task_name': 'ccks_LEBert_pro_4_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_4_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_4_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_4_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_4_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pre_4/Bert_19215/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/thuocl_key_pre_1/Bert_13890/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/tmul_pre_1/Bert_3535/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 20000,\n",
    "    'train_file': './data/ccks/before/subtask1_train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'ccks_v1_mul_pre_1_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_mul_pre_1_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_mul_pre_1_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_mul_pre_1_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_mul_pre_1_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1_Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 100,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 20000,\n",
    "    'train_file': './data/ccks/before/subtask1_train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'ccks_v1_seq_100_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_seq_100_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_seq_100_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_seq_100_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_seq_100_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 150,\n",
    "    'max_seq_length': 20,\n",
    "    'train_file': './data/ccks/conll/train_1000.txt',\n",
    "    'eval_file': './data/ccks/conll/subtask1_test_set_with_answer.txt',\n",
    "    'test_file': './data/ccks/conll/subtask1_test_set_with_answer.txt',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'output_eval':True,\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'Bert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'ccks_bert_seq_20_1',\n",
    "\n",
    "}\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "args['max_seq_length'] = 200\n",
    "args[\"task_name\"] = \"ccks_bert_seq_200_1\"\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_200_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_200_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_200_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_200_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args['max_seq_length'] = 220\n",
    "args[\"task_name\"] = \"ccks_bert_seq_220_1\"\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_220_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_220_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_220_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_220_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args['max_seq_length'] = 250\n",
    "args[\"task_name\"] = \"ccks_bert_seq_250_1\"\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_250_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_250_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_250_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_seq_250_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pre_4/Bert_19215/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'train_file': './data/ccks/0.5k/conll/train.txt',\n",
    "    'eval_file': './data/ccks/conll/subtask1_test_set_with_answer.txt',\n",
    "    'test_file': './data/ccks/conll/subtask1_test_set_with_answer.txt',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'output_eval':True,\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'Bert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'ccks_bert_lstm_crf_0.5k_1',\n",
    "\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_lstm_crf_0.5k_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_lstm_crf_0.5k_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_lstm_crf_0.5k_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_bert_lstm_crf_0.5k_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pre_4/Bert_19215/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'inter_max_scan_num': 3000,\n",
    "    'train_file': './data/ccks/0.5k/train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    # 'loader_name': 'le_loader_zl',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'classify':'crf',\n",
    "    'task_name': 'ccks_LEBert_pro_0.5k_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_0.5k_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_0.5k_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_0.5k_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_LEBert_pro_0.5k_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "from CC.trainer import NERTrainer\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pre_4/Bert_19215/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 20000,\n",
    "    'train_file': './data/ccks/0.5k/train.json',\n",
    "    'eval_file': './data/ccks/subtask1_test.json',\n",
    "    'test_file': './data/ccks/subtask1_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'ccks_v1_pro_0.5k_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_pro_0.5k_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_pro_0.5k_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_pro_0.5k_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"ccks_v1_pro_0.5k_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9392d1f0914889243d058bb73f0d89e61311fd6d751bbc8fa50e38d7d4ff811"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NER': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
