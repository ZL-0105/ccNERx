{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'train_file': './data/CDD/1k/conll/train.txt',\n",
    "    'eval_file': './data/CDD/conll/dev.txt',\n",
    "    'test_file': './data/CDD/conll/test.txt',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/CDD/cdd_tags_list.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'output_eval':True,\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'Bert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'cdd_bert_lstm_crf_1k_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"cdd_bert_lstm_crf_1k_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"cdd_bert_lstm_crf_1k_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"cdd_bert_lstm_crf_1k_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-5):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"cdd_bert_lstm_crf_1k_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 20000,\n",
    "    'train_file': './data/CDD/1k/json/train.json',\n",
    "    'eval_file': './data/CDD/dev.json',\n",
    "    'test_file': './data/CDD/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/CDD/cdd_tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'cdd_v1_lstm_crf_1k_1'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"cdd_v1_lstm_crf_1k_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"cdd_v1_lstm_crf_1k_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"cdd_v1_lstm_crf_1k_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-5):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"cdd_v1_lstm_crf_1k_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/CDD/1k/json/train.json\",\n",
      "    \"eval_file\": \"./data/CDD/dev.json\",\n",
      "    \"test_file\": \"./data/CDD/test.json\",\n",
      "    \"tag_file\": \"data/CDD/cdd_tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"cdd_LEBert_pro_1k_1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/CDD/1k/json/train.json etag: 100%|██████████| 1.16M/1.16M [00:00<00:00, 27.2MB/s]\n",
      "calculate ./data/CDD/dev.json etag: 100%|██████████| 1.00M/1.00M [00:00<00:00, 312MB/s]\n",
      "calculate ./data/CDD/test.json etag: 100%|██████████| 1.09M/1.09M [00:00<00:00, 319MB/s]\n",
      "calculate data/CDD/cdd_tags_list.txt etag: 100%|██████████| 18.0/18.0 [00:00<00:00, 2.18kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/matched_words\n",
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size data/CDD/cdd_tags_list.txt: 4L [00:00, 20971.52L/s]\n",
      "build line mapper: 4L [00:00, 6811.70L/s]/4 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 4/4 [00:00<00:00, 1104.64it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 31694.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "load dataset from ./data/CDD/1k/json/train.json: 100%|██████████| 1000/1000 [00:01<00:00, 878.43it/s]\n",
      "load dataset from ./data/CDD/dev.json: 100%|██████████| 929/929 [00:01<00:00, 902.17it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/cdd_pre_3/Bert_10470/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./save_pretrained/cdd_pre_3/Bert_10470/pytorch_model.bin and are newly initialized: ['word_embeddings.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.attn_W', 'bert.pooler.dense.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 125/125 [01:00<00:00,  2.05it/s, F1=0.173, train_acc=0.923, train_loss=12.1, train_precision=0.238, train_recall=0.155]     \n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.06s/it, F1=0.417, eval_acc=0.921, eval_loss=8.88, eval_precision=0.406, eval_recall=0.431]\n",
      "Epoch: 2/30 Train: 100%|██████████| 125/125 [00:57<00:00,  2.16it/s, F1=0.488, train_acc=0.941, train_loss=6.58, train_precision=0.568, train_recall=0.472]\n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.00s/it, F1=0.531, eval_acc=0.928, eval_loss=8.39, eval_precision=0.516, eval_recall=0.551]\n",
      "Epoch: 3/30 Train: 100%|██████████| 125/125 [00:58<00:00,  2.13it/s, F1=0.543, train_acc=0.948, train_loss=5.47, train_precision=0.613, train_recall=0.546]\n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it, F1=0.554, eval_acc=0.934, eval_loss=7.58, eval_precision=0.563, eval_recall=0.547]\n",
      "Epoch: 4/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.38it/s, F1=0.656, train_acc=0.961, train_loss=3.47, train_precision=0.703, train_recall=0.661]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.588, eval_acc=0.938, eval_loss=7.59, eval_precision=0.642, eval_recall=0.546]\n",
      "Epoch: 5/30 Train: 100%|██████████| 125/125 [00:54<00:00,  2.28it/s, F1=0.683, train_acc=0.964, train_loss=3.38, train_precision=0.721, train_recall=0.701]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.565, eval_acc=0.935, eval_loss=9.29, eval_precision=0.666, eval_recall=0.493]\n",
      "Epoch: 6/30 Train: 100%|██████████| 125/125 [00:56<00:00,  2.21it/s, F1=0.783, train_acc=0.975, train_loss=2.2, train_precision=0.814, train_recall=0.784] \n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.00s/it, F1=0.585, eval_acc=0.936, eval_loss=8.37, eval_precision=0.627, eval_recall=0.552]\n",
      "Epoch: 7/30 Train: 100%|██████████| 125/125 [00:53<00:00,  2.33it/s, F1=0.855, train_acc=0.986, train_loss=1.13, train_precision=0.869, train_recall=0.851]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.57, eval_acc=0.937, eval_loss=11.1, eval_precision=0.653, eval_recall=0.508] \n",
      "Epoch: 8/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.36it/s, F1=0.891, train_acc=0.99, train_loss=0.94, train_precision=0.895, train_recall=0.895]  \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.551, eval_acc=0.934, eval_loss=10.9, eval_precision=0.662, eval_recall=0.473]\n",
      "Epoch: 9/30 Train: 100%|██████████| 125/125 [00:50<00:00,  2.46it/s, F1=0.905, train_acc=0.991, train_loss=0.844, train_precision=0.907, train_recall=0.912]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.552, eval_acc=0.934, eval_loss=13, eval_precision=0.629, eval_recall=0.494]  \n",
      "Epoch: 10/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.42it/s, F1=0.913, train_acc=0.991, train_loss=0.742, train_precision=0.921, train_recall=0.911]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.571, eval_acc=0.935, eval_loss=12.3, eval_precision=0.632, eval_recall=0.524]\n",
      "Epoch: 11/30 Train: 100%|██████████| 125/125 [00:56<00:00,  2.19it/s, F1=0.928, train_acc=0.993, train_loss=0.672, train_precision=0.938, train_recall=0.925]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.557, eval_acc=0.935, eval_loss=13.2, eval_precision=0.622, eval_recall=0.507]\n",
      "Epoch: 12/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.39it/s, F1=0.923, train_acc=0.992, train_loss=0.72, train_precision=0.927, train_recall=0.925] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.06it/s, F1=0.552, eval_acc=0.935, eval_loss=13.9, eval_precision=0.644, eval_recall=0.485]\n",
      "Epoch: 13/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.40it/s, F1=0.926, train_acc=0.993, train_loss=0.609, train_precision=0.931, train_recall=0.927]\n",
      "Eval Result: 100%|██████████| 15/15 [00:13<00:00,  1.14it/s, F1=0.561, eval_acc=0.935, eval_loss=15.6, eval_precision=0.665, eval_recall=0.487]\n",
      "Epoch: 14/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.41it/s, F1=0.923, train_acc=0.992, train_loss=0.831, train_precision=0.928, train_recall=0.924]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.574, eval_acc=0.933, eval_loss=15, eval_precision=0.602, eval_recall=0.552]  \n",
      "Epoch: 15/30 Train: 100%|██████████| 125/125 [01:05<00:00,  1.90it/s, F1=0.922, train_acc=0.992, train_loss=0.97, train_precision=0.932, train_recall=0.925] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.574, eval_acc=0.933, eval_loss=14.6, eval_precision=0.606, eval_recall=0.548]\n",
      "Epoch: 16/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.42it/s, F1=0.919, train_acc=0.991, train_loss=0.986, train_precision=0.933, train_recall=0.92] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.546, eval_acc=0.933, eval_loss=19.5, eval_precision=0.645, eval_recall=0.476]\n",
      "Epoch: 17/30 Train: 100%|██████████| 125/125 [00:58<00:00,  2.15it/s, F1=0.914, train_acc=0.991, train_loss=1.09, train_precision=0.922, train_recall=0.917] \n",
      "Eval Result: 100%|██████████| 15/15 [00:41<00:00,  2.79s/it, F1=0.566, eval_acc=0.932, eval_loss=17.1, eval_precision=0.607, eval_recall=0.533]\n",
      "Epoch: 18/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.41it/s, F1=0.92, train_acc=0.991, train_loss=0.984, train_precision=0.931, train_recall=0.919] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.59, eval_acc=0.935, eval_loss=14.5, eval_precision=0.609, eval_recall=0.575] \n",
      "Epoch: 19/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.38it/s, F1=0.942, train_acc=0.994, train_loss=0.613, train_precision=0.946, train_recall=0.946]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.536, eval_acc=0.933, eval_loss=21.8, eval_precision=0.685, eval_recall=0.441]\n",
      "Epoch: 20/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.40it/s, F1=0.964, train_acc=0.996, train_loss=0.32, train_precision=0.969, train_recall=0.963] \n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it, F1=0.565, eval_acc=0.935, eval_loss=17, eval_precision=0.711, eval_recall=0.47]   \n",
      "Epoch: 21/30 Train: 100%|██████████| 125/125 [01:04<00:00,  1.92it/s, F1=0.965, train_acc=0.997, train_loss=0.289, train_precision=0.967, train_recall=0.965]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.01it/s, F1=0.566, eval_acc=0.936, eval_loss=20.8, eval_precision=0.697, eval_recall=0.478]\n",
      "Epoch: 22/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.37it/s, F1=0.975, train_acc=0.998, train_loss=0.176, train_precision=0.976, train_recall=0.975]\n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it, F1=0.553, eval_acc=0.935, eval_loss=19, eval_precision=0.64, eval_recall=0.488]   \n",
      "Epoch: 23/30 Train: 100%|██████████| 125/125 [00:57<00:00,  2.18it/s, F1=0.975, train_acc=0.998, train_loss=0.204, train_precision=0.977, train_recall=0.975]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.00it/s, F1=0.578, eval_acc=0.935, eval_loss=20, eval_precision=0.659, eval_recall=0.518]  \n",
      "Epoch: 24/30 Train: 100%|██████████| 125/125 [00:58<00:00,  2.15it/s, F1=0.98, train_acc=0.998, train_loss=0.204, train_precision=0.981, train_recall=0.981] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.592, eval_acc=0.939, eval_loss=19.5, eval_precision=0.672, eval_recall=0.532]\n",
      "Epoch: 25/30 Train: 100%|██████████| 125/125 [00:53<00:00,  2.35it/s, F1=0.974, train_acc=0.997, train_loss=0.216, train_precision=0.978, train_recall=0.973]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.584, eval_acc=0.938, eval_loss=17.5, eval_precision=0.671, eval_recall=0.52] \n",
      "Epoch: 26/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.42it/s, F1=0.976, train_acc=0.998, train_loss=0.209, train_precision=0.978, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.00it/s, F1=0.593, eval_acc=0.937, eval_loss=18.2, eval_precision=0.643, eval_recall=0.552]\n",
      "Epoch: 27/30 Train: 100%|██████████| 125/125 [00:50<00:00,  2.45it/s, F1=0.979, train_acc=0.998, train_loss=0.236, train_precision=0.979, train_recall=0.98] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.572, eval_acc=0.936, eval_loss=22.1, eval_precision=0.707, eval_recall=0.481]\n",
      "Epoch: 28/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.42it/s, F1=0.977, train_acc=0.997, train_loss=0.343, train_precision=0.979, train_recall=0.978]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.586, eval_acc=0.935, eval_loss=29.7, eval_precision=0.604, eval_recall=0.572]\n",
      "Epoch: 29/30 Train: 100%|██████████| 125/125 [00:54<00:00,  2.28it/s, F1=0.957, train_acc=0.996, train_loss=0.623, train_precision=0.958, train_recall=0.96] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.574, eval_acc=0.923, eval_loss=25.1, eval_precision=0.567, eval_recall=0.584]\n",
      "Epoch: 30/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.37it/s, F1=0.95, train_acc=0.994, train_loss=0.603, train_precision=0.957, train_recall=0.948] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.597, eval_acc=0.937, eval_loss=17.8, eval_precision=0.655, eval_recall=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/CDD/1k/json/train.json\",\n",
      "    \"eval_file\": \"./data/CDD/dev.json\",\n",
      "    \"test_file\": \"./data/CDD/test.json\",\n",
      "    \"tag_file\": \"data/CDD/cdd_tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"cdd_LEBert_pro_1k_2\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/CDD/1k/json/train.json etag: 100%|██████████| 1.16M/1.16M [00:00<00:00, 35.7MB/s]\n",
      "calculate ./data/CDD/dev.json etag: 100%|██████████| 1.00M/1.00M [00:00<00:00, 27.3MB/s]\n",
      "calculate ./data/CDD/test.json etag: 100%|██████████| 1.09M/1.09M [00:00<00:00, 40.0MB/s]\n",
      "calculate data/CDD/cdd_tags_list.txt etag: 100%|██████████| 18.0/18.0 [00:00<00:00, 1.77kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/lexicon_tree\n",
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/matched_words\n",
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size data/CDD/cdd_tags_list.txt: 4L [00:00, 11675.17L/s]\n",
      "build line mapper: 4L [00:00, 6450.29L/s]/4 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 4/4 [00:00<00:00, 979.35it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 43240.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/0abe8b84d3c54e6c1cff2e5b804cd936_349f7211c68e3ec4a8d14d462f359739_4bdeb826cacfa5eec1e7ad6c99dabacf_aae8c811d19923238e7599e515cbdb51/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "load dataset from ./data/CDD/1k/json/train.json: 100%|██████████| 1000/1000 [00:01<00:00, 890.91it/s]\n",
      "load dataset from ./data/CDD/dev.json: 100%|██████████| 929/929 [00:01<00:00, 927.50it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/cdd_pre_3/Bert_10470/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./save_pretrained/cdd_pre_3/Bert_10470/pytorch_model.bin and are newly initialized: ['word_embeddings.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.attn_W', 'bert.pooler.dense.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.43it/s, F1=0.0569, train_acc=0.839, train_loss=32.9, train_precision=0.0447, train_recall=0.0844]    \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.314, eval_acc=0.929, eval_loss=13.5, eval_precision=0.268, eval_recall=0.382]\n",
      "Epoch: 2/30 Train: 100%|██████████| 125/125 [00:55<00:00,  2.27it/s, F1=0.338, train_acc=0.939, train_loss=10.3, train_precision=0.316, train_recall=0.401]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.00it/s, F1=0.531, eval_acc=0.928, eval_loss=12, eval_precision=0.482, eval_recall=0.595]  \n",
      "Epoch: 3/30 Train: 100%|██████████| 125/125 [00:53<00:00,  2.36it/s, F1=0.576, train_acc=0.956, train_loss=6.47, train_precision=0.587, train_recall=0.597]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.55, eval_acc=0.937, eval_loss=11.1, eval_precision=0.599, eval_recall=0.511] \n",
      "Epoch: 4/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.37it/s, F1=0.68, train_acc=0.967, train_loss=4.28, train_precision=0.692, train_recall=0.694] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.487, eval_acc=0.93, eval_loss=11.5, eval_precision=0.659, eval_recall=0.389] \n",
      "Epoch: 5/30 Train: 100%|██████████| 125/125 [00:53<00:00,  2.36it/s, F1=0.72, train_acc=0.974, train_loss=3.11, train_precision=0.724, train_recall=0.737] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.507, eval_acc=0.931, eval_loss=14.4, eval_precision=0.69, eval_recall=0.403] \n",
      "Epoch: 6/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.39it/s, F1=0.756, train_acc=0.977, train_loss=2.74, train_precision=0.773, train_recall=0.761]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.57, eval_acc=0.937, eval_loss=11.2, eval_precision=0.612, eval_recall=0.537] \n",
      "Epoch: 7/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.36it/s, F1=0.776, train_acc=0.976, train_loss=2.83, train_precision=0.793, train_recall=0.791]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.01it/s, F1=0.537, eval_acc=0.934, eval_loss=12.9, eval_precision=0.651, eval_recall=0.461]\n",
      "Epoch: 8/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.43it/s, F1=0.744, train_acc=0.973, train_loss=3.45, train_precision=0.771, train_recall=0.766]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.05it/s, F1=0.558, eval_acc=0.935, eval_loss=11.4, eval_precision=0.64, eval_recall=0.498] \n",
      "Epoch: 9/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.41it/s, F1=0.801, train_acc=0.98, train_loss=2.75, train_precision=0.818, train_recall=0.815] \n",
      "Eval Result: 100%|██████████| 15/15 [00:16<00:00,  1.09s/it, F1=0.55, eval_acc=0.934, eval_loss=13.5, eval_precision=0.635, eval_recall=0.489] \n",
      "Epoch: 10/30 Train: 100%|██████████| 125/125 [00:52<00:00,  2.36it/s, F1=0.832, train_acc=0.983, train_loss=1.99, train_precision=0.852, train_recall=0.83] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.585, eval_acc=0.936, eval_loss=11.7, eval_precision=0.619, eval_recall=0.557]\n",
      "Epoch: 11/30 Train: 100%|██████████| 125/125 [00:55<00:00,  2.25it/s, F1=0.895, train_acc=0.991, train_loss=0.932, train_precision=0.899, train_recall=0.899]\n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.04s/it, F1=0.589, eval_acc=0.937, eval_loss=13, eval_precision=0.622, eval_recall=0.562]  \n",
      "Epoch: 12/30 Train: 100%|██████████| 125/125 [00:55<00:00,  2.24it/s, F1=0.934, train_acc=0.995, train_loss=0.64, train_precision=0.932, train_recall=0.941] \n",
      "Eval Result: 100%|██████████| 15/15 [00:09<00:00,  1.54it/s, F1=0.578, eval_acc=0.936, eval_loss=15.9, eval_precision=0.611, eval_recall=0.551]\n",
      "Epoch: 13/30 Train: 100%|██████████| 125/125 [00:48<00:00,  2.57it/s, F1=0.924, train_acc=0.994, train_loss=0.646, train_precision=0.925, train_recall=0.93] \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.578, eval_acc=0.934, eval_loss=15.3, eval_precision=0.607, eval_recall=0.555]\n",
      "Epoch: 14/30 Train: 100%|██████████| 125/125 [00:53<00:00,  2.35it/s, F1=0.933, train_acc=0.995, train_loss=0.581, train_precision=0.934, train_recall=0.936]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.07it/s, F1=0.575, eval_acc=0.936, eval_loss=14.3, eval_precision=0.63, eval_recall=0.531] \n",
      "Epoch: 15/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.41it/s, F1=0.95, train_acc=0.996, train_loss=0.412, train_precision=0.95, train_recall=0.953]  \n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.02it/s, F1=0.572, eval_acc=0.936, eval_loss=15.7, eval_precision=0.642, eval_recall=0.518]\n",
      "Epoch: 16/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.43it/s, F1=0.966, train_acc=0.997, train_loss=0.318, train_precision=0.966, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.01it/s, F1=0.564, eval_acc=0.936, eval_loss=19.3, eval_precision=0.646, eval_recall=0.503]\n",
      "Epoch: 17/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.42it/s, F1=0.964, train_acc=0.997, train_loss=0.352, train_precision=0.966, train_recall=0.965]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.04it/s, F1=0.589, eval_acc=0.937, eval_loss=17.8, eval_precision=0.626, eval_recall=0.56] \n",
      "Epoch: 18/30 Train: 100%|██████████| 125/125 [00:58<00:00,  2.14it/s, F1=0.973, train_acc=0.998, train_loss=0.27, train_precision=0.976, train_recall=0.974] \n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it, F1=0.577, eval_acc=0.936, eval_loss=17.7, eval_precision=0.63, eval_recall=0.534] \n",
      "Epoch: 19/30 Train: 100%|██████████| 125/125 [00:57<00:00,  2.16it/s, F1=0.968, train_acc=0.997, train_loss=0.291, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 15/15 [00:14<00:00,  1.03it/s, F1=0.573, eval_acc=0.933, eval_loss=20.4, eval_precision=0.595, eval_recall=0.556]\n",
      "Epoch: 20/30 Train: 100%|██████████| 125/125 [00:59<00:00,  2.10it/s, F1=0.971, train_acc=0.997, train_loss=0.307, train_precision=0.971, train_recall=0.974]\n",
      "Eval Result: 100%|██████████| 15/15 [00:15<00:00,  1.00s/it, F1=0.583, eval_acc=0.934, eval_loss=19.6, eval_precision=0.594, eval_recall=0.575]\n",
      "Epoch: 21/30 Train: 100%|██████████| 125/125 [00:58<00:00,  2.14it/s, F1=0.968, train_acc=0.997, train_loss=0.285, train_precision=0.97, train_recall=0.969] \n",
      "Eval Result: 100%|██████████| 15/15 [00:10<00:00,  1.49it/s, F1=0.568, eval_acc=0.932, eval_loss=19.8, eval_precision=0.583, eval_recall=0.557]\n",
      "Epoch: 22/30 Train: 100%|██████████| 125/125 [00:51<00:00,  2.44it/s, F1=0.966, train_acc=0.997, train_loss=0.364, train_precision=0.966, train_recall=0.968]\n",
      "Eval Result:  87%|████████▋ | 13/15 [00:13<00:02,  1.00s/it, F1=0.58, eval_acc=0.935, eval_loss=15.8, eval_precision=0.601, eval_recall=0.563] "
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "# from CC.trainer import NERTrainer\n",
    "\n",
    "# args = {\n",
    "#     'num_epochs': 30,\n",
    "#     'num_gpus': [0],\n",
    "#     'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "#     # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "#     'pretrained_file_name': './save_pretrained/cdd_pre_3/Bert_10470/pytorch_model.bin',\n",
    "#     'hidden_dim': 300,\n",
    "#     'max_seq_length': 150,\n",
    "#     'max_scan_num': 1000000,\n",
    "#     'inter_max_scan_num': 20000,\n",
    "#     'train_file': './data/CDD/0.5k/json/train.json',\n",
    "#     'eval_file': './data/CDD/dev.json',\n",
    "#     'test_file': './data/CDD/test.json',\n",
    "#     'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "#     'tag_file': 'data/CDD/cdd_tags_list.txt',\n",
    "#     'loader_name': 'le_loader',\n",
    "#     'output_eval':True,\n",
    "#     \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "#     \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "#     \"default_tag\":\"O\",\n",
    "#     'batch_size': 8,\n",
    "#     'eval_batch_size': 64,\n",
    "#     'do_shuffle': True,\n",
    "#     \"use_gpu\": True,\n",
    "#     \"debug\": True,\n",
    "#     'model_name': 'LEBert',\n",
    "#     'classify':'crf',\n",
    "#     'task_name': 'cdd_LEBert_pro_0.5k_1'\n",
    "# }\n",
    "\n",
    "# # Trainer\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_0.5k_2\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_0.5k_3\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_0.5k_4\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-5):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_0.5k_5\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# from CC.trainer import NERTrainer\n",
    "\n",
    "# args = {\n",
    "#     'num_epochs': 30,\n",
    "#     'num_gpus': [0],\n",
    "#     'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "#     # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "#     'pretrained_file_name': './save_pretrained/cdd_pre_3/Bert_10470/pytorch_model.bin',\n",
    "#     'hidden_dim': 300,\n",
    "#     'max_seq_length': 150,\n",
    "#     'max_scan_num': 1000000,\n",
    "#     'inter_max_scan_num': 20000,\n",
    "#     'train_file': './data/CDD/1k/json/train.json',\n",
    "#     'eval_file': './data/CDD/dev.json',\n",
    "#     'test_file': './data/CDD/test.json',\n",
    "#     'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "#     'tag_file': 'data/CDD/cdd_tags_list.txt',\n",
    "#     'loader_name': 'le_loader',\n",
    "#     'output_eval':True,\n",
    "#     \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "#     \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "#     \"default_tag\":\"O\",\n",
    "#     'batch_size': 8,\n",
    "#     'eval_batch_size': 64,\n",
    "#     'do_shuffle': True,\n",
    "#     \"use_gpu\": True,\n",
    "#     \"debug\": True,\n",
    "#     'model_name': 'LEBert',\n",
    "#     'classify':'crf',\n",
    "#     'task_name': 'cdd_LEBert_pro_1k_1'\n",
    "# }\n",
    "\n",
    "# # Trainer\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_1k_2\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_1k_3\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_1k_4\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-5):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_1k_5\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# from CC.trainer import NERTrainer\n",
    "\n",
    "# args = {\n",
    "#     'num_epochs': 30,\n",
    "#     'num_gpus': [0],\n",
    "#     'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "#     # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "#     'pretrained_file_name': './save_pretrained/cdd_pre_3/Bert_10470/pytorch_model.bin',\n",
    "#     'hidden_dim': 300,\n",
    "#     'max_seq_length': 150,\n",
    "#     'max_scan_num': 1000000,\n",
    "#     'inter_max_scan_num': 20000,\n",
    "#     'train_file': './data/CDD/2k/json/train.json',\n",
    "#     'eval_file': './data/CDD/dev.json',\n",
    "#     'test_file': './data/CDD/test.json',\n",
    "#     'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "#     'tag_file': 'data/CDD/cdd_tags_list.txt',\n",
    "#     'loader_name': 'le_loader',\n",
    "#     'output_eval':True,\n",
    "#     \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "#     \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "#     \"default_tag\":\"O\",\n",
    "#     'batch_size': 8,\n",
    "#     'eval_batch_size': 64,\n",
    "#     'do_shuffle': True,\n",
    "#     \"use_gpu\": True,\n",
    "#     \"debug\": True,\n",
    "#     'model_name': 'LEBert',\n",
    "#     'classify':'crf',\n",
    "#     'task_name': 'cdd_LEBert_pro_2k_1'\n",
    "# }\n",
    "\n",
    "# # Trainer\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_2k_2\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_2k_3\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_2k_4\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-5):\n",
    "#     a = i\n",
    "\n",
    "\n",
    "# args[\"task_name\"] = \"cdd_LEBert_pro_2k_5\"\n",
    "\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9392d1f0914889243d058bb73f0d89e61311fd6d751bbc8fa50e38d7d4ff811"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NER': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
