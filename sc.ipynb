{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_pre_3/Bert_1400/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'train_file': './data/FN/sc_conll/train.csv',\n",
    "    'eval_file': './data/FN/sc_conll/test.csv',\n",
    "    'test_file': './data/FN/sc_conll/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'output_eval':True,\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'Bert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'sc_bert_pro_3_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_bert_pro_3_2\"\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_bert_pro_3_3\"\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_bert_pro_3_4\"\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_bert_pro_3_5\"\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert预训练\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_2/Bert_8960/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/FN/tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'sc_pre_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\": \"关键词\",\n",
    "        # \"DIS\": \"疾病或诊断\",\n",
    "        # \"ANA\": \"解剖部位\",\n",
    "        # \"LAB\": \"实验室检验\",\n",
    "        # \"MED\": \"药物\",\n",
    "        # \"OPE\": \"手术\",\n",
    "        # \"IMA\": \"影像检查\",\n",
    "        # \"CHECK\":\"检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四川\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_pre_3/Bert_1400/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'classify':'crf',\n",
    "    'task_name': 'sc_LEBert_pro_3_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_3_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_3_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_3_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_3_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"sc_sum_1_v1_1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 103MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 152MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 347MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 24.6kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 7277.57L/s]\n",
      "build line mapper: 3L [00:00, 20426.81L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 183.30it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 44150.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:04<00:00, 94.51it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:01<00:00, 92.39it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'word_embeddings.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train:  20%|██        | 10/50 [00:10<00:42,  1.07s/it, F1=0.00129, train_acc=0.343, train_loss=760, train_precision=0.000915, train_recall=0.00466]/home/zl/anaconda3/envs/NER/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:52<00:00,  1.05s/it, F1=0.000257, train_acc=0.791, train_loss=305, train_precision=0.000183, train_recall=0.000933]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.36s/it, F1=0, eval_acc=0.91, eval_loss=146, eval_precision=0, eval_recall=0]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.0904, train_acc=0.922, train_loss=121, train_precision=0.101, train_recall=0.0857]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.287, eval_acc=0.942, eval_loss=83.8, eval_precision=0.304, eval_recall=0.272]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.146, train_acc=0.933, train_loss=78.8, train_precision=0.129, train_recall=0.174]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.131, eval_acc=0.948, eval_loss=56.8, eval_precision=0.112, eval_recall=0.157]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.256, train_acc=0.94, train_loss=56.5, train_precision=0.228, train_recall=0.297]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.366, eval_acc=0.941, eval_loss=45.8, eval_precision=0.35, eval_recall=0.384] \n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:52<00:00,  1.04s/it, F1=0.447, train_acc=0.951, train_loss=41.7, train_precision=0.422, train_recall=0.482]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.391, eval_acc=0.948, eval_loss=38.4, eval_precision=0.378, eval_recall=0.405]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.516, train_acc=0.954, train_loss=33.5, train_precision=0.502, train_recall=0.535]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.505, eval_acc=0.957, eval_loss=30.9, eval_precision=0.511, eval_recall=0.499]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.594, train_acc=0.962, train_loss=25, train_precision=0.584, train_recall=0.607]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.501, eval_acc=0.958, eval_loss=27.5, eval_precision=0.515, eval_recall=0.488]\n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.641, train_acc=0.965, train_loss=20.4, train_precision=0.632, train_recall=0.652]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.551, eval_acc=0.959, eval_loss=24.5, eval_precision=0.572, eval_recall=0.531]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.667, train_acc=0.968, train_loss=17.2, train_precision=0.668, train_recall=0.668]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.53, eval_acc=0.954, eval_loss=24.5, eval_precision=0.539, eval_recall=0.522]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.699, train_acc=0.97, train_loss=15.1, train_precision=0.705, train_recall=0.696] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.566, eval_acc=0.949, eval_loss=24.2, eval_precision=0.577, eval_recall=0.556]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.712, train_acc=0.969, train_loss=14.3, train_precision=0.719, train_recall=0.709]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.533, eval_acc=0.952, eval_loss=24, eval_precision=0.537, eval_recall=0.528]  \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.721, train_acc=0.969, train_loss=14.6, train_precision=0.735, train_recall=0.713]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.643, eval_acc=0.959, eval_loss=22.9, eval_precision=0.675, eval_recall=0.614]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.727, train_acc=0.971, train_loss=12.8, train_precision=0.737, train_recall=0.724]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.653, eval_acc=0.959, eval_loss=22.1, eval_precision=0.698, eval_recall=0.614]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.782, train_acc=0.978, train_loss=9.55, train_precision=0.787, train_recall=0.78] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.644, eval_acc=0.959, eval_loss=23.5, eval_precision=0.671, eval_recall=0.619]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:52<00:00,  1.05s/it, F1=0.814, train_acc=0.981, train_loss=7.77, train_precision=0.818, train_recall=0.814]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.651, eval_acc=0.959, eval_loss=29.8, eval_precision=0.681, eval_recall=0.623]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.809, train_acc=0.982, train_loss=7.22, train_precision=0.811, train_recall=0.81] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.669, eval_acc=0.959, eval_loss=28, eval_precision=0.69, eval_recall=0.649]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.825, train_acc=0.983, train_loss=6.88, train_precision=0.831, train_recall=0.822]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.664, eval_acc=0.959, eval_loss=33.6, eval_precision=0.677, eval_recall=0.651]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.833, train_acc=0.983, train_loss=6.15, train_precision=0.839, train_recall=0.83] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.663, eval_acc=0.957, eval_loss=31.8, eval_precision=0.653, eval_recall=0.673]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.853, train_acc=0.986, train_loss=5.62, train_precision=0.86, train_recall=0.847] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.68, eval_acc=0.961, eval_loss=34, eval_precision=0.692, eval_recall=0.669] \n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.845, train_acc=0.983, train_loss=6.59, train_precision=0.849, train_recall=0.844]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.682, eval_acc=0.961, eval_loss=30.8, eval_precision=0.683, eval_recall=0.68] \n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.85, train_acc=0.984, train_loss=5.63, train_precision=0.855, train_recall=0.849] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.665, eval_acc=0.962, eval_loss=32.3, eval_precision=0.666, eval_recall=0.665]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.85, train_acc=0.985, train_loss=5.05, train_precision=0.855, train_recall=0.848] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.664, eval_acc=0.96, eval_loss=33.3, eval_precision=0.663, eval_recall=0.665]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.86, train_acc=0.985, train_loss=5.28, train_precision=0.862, train_recall=0.86]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.672, eval_acc=0.961, eval_loss=39.5, eval_precision=0.688, eval_recall=0.656]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.852, train_acc=0.985, train_loss=5.3, train_precision=0.851, train_recall=0.854] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.46s/it, F1=0.671, eval_acc=0.959, eval_loss=40.5, eval_precision=0.675, eval_recall=0.666]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.02s/it, F1=0.86, train_acc=0.984, train_loss=5.45, train_precision=0.866, train_recall=0.857] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.676, eval_acc=0.96, eval_loss=39, eval_precision=0.673, eval_recall=0.68]    \n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.874, train_acc=0.987, train_loss=4.43, train_precision=0.875, train_recall=0.874]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.671, eval_acc=0.959, eval_loss=29.4, eval_precision=0.657, eval_recall=0.686]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.885, train_acc=0.987, train_loss=4.25, train_precision=0.885, train_recall=0.887]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.659, eval_acc=0.955, eval_loss=30.3, eval_precision=0.647, eval_recall=0.672]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.918, train_acc=0.991, train_loss=3.14, train_precision=0.917, train_recall=0.921]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.662, eval_acc=0.959, eval_loss=35, eval_precision=0.651, eval_recall=0.674] \n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.913, train_acc=0.991, train_loss=3.1, train_precision=0.915, train_recall=0.911] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it, F1=0.669, eval_acc=0.958, eval_loss=28.5, eval_precision=0.656, eval_recall=0.683]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.91, train_acc=0.991, train_loss=2.95, train_precision=0.909, train_recall=0.912] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.647, eval_acc=0.953, eval_loss=35, eval_precision=0.618, eval_recall=0.679]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"sc_sum_1_v1_2\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 360MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 344MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 353MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 128kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 7786.46L/s]\n",
      "build line mapper: 3L [00:00, 5318.22L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 829.08it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 33376.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:04<00:00, 93.08it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:01<00:00, 90.86it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'word_embeddings.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0, train_acc=0.871, train_loss=211, train_precision=0, train_recall=0]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.00481, eval_acc=0.921, eval_loss=109, eval_precision=0.0098, eval_recall=0.00318]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.129, train_acc=0.925, train_loss=93.7, train_precision=0.144, train_recall=0.121]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.161, eval_acc=0.936, eval_loss=69.8, eval_precision=0.22, eval_recall=0.127] \n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.356, train_acc=0.94, train_loss=61.3, train_precision=0.34, train_recall=0.379]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.415, eval_acc=0.941, eval_loss=55.1, eval_precision=0.439, eval_recall=0.393]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.506, train_acc=0.947, train_loss=44.8, train_precision=0.491, train_recall=0.528]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.537, eval_acc=0.946, eval_loss=44.3, eval_precision=0.57, eval_recall=0.508]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.577, train_acc=0.952, train_loss=34.1, train_precision=0.576, train_recall=0.585]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.593, eval_acc=0.956, eval_loss=30.9, eval_precision=0.597, eval_recall=0.589]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.617, train_acc=0.957, train_loss=26.4, train_precision=0.628, train_recall=0.613]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.631, eval_acc=0.957, eval_loss=26.1, eval_precision=0.651, eval_recall=0.612]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.654, train_acc=0.96, train_loss=20.9, train_precision=0.663, train_recall=0.651] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.608, eval_acc=0.956, eval_loss=23.4, eval_precision=0.63, eval_recall=0.587] \n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.678, train_acc=0.961, train_loss=18, train_precision=0.687, train_recall=0.674]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.62, eval_acc=0.951, eval_loss=23.7, eval_precision=0.665, eval_recall=0.58]  \n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.682, train_acc=0.963, train_loss=16, train_precision=0.689, train_recall=0.68]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.612, eval_acc=0.948, eval_loss=25.3, eval_precision=0.696, eval_recall=0.547]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.711, train_acc=0.965, train_loss=14.6, train_precision=0.716, train_recall=0.713]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.627, eval_acc=0.954, eval_loss=20.6, eval_precision=0.684, eval_recall=0.58] \n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.731, train_acc=0.967, train_loss=12.7, train_precision=0.745, train_recall=0.728]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.666, eval_acc=0.96, eval_loss=18.2, eval_precision=0.673, eval_recall=0.66]  \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.749, train_acc=0.971, train_loss=10.9, train_precision=0.754, train_recall=0.748]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.615, eval_acc=0.955, eval_loss=18.9, eval_precision=0.656, eval_recall=0.579]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.77, train_acc=0.973, train_loss=9.24, train_precision=0.776, train_recall=0.77]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.614, eval_acc=0.952, eval_loss=20.2, eval_precision=0.631, eval_recall=0.599]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.798, train_acc=0.976, train_loss=8.01, train_precision=0.806, train_recall=0.795]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.607, eval_acc=0.938, eval_loss=23.3, eval_precision=0.607, eval_recall=0.607]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.816, train_acc=0.98, train_loss=7.01, train_precision=0.816, train_recall=0.819] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it, F1=0.641, eval_acc=0.957, eval_loss=21.5, eval_precision=0.664, eval_recall=0.619]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.836, train_acc=0.981, train_loss=6.18, train_precision=0.84, train_recall=0.835] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.646, eval_acc=0.949, eval_loss=23.7, eval_precision=0.648, eval_recall=0.643]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.843, train_acc=0.983, train_loss=5.79, train_precision=0.844, train_recall=0.845]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.62, eval_acc=0.94, eval_loss=25.3, eval_precision=0.613, eval_recall=0.628]  \n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:51<00:00,  1.03s/it, F1=0.86, train_acc=0.985, train_loss=5.23, train_precision=0.86, train_recall=0.861]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.604, eval_acc=0.944, eval_loss=24.1, eval_precision=0.603, eval_recall=0.605]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.869, train_acc=0.986, train_loss=4.53, train_precision=0.871, train_recall=0.868]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.62, eval_acc=0.95, eval_loss=27.3, eval_precision=0.617, eval_recall=0.623]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.02s/it, F1=0.872, train_acc=0.986, train_loss=4.33, train_precision=0.876, train_recall=0.87] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.642, eval_acc=0.951, eval_loss=27.5, eval_precision=0.64, eval_recall=0.644] \n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.88, train_acc=0.987, train_loss=4.09, train_precision=0.881, train_recall=0.88]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it, F1=0.665, eval_acc=0.951, eval_loss=26.6, eval_precision=0.654, eval_recall=0.676]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.883, train_acc=0.986, train_loss=4.35, train_precision=0.88, train_recall=0.888] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.61, eval_acc=0.942, eval_loss=24.2, eval_precision=0.612, eval_recall=0.608]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.884, train_acc=0.987, train_loss=4.06, train_precision=0.886, train_recall=0.885]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.631, eval_acc=0.945, eval_loss=25.4, eval_precision=0.621, eval_recall=0.642]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.898, train_acc=0.988, train_loss=3.7, train_precision=0.899, train_recall=0.899] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.616, eval_acc=0.943, eval_loss=23.6, eval_precision=0.607, eval_recall=0.626]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.892, train_acc=0.989, train_loss=3.7, train_precision=0.893, train_recall=0.892] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.621, eval_acc=0.938, eval_loss=27.7, eval_precision=0.61, eval_recall=0.633] \n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.912, train_acc=0.989, train_loss=3.3, train_precision=0.914, train_recall=0.911] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.639, eval_acc=0.949, eval_loss=27.1, eval_precision=0.63, eval_recall=0.649] \n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.903, train_acc=0.99, train_loss=3.03, train_precision=0.903, train_recall=0.904] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.673, eval_acc=0.958, eval_loss=28, eval_precision=0.682, eval_recall=0.665]  \n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.908, train_acc=0.99, train_loss=3.23, train_precision=0.909, train_recall=0.909] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.673, eval_acc=0.958, eval_loss=25.2, eval_precision=0.683, eval_recall=0.664]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.912, train_acc=0.989, train_loss=3.18, train_precision=0.914, train_recall=0.912]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.643, eval_acc=0.955, eval_loss=23.2, eval_precision=0.654, eval_recall=0.633]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.924, train_acc=0.992, train_loss=2.58, train_precision=0.922, train_recall=0.927]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.681, eval_acc=0.956, eval_loss=26.4, eval_precision=0.71, eval_recall=0.654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"sc_sum_1_v1_3\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 357MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 346MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 379MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 64.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 8388.61L/s]\n",
      "build line mapper: 3L [00:00, 5667.98L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 871.45it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 36366.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:04<00:00, 91.40it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:01<00:00, 92.44it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'word_embeddings.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=3.07e-5, train_acc=0.784, train_loss=391, train_precision=1.56e-5, train_recall=0.000906]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0, eval_acc=0.91, eval_loss=246, eval_precision=0, eval_recall=0]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.00189, train_acc=0.908, train_loss=205, train_precision=0.00114, train_recall=0.00591]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.0527, eval_acc=0.938, eval_loss=130, eval_precision=0.0341, eval_recall=0.117]\n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.0851, train_acc=0.93, train_loss=134, train_precision=0.053, train_recall=0.222]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.194, eval_acc=0.946, eval_loss=94.6, eval_precision=0.123, eval_recall=0.458]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.216, train_acc=0.941, train_loss=97, train_precision=0.143, train_recall=0.458]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.309, eval_acc=0.951, eval_loss=77.7, eval_precision=0.211, eval_recall=0.576]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.385, train_acc=0.951, train_loss=70.4, train_precision=0.305, train_recall=0.543]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it, F1=0.436, eval_acc=0.957, eval_loss=60, eval_precision=0.349, eval_recall=0.581] \n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.536, train_acc=0.957, train_loss=52.9, train_precision=0.495, train_recall=0.593]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it, F1=0.594, eval_acc=0.959, eval_loss=48.3, eval_precision=0.57, eval_recall=0.619] \n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.599, train_acc=0.957, train_loss=42.1, train_precision=0.578, train_recall=0.63] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.594, eval_acc=0.959, eval_loss=42.4, eval_precision=0.583, eval_recall=0.607]\n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.63, train_acc=0.963, train_loss=32.6, train_precision=0.617, train_recall=0.65]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.591, eval_acc=0.959, eval_loss=42.2, eval_precision=0.571, eval_recall=0.612]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.674, train_acc=0.967, train_loss=27.2, train_precision=0.671, train_recall=0.684]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.605, eval_acc=0.958, eval_loss=36.5, eval_precision=0.6, eval_recall=0.609] \n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.694, train_acc=0.968, train_loss=21.9, train_precision=0.696, train_recall=0.698]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.595, eval_acc=0.957, eval_loss=34.8, eval_precision=0.61, eval_recall=0.58]  \n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.722, train_acc=0.972, train_loss=17.7, train_precision=0.721, train_recall=0.727]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.622, eval_acc=0.956, eval_loss=31.2, eval_precision=0.625, eval_recall=0.619]\n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.742, train_acc=0.974, train_loss=14.8, train_precision=0.744, train_recall=0.746]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.657, eval_acc=0.961, eval_loss=34, eval_precision=0.668, eval_recall=0.646]  \n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.02s/it, F1=0.767, train_acc=0.977, train_loss=12.9, train_precision=0.776, train_recall=0.764]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it, F1=0.643, eval_acc=0.956, eval_loss=33.7, eval_precision=0.632, eval_recall=0.654]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.78, train_acc=0.978, train_loss=11.3, train_precision=0.781, train_recall=0.783] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.644, eval_acc=0.957, eval_loss=31.6, eval_precision=0.656, eval_recall=0.632]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:51<00:00,  1.03s/it, F1=0.798, train_acc=0.98, train_loss=9.97, train_precision=0.8, train_recall=0.8]     \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.672, eval_acc=0.959, eval_loss=31.7, eval_precision=0.663, eval_recall=0.682]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.812, train_acc=0.981, train_loss=9.01, train_precision=0.814, train_recall=0.814]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.644, eval_acc=0.954, eval_loss=28.3, eval_precision=0.642, eval_recall=0.645]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.818, train_acc=0.982, train_loss=8.46, train_precision=0.823, train_recall=0.816]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.67, eval_acc=0.958, eval_loss=26.5, eval_precision=0.658, eval_recall=0.683] \n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:52<00:00,  1.04s/it, F1=0.824, train_acc=0.983, train_loss=8.71, train_precision=0.83, train_recall=0.821] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.636, eval_acc=0.95, eval_loss=29.5, eval_precision=0.634, eval_recall=0.639] \n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.847, train_acc=0.985, train_loss=6.86, train_precision=0.851, train_recall=0.845]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.651, eval_acc=0.954, eval_loss=29.2, eval_precision=0.649, eval_recall=0.655]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.85, train_acc=0.985, train_loss=6.17, train_precision=0.854, train_recall=0.847] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.657, eval_acc=0.954, eval_loss=27.2, eval_precision=0.641, eval_recall=0.674]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:47<00:00,  1.04it/s, F1=0.867, train_acc=0.987, train_loss=5.69, train_precision=0.871, train_recall=0.865]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it, F1=0.623, eval_acc=0.948, eval_loss=30.9, eval_precision=0.611, eval_recall=0.636]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.852, train_acc=0.985, train_loss=5.94, train_precision=0.856, train_recall=0.851]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.624, eval_acc=0.953, eval_loss=29.9, eval_precision=0.616, eval_recall=0.632]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.874, train_acc=0.987, train_loss=4.91, train_precision=0.878, train_recall=0.873]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.622, eval_acc=0.947, eval_loss=30.6, eval_precision=0.623, eval_recall=0.621]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.872, train_acc=0.988, train_loss=4.61, train_precision=0.873, train_recall=0.873]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it, F1=0.638, eval_acc=0.949, eval_loss=32.4, eval_precision=0.641, eval_recall=0.634]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.884, train_acc=0.988, train_loss=4.54, train_precision=0.887, train_recall=0.884]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it, F1=0.636, eval_acc=0.95, eval_loss=34.5, eval_precision=0.622, eval_recall=0.651] \n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.02s/it, F1=0.899, train_acc=0.99, train_loss=4.23, train_precision=0.901, train_recall=0.899] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it, F1=0.651, eval_acc=0.953, eval_loss=36, eval_precision=0.637, eval_recall=0.667] \n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.887, train_acc=0.989, train_loss=4.09, train_precision=0.89, train_recall=0.885] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.66, eval_acc=0.953, eval_loss=34.2, eval_precision=0.647, eval_recall=0.673] \n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.895, train_acc=0.99, train_loss=4.16, train_precision=0.896, train_recall=0.896] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.651, eval_acc=0.953, eval_loss=32.5, eval_precision=0.649, eval_recall=0.654]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.895, train_acc=0.99, train_loss=3.65, train_precision=0.895, train_recall=0.898] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.662, eval_acc=0.957, eval_loss=35.6, eval_precision=0.657, eval_recall=0.667]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.903, train_acc=0.99, train_loss=3.68, train_precision=0.907, train_recall=0.901] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.638, eval_acc=0.949, eval_loss=37.4, eval_precision=0.621, eval_recall=0.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"sc_sum_1_v1_4\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 362MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 348MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 377MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 63.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 8642.11L/s]\n",
      "build line mapper: 3L [00:00, 5893.64L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 890.51it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 46431.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:04<00:00, 94.15it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:01<00:00, 92.89it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'word_embeddings.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.0037, train_acc=0.905, train_loss=91.9, train_precision=0.0137, train_recall=0.00216]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0, eval_acc=0.923, eval_loss=48.9, eval_precision=0, eval_recall=0]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.19, train_acc=0.923, train_loss=44.1, train_precision=0.342, train_recall=0.149]    \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.514, eval_acc=0.948, eval_loss=29, eval_precision=0.515, eval_recall=0.513]  \n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.485, train_acc=0.934, train_loss=30.1, train_precision=0.573, train_recall=0.444]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.594, eval_acc=0.952, eval_loss=21.3, eval_precision=0.586, eval_recall=0.602]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.565, train_acc=0.944, train_loss=21.6, train_precision=0.615, train_recall=0.539]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.606, eval_acc=0.944, eval_loss=19.1, eval_precision=0.579, eval_recall=0.636]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.601, train_acc=0.95, train_loss=16.2, train_precision=0.634, train_recall=0.582] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.617, eval_acc=0.95, eval_loss=16.2, eval_precision=0.583, eval_recall=0.655]\n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.648, train_acc=0.956, train_loss=13.2, train_precision=0.675, train_recall=0.629]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.634, eval_acc=0.954, eval_loss=15.2, eval_precision=0.606, eval_recall=0.664]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.66, train_acc=0.957, train_loss=11.3, train_precision=0.685, train_recall=0.644] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.637, eval_acc=0.954, eval_loss=14.2, eval_precision=0.62, eval_recall=0.655] \n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.667, train_acc=0.957, train_loss=12, train_precision=0.698, train_recall=0.652]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.652, eval_acc=0.961, eval_loss=13.1, eval_precision=0.657, eval_recall=0.648]\n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.712, train_acc=0.961, train_loss=10.1, train_precision=0.728, train_recall=0.703]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.668, eval_acc=0.955, eval_loss=12.1, eval_precision=0.722, eval_recall=0.621]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.743, train_acc=0.966, train_loss=8.23, train_precision=0.753, train_recall=0.739]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.685, eval_acc=0.961, eval_loss=13.5, eval_precision=0.701, eval_recall=0.669]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:47<00:00,  1.04it/s, F1=0.758, train_acc=0.968, train_loss=7.45, train_precision=0.767, train_recall=0.754]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.658, eval_acc=0.96, eval_loss=11.8, eval_precision=0.658, eval_recall=0.658] \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.788, train_acc=0.972, train_loss=6.27, train_precision=0.801, train_recall=0.78] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.652, eval_acc=0.953, eval_loss=15, eval_precision=0.637, eval_recall=0.668] \n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.797, train_acc=0.975, train_loss=6.26, train_precision=0.81, train_recall=0.79]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.675, eval_acc=0.956, eval_loss=14.9, eval_precision=0.662, eval_recall=0.69]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.809, train_acc=0.975, train_loss=5.48, train_precision=0.821, train_recall=0.8]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.668, eval_acc=0.955, eval_loss=17, eval_precision=0.638, eval_recall=0.701] \n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.825, train_acc=0.978, train_loss=5.49, train_precision=0.833, train_recall=0.821]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.651, eval_acc=0.954, eval_loss=14.4, eval_precision=0.638, eval_recall=0.665]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s, F1=0.836, train_acc=0.979, train_loss=4.93, train_precision=0.846, train_recall=0.829]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.674, eval_acc=0.962, eval_loss=16.3, eval_precision=0.654, eval_recall=0.696]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.835, train_acc=0.977, train_loss=5.18, train_precision=0.846, train_recall=0.828]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.664, eval_acc=0.96, eval_loss=18.3, eval_precision=0.646, eval_recall=0.684]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.812, train_acc=0.974, train_loss=5.72, train_precision=0.823, train_recall=0.807]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.666, eval_acc=0.96, eval_loss=14.9, eval_precision=0.702, eval_recall=0.634] \n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.841, train_acc=0.975, train_loss=5.59, train_precision=0.848, train_recall=0.84] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.679, eval_acc=0.958, eval_loss=17.1, eval_precision=0.724, eval_recall=0.64]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.835, train_acc=0.973, train_loss=6.37, train_precision=0.842, train_recall=0.833]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.673, eval_acc=0.956, eval_loss=17.1, eval_precision=0.716, eval_recall=0.636]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.859, train_acc=0.979, train_loss=4.57, train_precision=0.866, train_recall=0.858]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.668, eval_acc=0.958, eval_loss=16.2, eval_precision=0.698, eval_recall=0.642]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.854, train_acc=0.979, train_loss=4.96, train_precision=0.861, train_recall=0.854]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.685, eval_acc=0.963, eval_loss=16.1, eval_precision=0.701, eval_recall=0.669]\n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.882, train_acc=0.985, train_loss=3.45, train_precision=0.89, train_recall=0.877] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it, F1=0.637, eval_acc=0.955, eval_loss=19.7, eval_precision=0.686, eval_recall=0.595]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.892, train_acc=0.985, train_loss=3.34, train_precision=0.895, train_recall=0.892]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.652, eval_acc=0.958, eval_loss=21, eval_precision=0.724, eval_recall=0.594]  \n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.884, train_acc=0.982, train_loss=4.22, train_precision=0.889, train_recall=0.88] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.622, eval_acc=0.951, eval_loss=24.7, eval_precision=0.723, eval_recall=0.547]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.893, train_acc=0.984, train_loss=4.3, train_precision=0.892, train_recall=0.897] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.584, eval_acc=0.948, eval_loss=31, eval_precision=0.662, eval_recall=0.523]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.886, train_acc=0.983, train_loss=4.35, train_precision=0.888, train_recall=0.885]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.629, eval_acc=0.953, eval_loss=26.1, eval_precision=0.696, eval_recall=0.574]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.878, train_acc=0.986, train_loss=3.84, train_precision=0.884, train_recall=0.875]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.653, eval_acc=0.958, eval_loss=24, eval_precision=0.703, eval_recall=0.61]\n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.922, train_acc=0.991, train_loss=2.54, train_precision=0.923, train_recall=0.921]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.675, eval_acc=0.959, eval_loss=21.1, eval_precision=0.684, eval_recall=0.667]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.928, train_acc=0.991, train_loss=2.14, train_precision=0.929, train_recall=0.928]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.67, eval_acc=0.961, eval_loss=19.8, eval_precision=0.673, eval_recall=0.666] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"inter_knowledge_file\": \"./data/tencent/THUOCL_FN_medical.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"inter_max_scan_num\": 20000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"sc_sum_1_v1_5\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 370MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 368MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 377MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 59.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 8892.52L/s]\n",
      "build line mapper: 3L [00:00, 26886.56L/s]3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 881.16it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 46776.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_lexicon_tree\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_word_vocab\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/inter_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:04<00:00, 94.65it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:01<00:00, 95.01it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin were not used when initializing ZLEBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing ZLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ZLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ZLEBertModel were not initialized from the model checkpoint at ./save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.word_word_weight.weight', 'word_embeddings.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.bias', 'inter_word_embeddings.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.000321, train_acc=0.796, train_loss=279, train_precision=0.000165, train_recall=0.00636]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0, eval_acc=0.91, eval_loss=119, eval_precision=0, eval_recall=0]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.0522, train_acc=0.916, train_loss=98.7, train_precision=0.0679, train_recall=0.044]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.215, eval_acc=0.94, eval_loss=64.2, eval_precision=0.269, eval_recall=0.179] \n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.385, train_acc=0.934, train_loss=59.8, train_precision=0.433, train_recall=0.366]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.516, eval_acc=0.947, eval_loss=44.1, eval_precision=0.525, eval_recall=0.507]\n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.522, train_acc=0.941, train_loss=41.9, train_precision=0.55, train_recall=0.521] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.582, eval_acc=0.953, eval_loss=34.6, eval_precision=0.623, eval_recall=0.547]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.559, train_acc=0.948, train_loss=31.8, train_precision=0.586, train_recall=0.552]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.601, eval_acc=0.952, eval_loss=27.7, eval_precision=0.6, eval_recall=0.603]  \n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.572, train_acc=0.946, train_loss=28.1, train_precision=0.595, train_recall=0.567]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.615, eval_acc=0.959, eval_loss=23.6, eval_precision=0.629, eval_recall=0.601]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.636, train_acc=0.956, train_loss=20.2, train_precision=0.645, train_recall=0.634]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.644, eval_acc=0.96, eval_loss=21.8, eval_precision=0.681, eval_recall=0.611] \n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:47<00:00,  1.04it/s, F1=0.647, train_acc=0.958, train_loss=18, train_precision=0.66, train_recall=0.647]   \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.603, eval_acc=0.949, eval_loss=21.9, eval_precision=0.68, eval_recall=0.542] \n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.66, train_acc=0.959, train_loss=16.3, train_precision=0.664, train_recall=0.664] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.526, eval_acc=0.936, eval_loss=29.5, eval_precision=0.741, eval_recall=0.409]\n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.647, train_acc=0.956, train_loss=16.4, train_precision=0.658, train_recall=0.646]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.606, eval_acc=0.941, eval_loss=26.3, eval_precision=0.779, eval_recall=0.497]\n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.67, train_acc=0.958, train_loss=14.3, train_precision=0.683, train_recall=0.668] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.621, eval_acc=0.957, eval_loss=21.4, eval_precision=0.641, eval_recall=0.603]\n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.704, train_acc=0.965, train_loss=11.5, train_precision=0.711, train_recall=0.704]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.623, eval_acc=0.956, eval_loss=21.7, eval_precision=0.64, eval_recall=0.606]\n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it, F1=0.749, train_acc=0.97, train_loss=9.38, train_precision=0.75, train_recall=0.753]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.625, eval_acc=0.955, eval_loss=23.4, eval_precision=0.666, eval_recall=0.588]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:52<00:00,  1.05s/it, F1=0.759, train_acc=0.972, train_loss=8.43, train_precision=0.757, train_recall=0.765]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.633, eval_acc=0.955, eval_loss=23.9, eval_precision=0.661, eval_recall=0.607]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:47<00:00,  1.04it/s, F1=0.784, train_acc=0.974, train_loss=7.45, train_precision=0.786, train_recall=0.788]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.643, eval_acc=0.954, eval_loss=28.1, eval_precision=0.673, eval_recall=0.615]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.02s/it, F1=0.801, train_acc=0.976, train_loss=6.9, train_precision=0.799, train_recall=0.807] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.675, eval_acc=0.957, eval_loss=23, eval_precision=0.693, eval_recall=0.659]  \n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:51<00:00,  1.02s/it, F1=0.815, train_acc=0.979, train_loss=6.12, train_precision=0.815, train_recall=0.817]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.647, eval_acc=0.957, eval_loss=26.8, eval_precision=0.652, eval_recall=0.643]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s, F1=0.829, train_acc=0.982, train_loss=5.35, train_precision=0.827, train_recall=0.834]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.658, eval_acc=0.959, eval_loss=28, eval_precision=0.669, eval_recall=0.648]  \n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.02s/it, F1=0.837, train_acc=0.981, train_loss=5.19, train_precision=0.835, train_recall=0.841]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.659, eval_acc=0.958, eval_loss=25.7, eval_precision=0.67, eval_recall=0.648] \n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s, F1=0.858, train_acc=0.983, train_loss=4.55, train_precision=0.854, train_recall=0.863]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.652, eval_acc=0.957, eval_loss=26.1, eval_precision=0.663, eval_recall=0.642]\n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:51<00:00,  1.03s/it, F1=0.863, train_acc=0.983, train_loss=4.41, train_precision=0.866, train_recall=0.862]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.664, eval_acc=0.956, eval_loss=27.3, eval_precision=0.691, eval_recall=0.639]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:47<00:00,  1.04it/s, F1=0.864, train_acc=0.984, train_loss=4.52, train_precision=0.862, train_recall=0.87] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.68, eval_acc=0.959, eval_loss=32.8, eval_precision=0.696, eval_recall=0.666] \n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.876, train_acc=0.983, train_loss=4.21, train_precision=0.873, train_recall=0.882]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it, F1=0.675, eval_acc=0.958, eval_loss=32.7, eval_precision=0.685, eval_recall=0.665]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.855, train_acc=0.984, train_loss=3.96, train_precision=0.853, train_recall=0.86] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.676, eval_acc=0.961, eval_loss=28, eval_precision=0.661, eval_recall=0.691]\n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s, F1=0.864, train_acc=0.984, train_loss=4.41, train_precision=0.867, train_recall=0.863]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.673, eval_acc=0.959, eval_loss=17, eval_precision=0.654, eval_recall=0.693] \n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.87, train_acc=0.985, train_loss=4.28, train_precision=0.873, train_recall=0.871] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.663, eval_acc=0.955, eval_loss=24.5, eval_precision=0.643, eval_recall=0.685]\n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:48<00:00,  1.03it/s, F1=0.86, train_acc=0.981, train_loss=4.88, train_precision=0.868, train_recall=0.858] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it, F1=0.665, eval_acc=0.959, eval_loss=19, eval_precision=0.667, eval_recall=0.663]  \n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:49<00:00,  1.00it/s, F1=0.868, train_acc=0.983, train_loss=4.66, train_precision=0.87, train_recall=0.87]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.632, eval_acc=0.95, eval_loss=25.8, eval_precision=0.612, eval_recall=0.652] \n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:52<00:00,  1.06s/it, F1=0.872, train_acc=0.984, train_loss=4.46, train_precision=0.875, train_recall=0.872]\n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it, F1=0.61, eval_acc=0.944, eval_loss=28.6, eval_precision=0.564, eval_recall=0.663] \n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:50<00:00,  1.01s/it, F1=0.859, train_acc=0.983, train_loss=4.9, train_precision=0.869, train_recall=0.854] \n",
      "Eval Result: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it, F1=0.63, eval_acc=0.95, eval_loss=25, eval_precision=0.609, eval_recall=0.652]   \n"
     ]
    }
   ],
   "source": [
    "# 四川\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_pre_3/Bert_1400/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/summary_pre_1/Bert_2020/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 20000,\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'sc_sum_1_v1_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"sc_sum_1_v1_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_sum_1_v1_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_sum_1_v1_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"sc_sum_1_v1_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四川\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_pre_4/Bert_21105/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'classify':'crf',\n",
    "    'task_name': 'sc_LEBert_pro_4_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_4_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_4_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_4_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_LEBert_pro_4_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四川\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_pre_5/Bert_11865/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'inter_max_scan_num': 20000,\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader_zl',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"inter_knowledge_file\":\"./data/tencent/THUOCL_FN_medical.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'ZLEBert',\n",
    "    'classify':'lstm_crf',\n",
    "    'task_name': 'sc_v1_pro_5_1'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"sc_v1_pro_5_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_v1_pro_5_3\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "args[\"task_name\"] = \"sc_v1_pro_5_4\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i\n",
    "\n",
    "\n",
    "args[\"task_name\"] = \"sc_v1_pro_5_5\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b5eafe76d1bce12a884379452caff4a891b716fe38e21da9ac26a4bcd47e2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
